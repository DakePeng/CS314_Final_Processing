Published as a conference paper at ICLR 2018 PARAMETER SPACE NOISE FOR EXPLORATION Matthias Plappertyz, Rein Houthoofty, Prafulla Dhariwaly, Szymon Sidory, Richard Y. Cheny, Xi Chenyy, Tamim Asfourz, Pieter Abbeelyy, and Marcin Andrychowiczy yOpenAI zKarlsruhe Institute of Technology (KIT) yyUniversity of California, Berkeley Correspondence to matthias@openai.com ABSTRACT Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent‚Äôs parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both offand on-policy methods beneÔ¨Åt from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. 1 I NTRODUCTION Exploration remains a key challenge in contemporary deep reinforcement learning (RL). Its main purpose is to ensure that the agent‚Äôs behavior does not converge prematurely to a local optimum. Enabling efÔ¨Åcient and effective exploration is, however, not trivial since it is not directed by the reward function of the underlying Markov decision process (MDP). Although a plethora of methods have been proposed to tackle this challenge in high-dimensional and/or continuous-action MDPs, they often rely on complex additional structures such as counting tables (Tang et al., 2016), density modeling of the state space (Ostrovski et al., 2017), learned dynamics models (Houthooft et al., 2016; Achiam & Sastry, 2017; Stadie et al., 2015), or self-supervised curiosity (Pathak et al., 2017). An orthogonal way of increasing the exploratory nature of these algorithms is through the addition of temporally-correlated noise, for example as done in bootstrapped DQN (Osband et al., 2016a). Along the same lines, it was shown that the addition of parameter noise leads to better exploration by obtaining a policy that exhibits a larger variety of behaviors (Sun et al., 2009b; Salimans et al., 2017). We discuss these related approaches in greater detail in Section 5. Their main limitation, however, is that they are either only proposed and evaluated for the on-policy setting with relatively small and shallow function approximators (R√ºckstie√ü et al., 2008) or disregard all temporal structure and gradient information (Salimans et al., 2017; Kober & Peters, 2008; Sehnke et al., 2010). This paper investigates how parameter space noise can be effectively combined with off-the-shelf deep RL algorithms such as DQN (Mnih et al., 2015), DDPG (Lillicrap et al., 2015), and TRPO (Schulman et al., 2015b) to improve their exploratory behavior. Experiments show that this form of exploration is applicable to both high-dimensional discrete environments and continuous control tasks, using onand off-policy methods. Our results indicate that parameter noise outperforms traditional action space noise-based baselines, especially in tasks where the reward signal is extremely sparse. 2 B ACKGROUND We consider the standard RL framework consisting of an agent interacting with an environment. To simplify the exposition we assume that the environment is fully observable. An environment is modeled as a Markov decision process (MDP) and is deÔ¨Åned by a set of states S, a set of actions A, a distribution over initial states p(s0), a reward function r:SA7! R, transition probabilities 1arXiv:1706.01905v2  [cs.LG]  31 Jan 2018Published as a conference paper at ICLR 2018 p(st+1jst;at), a time horizon T, and a discount factor  2[0;1). We denote by a policy parametrized by , which can be either deterministic, :S7!A , or stochastic, :S7!P (A). The agent‚Äôs goal is to maximize the expected discounted return () =E[PT t=0 tr(st;at)], where= (s0;a0;:::;sT)denotes a trajectory with s0p(s0),at(atjst), andst+1p(st+1jst;at). Experimental evaluation is based on the undiscounted return E[PT t=0r(st;at)].1 2.1 O FF-POLICY METHODS Off-policy RL methods allow learning based on data captured by arbitrary policies. This paper considers two popular off-policy algorithms, namely Deep Q-Networks (DQN, Mnih et al. (2015)) and Deep Deterministic Policy Gradients (DDPG, Lillicrap et al. (2015)). Deep Q-Networks (DQN) DQN uses a deep neural network as a function approximator to estimate the optimalQ-value function, which conforms to the Bellman optimality equation: Q(st;at) =r(st;at) + max a02AQ(st+1;a0): The policy is implicitly deÔ¨Åned by Qas(st) = argmaxa02AQ(st;a0). Typically, a stochastic greedy or Boltzmann policy (Sutton & Barto, 1998) is derived from the Q-value function to encourage exploration, which relies on sampling noise in the action space. The Q-network predicts a Q-value for each action and is updated using off-policy data from a replay buffer. Deep Deterministic Policy Gradients (DDPG) DDPG is an actor-critic algorithm, applicable to continuous action spaces. Similar to DQN, the critic estimates the Q-value function using off-policy data and the recursive Bellman equation: Q(st;at) =r(st;at) + Q(st+1;(st+1)); whereis the actor or policy. The actor is trained to maximize the critic‚Äôs estimated Q-values by back-propagating through both networks. For exploration, DDPG uses a stochastic policy of the formc(st) =(st) +w, wherewis eitherwN (0;2I)(uncorrelated) or wOU(0;2) (correlated).2Again, exploration is realized through action space noise. 2.2 O N-POLICY METHODS In contrast to off-policy algorithms, on-policy methods require updating function approximators according to the currently followed policy. In particular, we will consider Trust Region Policy Optimization (TRPO, Schulman et al. (2015a)), an extension of traditional policy gradient methods (Williams, 1992b) using the natural gradient direction (Peters & Schaal, 2008; Kakade, 2001). Trust Region Policy Optimization (TRPO) TRPO improves upon REINFORCE (Williams, 1992b) by computing an ascent direction that ensures a small change in the policy distribution. More speciÔ¨Åcally, TRPO solves the following constrained optimization problem: maximizeEs0;a0(ajs) 0 (ajs)A(s;a) s.t.Es0[DKL(0(js)k(js))]KL where=is the discounted state-visitation frequencies induced by ,A(s;a)denotes the advantage function estimated by the empirical return minus the baseline, and KLis a step size parameter which controls how much the policy is allowed to change per iteration. 3 P ARAMETER SPACE NOISE FOR EXPLORATION This work considers policies that are realized as parameterized functions, which we denote as , withbeing the parameter vector. We represent policies as neural networks but our technique can 1Ift=T, we write r(sT; aT)to denote the terminal reward, even though it has has no dependence on aT, to simplify notation. 2OU(;)denotes the Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930). 2Published as a conference paper at ICLR 2018 be applied to arbitrary parametric models. To achieve structured exploration, we sample from a set of policies by applying additive Gaussian noise to the parameter vector of the current policy: e=+N(0;2I). Importantly, the perturbed policy is sampled at the beginning of each episode and kept Ô¨Åxed for the entire rollout. For convenience and readability, we denote this perturbed policy ase:=eand analogously deÔ¨Åne :=. State-dependent exploration As pointed out by R√ºckstie√ü et al. (2008), there is a crucial difference between action space noise and parameter space noise. Consider the continuous action space case. When using Gaussian action noise, actions are sampled according to some stochastic policy, generatingat=(st) +N(0;2I). Therefore, even for a Ô¨Åxed states, we will almost certainly obtain a different action whenever that state is sampled again in the rollout, since action space noise is completely independent of the current state st(notice that this is equally true for correlated action space noise). In contrast, if the parameters of the policy are perturbed at the beginning of each episode, we get at=e(st). In this case, the same action will be taken every time the same state stis sampled in the rollout. This ensures consistency in actions, and directly introduces a dependence between the state and the exploratory action taken. Perturbing deep neural networks It is not immediately obvious that deep neural networks, with potentially millions of parameters and complicated nonlinear interactions, can be perturbed in meaningful ways by applying spherical Gaussian noise. However, as recently shown by Salimans et al. (2017), a simple reparameterization of the network achieves exactly this. More concretely, we use layer normalization (Ba et al., 2016) between perturbed layers.3Due to this normalizing across activations within a layer, the same perturbation scale can be used across all layers, even though different layers may exhibit different sensitivities to noise. Adaptive noise scaling Parameter space noise requires us to pick a suitable scale . This can be problematic since the scale will strongly depend on the speciÔ¨Åc network architecture, and is likely to vary over time as parameters become more sensitive to noise as learning progresses. Additionally, while it is easy to intuitively grasp the scale of action space noise, it is far harder to understand the scale in parameter space. We propose a simple solution that resolves all aforementioned limitations in an easy and straightforward way. This is achieved by adapting the scale of the parameter space noise over time and relating it to the variance in action space that it induces. More concretely, we can deÔ¨Åne a distance measure between perturbed and non-perturbed policy in action space and adaptively increase or decrease the parameter space noise depending on whether it is below or above a certain threshold: k+1=kifd(;e), 1 kotherwise,(1) where2R>0is a scaling factor and 2R>0a threshold value. The concrete realization of d(;) depends on the algorithm at hand and we describe appropriate distance measures for DQN, DDPG, and TRPO in Appendix C. Parameter space noise for off-policy methods In the off-policy case, parameter space noise can be applied straightforwardly since, by deÔ¨Ånition, data that was collected off-policy can be used. More concretely, we only perturb the policy for exploration and train the non-perturbed network on this data by replaying it. Parameter space noise for on-policy methods Parameter noise can be incorporated in an onpolicy setting, using an adapted policy gradient, as set forth by R√ºckstie√ü et al. (2008). Policy gradient methods optimize E(;p)[R()]. Given a stochastic policy (ajs)withN (;), the expected return can be expanded using likelihood ratios and the re-parametrization trick (Kingma & Welling, 2013) as r;E[R()]1 NX i;i"T 1X t=0r;log(atjst;+i1 2)Rt(i)# (2) 3This is in contrast to Salimans et al. (2017), who use virtual batch normalization, which we found to perform less consistently 3Published as a conference paper at ICLR 2018 forNsamplesiN(0;I)andi(+i1 2;p)(see Appendix B for a full derivation). Rather than updating according to the previously derived policy gradient, we Ô¨Åx its value to 2Iand scale it adaptively as described in Appendix C. 4 E XPERIMENTS This section answers the following questions: (i)Do existing state-of-the-art RL algorithms beneÔ¨Åt from incorporating parameter space noise? (ii)Does parameter space noise aid in exploring sparse reward environments more effectively? (iii) How does parameter space noise exploration compare against evolution strategies for deep policies (Salimans et al., 2017) with respect to sample efÔ¨Åciency? Reference implementations of DQN and DDPG with adaptive parameter space noise are available online.4 4.1 C OMPARING PARAMETER SPACE NOISE TO ACTION SPACE NOISE The added value of parameter space noise over action space noise is measured on both highdimensional discrete-action environments and continuous control tasks. For the discrete environments, comparisons are made using DQN, while DDPG and TRPO are used on the continuous control tasks. Discrete-action environments For discrete-action environments, we use the Arcade Learning Environment (ALE, Bellemare et al. (2013)) benchmark along with a standard DQN implementation. We compare a baseline DQN agent with -greedy action noise against a version of DQN with parameter noise. We linearly anneal from 1:0to0:1over the Ô¨Årst 1million timesteps. For parameter noise, we adapt the scale using a simple heuristic that increases the scale if the KL divergence between perturbed and non-perturbed policy is less than the KL divergence between greedy and -greedy policy and decreases it otherwise (see Section C.1 for details). By using this approach, we achieve a fair comparison between action space noise and parameter space noise since the magnitude of the noise is similar and also avoid the introduction of an additional hyperparameter. For parameter perturbation, we found it useful to reparametrize the network in terms of an explicit policy that represents the greedy policy implied by the Q-values, rather than perturbing the Qfunction directly. To represent the policy (ajs), we add a single fully connected layer after the convolutional part of the network, followed by a softmax output layer. Thus, predicts a discrete probability distribution over actions, given a state. We Ô¨Ånd that perturbing instead ofQresults in more meaningful changes since we now deÔ¨Åne an explicit behavioral policy. In this setting, the Q-network is trained according to standard DQN practices. The policy is trained by maximizing the probability of outputting the greedy action accordingly to the current Q-network. Essentially, the policy is trained to exhibit the same behavior as running greedy DQN. To rule out this double-headed version of DQN alone exhibits signiÔ¨Åcantly different behavior, we always compare our parameter space noise approach against two baselines, regular DQN and two-headed DQN, both with -greedy exploration. We furthermore randomly sample actions for the Ô¨Årst 50thousand timesteps in all cases to Ô¨Åll the replay buffer before starting training. Moreover, we found that parameter space noise performs better if it is combined with a bit of action space noise (we use a -greedy behavioral policy with = 0:01 for the parameter space noise experiments). Full experimental details are described in Section A.1. We chose 21 games of varying complexity, according to the taxonomy presented by (Bellemare et al., 2016). The learning curves are shown in Figure 1 for a selection of games (see Appendix D for full results). Each agent is trained for 40 M frames. The overall performance is estimated by running each conÔ¨Åguration with three different random seeds, and we plot the median return (line) as well as the interquartile range (shaded area). Note that performance is evaluated on the exploratory policy since we are interested in its behavior especially. 4https://github.com/openai/baselines 4Published as a conference paper at ICLR 2018 50010001500returnAlien 0100200300400Amidar 0200400600BankHeist 025005000750010000BeamRider 0100200300Breakout 050010001500returnEnduro 0102030Freeway 2505007501000Frostbite 0 1 2 3 4 steps 1e720 10 01020Pong 0 1 2 3 4 steps 1e70200040006000Qbert 0 1 2 3 4 steps 1e7050100150200returnTutankham 0 1 2 3 4 steps 1e7010002000WizardOfWor 0 1 2 3 4 steps 1e702000400060008000Zaxxonparameter noise, separate policy head -greedy, separate policy head  -greedy Figure 1: Median DQN returns for several ALE environment plotted over training steps. Overall, our results show that parameter space noise often outperforms action space noise, especially on games that require consistency (e.g. Enduro, Freeway) and performs comparably on the remaining ones. Additionally, learning progress usually starts much sooner when using parameter space noise. Finally, we also compare against a double-headed version of DQN with -greedy exploration to ensure that this change in architecture is not responsible for improved exploration, which our results conÔ¨Årm. Full results are available in Appendix D. That being said, parameter space noise is unable to sufÔ¨Åciently explore in extremely challenging games like Montezuma‚Äôs Revenge. More sophisticated exploration methods like Bellemare et al. (2016) are likely necessary to successfully learn these games. However, such methods often rely on some form of ‚Äúinner‚Äù exploration method, which is usually traditional action space noise. It would be interesting to evaluate the effect of parameter space noise when combined with exploration methods. On a Ô¨Ånal note, proposed improvements to DQN like double DQN (Hasselt, 2010), prioritized experience replay (Schaul et al., 2015), and dueling networks (Wang et al., 2015) are orthogonal to our improvements and would therefore likely improve results further. We leave the experimental validation of this theory to future work. Continuous control environments We now compare parameter noise with action noise on the continuous control environments implemented in OpenAI Gym (Brockman et al., 2016). We use DDPG (Lillicrap et al., 2015) as the RL algorithm for all environments with similar hyperparameters as outlined in the original paper except for the fact that layer normalization (Ba et al., 2016) is applied after each layer before the nonlinearity, which we found to be useful in either case and especially important for parameter space noise. We compare the performance of the following conÔ¨Ågurations: (a) no noise at all, (b) uncorrelated additive Gaussian action space noise ( = 0:2), (c) correlated additive Gaussian action space noise (Ornstein‚ÄìUhlenbeck process (Uhlenbeck & Ornstein, 1930) with = 0:2), and (d) adaptive parameter space noise. In the case of parameter space noise, we adapt the scale so that the resulting change in action space is comparable to our baselines with uncorrelated Gaussian action space noise (see Section C.2 for full details). We evaluate the performance on several continuous control tasks. Figure 2 depicts the results for three exemplary environments. Each agent is trained for 1 Mtimesteps, where 1epoch consists of 10thousand timesteps. In order to make results comparable between conÔ¨Ågurations, we evaluate the performance of the agent every 10thousand steps by using no noise for 20episodes. OnHalfCheetah , parameter space noise achieves signiÔ¨Åcantly higher returns than all other conÔ¨Ågurations. We Ô¨Ånd that, in this environment, all other exploration schemes quickly converge to a local optimum (in which the agent learns to Ô¨Çip on its back and then ‚Äúwiggles‚Äù its way forward). Parameter 5Published as a conference paper at ICLR 2018 20 40 60 80 100 epoch010002000300040005000returnHalfCheetah 20 40 60 80 100 epoch250500750100012501500Hopper 20 40 60 80 100 epoch5001000150020002500Walker2dadaptive parameter noise correlated action noise uncorrelated action noise no noise Figure 2: Median DDPG returns for continuous control environments plotted over epochs. space noise behaves similarly initially but still explores other options and quickly learns to break out of this sub-optimal behavior. Also notice that parameter space noise vastly outperforms correlated action space noise on this environment, clearly indicating that there is a signiÔ¨Åcant difference between the two. On the remaining two environments, parameter space noise performs on par with other exploration strategies. Notice, however, that even if no noise is present, DDPG is capable of learning good policies. We Ô¨Ånd that this is representative for the remaining environments (see Appendix E for full results), which indicates that these environments do not require a lot of exploration to begin with due to their well-shaped reward function. 0 2000 4000 6000 8000 10000 epoch010002000300040005000returnHalfCheetah 0 2000 4000 6000 8000 10000 epoch05001000150020002500Hopper 0 2000 4000 6000 8000 10000 epoch050010001500200025003000Walker2DTRPO with parameter noise (  = 0.01)  TRPO with parameter noise (  = 0.1)  TRPO with parameter noise (  = 1.0)  TRPO Figure 3: Median TRPO returns for continuous control environments plotted over epochs. The results for TRPO are depicted in Figure 3. Interestingly, in the Walker2D environment, we see that adding parameter noise decreases the performance variance between seeds. This indicates that parameter noise aids in escaping local optima. 4.2 D OES PARAMETER SPACE NOISE EXPLORE EFFICIENTLY ? The environments in the previous section required relatively little exploration. In this section, we evaluate whether parameter noise enables existing RL algorithms to learn on environments with very sparse rewards, where uncorrelated action noise generally fails (Osband et al., 2016a; Achiam & Sastry, 2017). A scalable toy example We Ô¨Årst evaluate parameter noise on a well-known toy problem, following the setup described by Osband et al. (2016a) as closely as possible. The environment consists of a chain ofNstates and the agent always starts in state s2, from where it can either move left or right. In states1, the agent receives a small reward of r= 0:001and a larger reward r= 1 in statesN. Obviously, it is much easier to discover the small reward in s1than the large reward in sN, with increasing difÔ¨Åculty as Ngrows. The environment is described in greater detail in Section A.3. We compare adaptive parameter space noise DQN, bootstrapped DQN, and -greedy DQN. The chain length Nis varied and for each Nthree different seeds are trained and evaluated. After each episode, we evaluate the performance of the current policy by performing a rollout with all noise disabled (in the case of bootstrapped DQN, we perform majority voting over all heads). The problem is considered solved if one hundred subsequent rollouts achieve the optimal return. We plot the median number of episodes before the problem is considered solved (we abort if the problem is still unsolved after 2thousand episodes). Full experimental details are available in Section A.3. 6Published as a conference paper at ICLR 2018 20 40 60 80 100 chain length0500100015002000number of episodes Parameter space noise DQN 20 40 60 80 100 chain length Bootstrapped DQN 20 40 60 80 100 chain length -greedy DQN Figure 4: Median number of episodes before considered solved for DQN with different exploration strategies. Green indicates that the problem was solved whereas blue indicates that no solution was found within 2 Kepisodes. Note that less number of episodes before solved is better. Figure 4 shows that parameter space noise clearly outperforms action space noise (which completely fails for moderately large N) and even outperforms the more computational expensive bootstrapped DQN. However, it is important to note that this environment is extremely simple in the sense that the optimal strategy is to always go right. In a case where the agent needs to select a different optimal action depending on the current state, parameter space noise would likely work less well since weight randomization of the policy is less likely to yield this behavior. Our results thus only highlight the difference in exploration behavior compared to action space noise in this speciÔ¨Åc case. In the general case, parameter space noise does not guarantee optimal exploration. Continuous control with sparse rewards We now make the continuous control environments more challenging for exploration. Instead of providing a reward at every timestep, we use environments that only yield a non-zero reward after signiÔ¨Åcant progress towards a goal. More concretely, we consider the following environments from rllab5(Duan et al., 2016), modiÔ¨Åed according to Houthooft et al. (2016): (a) SparseCartpoleSwingup , which only yields a reward if the paddle is raised above a given threshold, (b) SparseDoublePendulum , which only yields a reward if the agent reaches the upright position, and (c) SparseHalfCheetah , which only yields a reward if the agent crosses a target distance, (d) SparseMountainCar , which only yields a reward if the agent drives up the hill, (e) SwimmerGather , yields a positive or negative reward upon reaching targets. For all tasks, we use a time horizon of T= 500 steps before resetting. 020406080returnSparseCartpoleSwingup 100200300SparseDoublePendulum 20 40 60 80 100 epoch0.00.20.40.6SparseHalfCheetah 20 40 60 80 100 epoch0.00.20.40.60.81.0returnSparseMountainCar 20 40 60 80 100 epoch0.04 0.02 0.000.020.04SparseSwimmerGatheradaptive parameter noise correlated action noise uncorrelated action noise no noise Figure 5: Median DDPG returns for environments with sparse rewards plotted over epochs. We consider both DDPG and TRPO to solve these environments (the exact experimental setup is described in Section A.2). Figure 5 shows the performance of DDPG, while the results for TRPO have been moved to Appendix F. The overall performance is estimated by running each conÔ¨Åguration with 5https://github.com/openai/rllab 7Published as a conference paper at ICLR 2018 Ô¨Åve different random seeds, after which we plot the median return (line) as well as the interquartile range (shaded area). For DDPG, SparseDoublePendulum seems to be easy to solve in general, with even no noise Ô¨Ånding a successful policy relatively quickly. The results for SparseCartpoleSwingup andSparseMountainCar are more interesting: Here, only parameter space noise is capable of learning successful policies since all other forms of noise, including correlated action space noise, never Ô¨Ånd states with nonzero rewards. For SparseHalfCheetah , DDPG at least Ô¨Ånds the non-zero reward but never learns a successful policy from that signal. On the challenging SwimmerGather task, all conÔ¨Ågurations of DDPG fail. Our results clearly show that parameter space noise can be used to improve the exploration behavior of these off-the-shelf algorithms. However, it is important to note that improvements in exploration are not guaranteed for the general case. It is therefore necessary to evaluate the potential beneÔ¨Åt of parameter space noise on a case-by-case basis. 4.3 I SRL WITH PARAMETER SPACE NOISE MORE SAMPLE -EFFICIENT THAN ES? Evolution strategies (ES) are closely related to our approach since both explore by introducing noise in the parameter space, which can lead to improved exploration behavior (Salimans et al., 2017).6However, ES disregards temporal information and uses black-box optimization to train the neural network. By combining parameter space noise with traditional RL algorithms, we can include temporal information as well rely on gradients computed by back-propagation for optimization while still beneÔ¨Åting from improved exploratory behavior. We now compare ES and traditional RL with parameter space noise directly. We compare performance on the 21ALE games that were used in Section 4.1. The performance is estimated by running 10episodes for each seed using the Ô¨Ånal policy with exploration disabled and computing the median returns. For ES, we use the results obtained by Salimans et al. (2017), which were obtained after training on 1 000 M frames. For DQN, we use the same parameter space noise for exploration that was previously described and train on 40 M frames. Even though DQN with parameter space noise has been exposed to 25times less data, it outperforms ES on 15out of 21Atari games (full results are available in Appendix D). Combined with the previously described results, this demonstrates that parameter space noise combines the desirable exploration properties of ES with the sample efÔ¨Åciency of traditional RL. 5 R ELATED WORK The problem of exploration in reinforcement has been studied extensively. A range of algorithms (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Auer et al., 2008) have been proposed that guarantee near-optimal solutions after a number of steps that are polynomial in the number of states, number of actions, and the horizon time. However, in many real-world reinforcements learning problems both the state and action space are continuous and high dimensional so that, even with discretization, these algorithms become impractical. In the context of deep reinforcement learning, a large variety of techniques have been proposed to improve exploration (Stadie et al., 2015; Houthooft et al., 2016; Tang et al., 2016; Osband et al., 2016a; Ostrovski et al., 2017; Sukhbaatar et al., 2017; Osband et al., 2016b). However, all are non-trivial to implement and are often computational expensive. The idea of perturbing the parameters of a policy has been proposed by R√ºckstie√ü et al. (2008) for policy gradient methods. The authors show that this form of perturbation generally outperforms random exploration and evaluate their exploration strategy with the REINFORCE (Williams, 1992a) and Natural Actor-Critic (Peters & Schaal, 2008) algorithms. However, their policies are relatively lowdimensional compared to modern deep architectures, they use environments with low-dimensional state spaces, and their contribution is strictly limited to the policy gradient case. In contrast, our 6To clarify, when we refer to ES in this context, we refer to the recent work by Salimans et al. (2017), which demonstrates that deep policy networks that learn from pixels can be trained using ES. We understand that there is a vast body of other work in this Ô¨Åeld (compare section 5). 8Published as a conference paper at ICLR 2018 method is applied and evaluated for both on and off-policy setting, we use high-dimensional policies, and environments with large state spaces. Our work is also closely related to evolution strategies (ES, Rechenberg & Eigen (1973); Schwefel (1977)), and especially neural evolution strategies (NES, Sun et al. (2009a;b); Glasmachers et al. (2010a;b); Schaul et al. (2011); Wierstra et al. (2014)). In the context of policy optimization, our work is closely related to Kober & Peters (2008) and Sehnke et al. (2010). More recently, Salimans et al. (2017) showed that ES can work for high-dimensional environments like Atari and OpenAI Gym continuous control problems. However, ES generally disregards any temporal structure that may be present in trajectories and typically suffers from sample inefÔ¨Åciency. Bootstrapped DQN (Osband et al., 2016a) has been proposed to aid with more directed and consistent exploration by using a network with multiple heads, where one speciÔ¨Åc head is selected at the beginning of each episode. In contrast, our approach perturbs the parameters of the network directly, thus achieving similar yet simpler (and as shown in Section 4.2, sometimes superior) exploration behavior. Concurrently to our work, Fortunato et al. (2017) have proposed a similar approach that utilizes parameter perturbations for more efÔ¨Åcient exploration. 6 C ONCLUSION In this work, we propose parameter space noise as a conceptually simple yet effective replacement for traditional action space noise like -greedy and additive Gaussian noise. This work shows that parameter perturbations can successfully be combined with contemporary onand off-policy deep RL algorithms such as DQN, DDPG, and TRPO and often results in improved performance compared to action noise. Experimental results further demonstrate that using parameter noise allows solving environments with very sparse rewards, in which action noise is unlikely to succeed. Our results indicate that parameter space noise is a viable and interesting alternative to action space noise, which is still the de facto standard in most reinforcement learning applications. 