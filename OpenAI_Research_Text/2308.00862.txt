arXiv:2308.00862v2  [cs.CY]  3 Aug 2023Conﬁdence-Building Measures for Artiﬁcial Intelligence: Workshop Proceedings Sarah Shoker1*, Andrew Reddie2**, Sarah Barrington2, Ruby Booth3, Miles Brundage1, Husanjot Chahal1, Michael Depp4, Bill Drexel4, Ritwik Gupta2, Marina Favaro5, Jake Hecla2, Alan Hickey1, Margarita Konaev6, Kirthi Kumar2, Nathan Lambert7, Andrew Lohn6, Cullen O’Keefe1, Nazneen Rajani7, Michael Sellitto5, Robert Trager8, Leah Walker2, Alexa Wehsener9, Jessica Young10 1OpenAI,2University of California, Berkeley ,3Berkeley Risk and Security Lab, 4Center for a New American Security ,5Anthropic,6Center for Security and Emerging Technology , 7Hugging Face,8Centre for the Governance of AI,9Institute for Security and Technology , 10Microsoft August 2023 Abstract Foundation models could eventually introduce several path ways for undermining state security: accidents, inadvertent escalation, unintentional conﬂict, the prolifera tion of weapons, and the interference with human diplomacy are just a few on a long list. The Conﬁdence-Building Measure s for Artiﬁcial Intelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley Risk and Securit y Lab at the University of California brought together a multistakeholder group to think through the tools and stra tegies to mitigate the potential risks introduced by foundation models to international security . Originating in the Cold War, conﬁdence-building measures (CBMs) are actions that reduce hostility , prevent conﬂict escalat ion, and improve trust between parties. The ﬂexibility of CBMs make them a key instrument for navigating the rapid chan ges in the foundation model landscape. Participants identiﬁed the following CBMs that directly apply to fo undation models and which are further explained in this conference proceedings: 1. crisis hotlines 2. inciden t sharing 3. model, transparency , and system cards 4. content provenance and watermarks 5. collaborative red tea ming and table-top exercises and 6. dataset and evaluation sharing. Because most foundation model developers a re non-government entities, many CBMs will need to involve a wider stakeholder community . These measures ca n be implemented either by AI labs or by relevant government actors. All authors provided substantive contributions to the pape r through sharing their ideas as participants in the worksho p, writing the paper, and/or editorial feedback and direction. The ﬁrst two authors ar e listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remai n anonymous. The claims in this paper do not represent the vie ws of any author’s organization. For questions about this paper, contact Sara h Shoker at sshoker@openai.com and Andrew Reddie at areddie @berkeley.edu. *Signiﬁcant contribution, including writing, providing de tailed input for the paper, research, workshop organizatio n, and setting the direction of the paper. **Signiﬁcant contribution, including providing detailed in put for the paper, research, workshop organization, and set ting the direction of the paper. 11 Introduction Foundation models could eventually introduce several oppo rtunities for undermining state security: accidents, inad vertent escalation, unintentional conﬂict,1the proliferation of weapons,2and the interference with human diplomacy are just a few on a long list.3Meanwhile, new defense and security actors continue to deve lop foundation model capabilities,4increasing the risk of an international crisis even further . TheConﬁdence-Building Measures for Artiﬁcial Intelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley Risk and Security Lab (BRSL) at the University o f California brought together participants from AI labs, government, academia, and civil society to propose tools an d strategies to mitigate the potential risks introduced by foundation models to international security . By foundatio n models, we mean models that use vast amounts of data, self-supervision and deep learning methods which “can be ad apted...to a wide range of downstream tasks.”5The workshop included a mix of presentations and breakout group s, where participants had the opportunity to design possible conﬁdence-building measures (CBMs). Together, p articipants identiﬁed the following CBMs that directly apply to foundation models: • crisis hotlines • incident sharing • model, transparency , and system cards • content provenance and watermarks • collaborative red teaming exercises • table-top exercises • dataset and evaluation sharing Popularized during the Cold War, CBMs represent “measures t hat address, prevent, or resolve uncertainties among states. Designed to prevent the escalation of hostilities a nd build mutual trust among erstwhile adversaries, CBMs can be formal or informal, unilateral, bilateral, or multil ateral,[such as]military or political, and can be state-to-state or non-governmental.”6Because states do not have perfect information about the cap abilities or intentions of their allies and adversaries, formal and informal rules can estab lish predictability around state behavior, which in turn has the potential to reduce misunderstandings and miscommu nications between state governments. This is in the interest of all parties. 1. In this context, accidents occur when AI systems malfunct ion. Inadvertent escalation happens due to inappropriate u se of AI systems by leaders or operators that intensify situations. Uninten tional conﬂict occurs when uncertainties in algorithm beha vior hinder the ability of states to signal effectively to adversaries, potentially i ncreasing the likelihood of conﬂict despite the ultimate in tentions of involved states. Michael C. Horowitz and Lauren Kahn, “Leading in Artiﬁcial I ntelligence through Conﬁdence Building Measures,” The Washington Quarterly 44, no. 4 (October 2021): 91–106, ISSN: 0163-660X, accessed July 17, 2023, https: //doi.org/10.1080/0163660X.2021.2018794 2.GPT-4 System Card , March 2023. 3. Alexander Ward, Matt Berg, and Lawrence Ukenye, Shaheen to Admin: Get Me the Black Sea Strategy , https://www.politico.com /newsletters/national-security-daily /2023/03/21/shaheen-to-admin-get-me-the-black-sea-strategy-0008 8048, July 2023, accessed July 17, 2023. 4.Palantir Artiﬁcial Intelligence Platform , https://www.palantir.com /platforms/aip/, accessed July 17, 2023; Donovan: AI-powered DecisionMaking for Defense. | Scale AI , https://scale.com/donovan, accessed July 17, 2023; Dan Milmo and Alex Hern, “UK to Invest £900m in Supercomputer in Bid to Build Own ‘BritGPT’,” The Guardian , March 2023, chap. Technology, ISSN: 0261-3077, accessed July 17, 2023; Jeffrey Ding and Jenny Xiao, “Recent Trends in China’s Large Language Model Landscape,” Centre for the Governance of AI , April 2023, accessed July 17, 2023. 5. Rishi Bommasani et al., On the Opportunities and Risks of Foundation Models , arXiv:2108.07258, July 2022, accessed July 17, 2023, https://doi.org/10.48550/arXiv.2108.07258, arXiv: 2108.07258 [cs] . 6.Conﬁdence-Building Measures | Cross-Strait Security Init iative | CSIS , https://www.csis.org/programs/international-securityprogram/isp-archives/asia-division/cross-strait-security-1, accessed July 17, 2023. 2Historical examples of CBMs include direct call lines betwe en countries to communicate during nuclear crises, reporting on weapon transfers between states, inviting obser vers to witness military exercises that an outside nation might otherwise construe as threatening, establishing cle ar “rules of the road” for how adversarial navies should interact on the high seas in peacetime, data exchanges on tro op movements such as those mandated by the Treaty on Conventional Forces in Europe, or on-site monitoring of tec hnology capabilities. In contrast to domestic or regional AI regulations that govern the relationship between compan ies and consumers, CBMs target and address the risks associated with state-to-state interactions by introduci ng predictability into a typically opaque international en vironment. While CBMs can target the prevention of a range of ha rms, workshop participants focused on CBMs that mitigate human rights abuses, the proliferation of unconve ntional weapons, and escalation due to misperceptions exacerbated by foundation models. Defense strategies now routinely address the risks and oppo rtunities associated with artiﬁcial intelligence, with so me governments and think tanks calling explicitly for conﬁden ce-building measures.7Yet with the notable exception of the United Kingdom’s Integrated Review Refresh 2023, most g overnments have not fully grappled with the implications of military AI, much less foundation models.8Though many existing defense documents do not directly targ et foundation models, governments can still fold the CBMs iden tiﬁed in these proceedings into existing AI commitments, such as the U.S Government’s Political Declaration o n Responsible Military Use of Artiﬁcial Intelligence and Autonomy .9 Building on the literature addressing the risks of AI to inte rnational security , this workshop focused on generating practical CBMs that apply to foundation models. The CBMs ide ntiﬁed in these proceedings are not exhaustive or equally feasible in today’s international climate. Where a ppropriate, we outline political and technical limitation s that could interfere with the CBM’s success. 1.1 A Note on Terminology: Foundation Models, Generative AI , and Large Language Models For the sake of brevity , we use the term ‘foundation model’ to refer to both base and ﬁne-tuned models, generative AI, and large language models. Where appropriate, we identi fy the speciﬁc type of AI model the CBM is meant to address. The terms foundation model, large language model, and generative AI are often used interchangeably , but there are signiﬁcant, if imprecise, differences between th ese terms. As Helen Toner notes, these terms do not have “crisp boundaries... [but]...have emerged as attempts to point to a cluster of research directions and AI systems that have become especially noteworthy in recent years.”10 Foundation models are built using deep learning and self-su pervision learning methods and use vast amounts of data which, according to a 2022 paper by Rishi Bommasani et al. at S tanford University , “can be adapted (e.g. ﬁne-tuned) to a wide range of downstream tasks.”11The large amount of data and computational power used to trai n foundation models have led to impressive improvements across a variety of domains.12 While foundation models are often associated with generati ve AI applications like language and imagery (see below), these models can also be applied to domains such as robotics, human-machine interaction, reasoning, and sentiment analysis. On the other hand, generative AI is a narrower cate gory of AI that includes models and algorithms capable of generating media. These models produce content like text , audio, imagery , and software code. Many publicfacing models that are available today have already been ﬁne -tuned on a foundation model. For example, ChatGPT models are ﬁne-tuned on foundation models called GPT-3.5 an d GPT-4, while Stability AI uses foundation models 7.Chapter 4 NSCAI Final Report , technical report (National Security Commision on Artiﬁci al Intelligence), accessed July 17, 2023. 8. Page 56 Rishi Sunak, Integrated Review Refresh 2023 , UK HM Government Report (HM Government, March 2023), 56 9. Bureau of Arms Control, Political Declaration on Responsible Military Use of Artiﬁ cial Intelligence and Autonomy United States Department of State , technical report (U.S. Department of State, February 2023 ), accessed July 17, 2023. 10. Helen Toner, What Are Generative AI, Large Language Models, and Foundati on Models? , May 2023, accessed July 18, 2023. 11. Bommasani et al., On the Opportunities and Risks of Foundation Models . 12.ibid. 3like StableLM to generate imagery . 1.2 Why do we need conﬁdence-building measures for foundati on models? There is no shortage of historical crises where mispercepti on or miscommunication led to military escalation that neither side wanted.13Misperception plays a prominent causal role in the bloodies t wars of the 20th century , whether that be in both World Wars, Cold War ‘proxy’ conﬂicts like Vie tnam, Korea, and Afghanistan, or more recent 21st century conﬂicts like the Second Gulf War and Syrian Civil Wa r. There are ample cases of militaries mistakenly targeting civilian planes and killing most or all civilians onboard,14and there are numerous historical false positives that only narrowly avoided nuclear exchange.15 The ﬂexibility of CBMs make them a key instrument for navigat ing the rapid changes in the foundation model landscape. AI is a general purpose “enabling technology” rather than a military technology in and of itself.16For instance, current rule-making at the United Nations Conventi on on Certain Conventional Weapons (UN CCW) focuses on weapons identiﬁed by the forum,17which excludes many AI applications–such as generative AI– that are not obviously categorized as a ‘weapon’ but that can neverthele ss inﬂuence the direction of international conﬂict. In particular, their non-binding, build-as-you-go nature al lows the CBMs to grow in speciﬁcity as the technology necessarily evolves. This is essential, since it is not obvious what capabilities foundation models possess after they are trained and new capabilities are often revealed only after f urther red teaming and conducting safety evaluations. Though several benchmarks exist for assessing foundation m odels, they overwhelmingly point to rapid improvement in domain knowledge and deduction.18These capabilities are already associated with internatio nal security risks like providing information on the construction of conventional and unconventional weapons.19 CBMs do not overrule or subvert important efforts at fora lik e the United Nations and can act as an accompaniment to ongoing international regulatory discussions. CBMs are , however, uniquely equipped to target risks associated with foundation models due to the speed of their innovation a nd proliferation. In comparison to formal rules or international treaties, CBMs can lower coordination costs (such as time and money spent on bargaining) by reducing the number of negotiating parties involved in discussions. CBMs are often voluntary , which can incentivize participation from parties who are reluctant to risk the full weight of their national credibility on formal treaties. CBMs are more easily modiﬁed (and discarded).20CBMs can also ‘start small’ and build into formal rules, an es pecially useful feature in a low-trust international environment.21 Model performance and model safety are also separate resear ch pursuits, meaning that the performance of foundation models can improve with little change to their safety proﬁle. A large language model that can generate 13. Misperception continues to be a popular research area fo r scholars of military conﬂict, and some researchers sugges t that the academic existence of international relations is fundamentally lin ked to managing problems related to information asymmetry a nd the anarchical conditions that make misperception possible. 14. Ron DePasquale, “Civilian Planes Shot Down: A Grim Histo ry,”The New York Times , January 2020, chap. World, ISSN: 0362-4331, accessed July 18, 2023. 15. For a full list of nuclear false alarms, please visit comp endium of events. Close Calls with Nuclear Weapons , technical report (Union of Concerned Scientists, January 2015), accessed July 18, 202 3 16. Page 6 Iona Puscas, “Conﬁdence-Building Measures for Ar tiﬁcial Intelligence: A Framing Paper,” United Nations Institute for Disarmament Research , 2022, accessed July 17, 2023 17.The Convention on Certain Conventional Weapons – UNODA , technical report (United Nations Ofﬁce for Diasmament Aff airs), accessed July 18, 2023. 18. Dheeru Dua et al., DROP: A Reading Comprehension Benchmark Requiring Discret e Reasoning Over Paragraphs , arXiv:1903.00161, April 2019, accessed July 18, 2023, https: //doi.org/10.48550/arXiv.1903.00161, arXiv: 1903.00161 [cs] ; Dan Hendrycks, Measuring Massive Multitask Language Understanding , July 2023, accessed July 18, 2023; Papers with Code MMLU Benchmark (Multi-task Language Unde rstanding), https://paperswithcode.com /sota/multi-task-language-understanding-on-mmlu, accessed J uly 18, 2023. 19. Page 12 GPT-4 System Card 20. Michael C. Horowitz, Lauren Kahn, and Casey Mahoney, “Th e Future of Military Applications of Artiﬁcial Intelligenc e: A Role for Conﬁdence-Building Measures?,” Orbis 64, no. 4 (January 2020): 528–543, ISSN: 0030-4387, accessed July 18, 2023, https://doi.org/10.1016/j.orbis.2020.08.003. 21. Horowitz and Kahn, “Leading in Artiﬁcial Intelligence t hrough Conﬁdence Building Measures.” 4information about nuclear physics is an example of a capabil ity , while a large language model that refuses a user request to output speciﬁc details about bomb-building is an example of a safety mitigation. To date, AI labs have tackled the gap between model performance and safety by inve sting in a range of sociotechnical measures. Such measures include research into interpretability and align ment,22public disclosure of risks through system cards23 and transparency notes,24delaying the release of models until sufﬁcient safety mitig ations have been implemented,25 and open-sourcing evaluations26and provenance research.27Despite these efforts, the machine learning community is in general consensus that harm mitigations need furt her improvement to keep up with the rapidly increasing performance of LLMs.28 This landscape is further challenged by typical state behav ior at the international level. States are often reluctant t o engage in cooperative security agreements that require too much transparency into national capabilities. They are even less likely to place limits on the development of their o wn capabilities in the absence of any guarantees that their adversaries will do the same.29However, because performance and safety research are two di fferent research streams, it is possible to coordinate on security while limi ting availability of research into performance improvemen ts. This unintended silver-lining is known to AI labs, which is w hy commercial labs are often willing to open-source safety research into evaluations and provenance technologies. 1.3 An Overview of CBMs for Foundation Models Drawing from the list published by the United Nations Ofﬁce o f Disarmament Affairs, these proceedings organize CBMs under four categories: communication and coordinatio n, observation and veriﬁcation, cooperation and integration, and transparency .30These categories are not discrete; many CBMs can comfortabl y ﬁt into more than one category . 22. Jeff Wu et al., Recursively Summarizing Books with Human Feedback , arXiv:2109.10862, September 2021, accessed July 18, 2023 , https://doi.org/10.48550/arXiv.2109.10862, arXiv: 2109.10862 [cs] ; Steven Bills et al., Language Models Can Explain Neurons in Language Models , OpenAI, May 2023; Yuntao Bai et al., Constitutional AI: Harmlessness from AI Feedback , arXiv:2212.08073, December 2022, accessed July 18, 2023, https: //doi.org/10.48550/arXiv.2212.08073, arXiv: 2212.08073 [cs] ; Nelson Elhage et al., Toy Models of Superposition , arXiv:2209.10652, September 2022, accessed July 18, 2023 , https://doi.org/10.48550/arXiv.2209.10652, arXiv: 2209.10652 [cs] ; Rohin Shah et al., Goal Misgeneralization: Why Correct Speciﬁcations Aren’t Enough For Correct Goals , arXiv:2210.01790, November 2022, accessed July 18, 2023, https: //doi.org/10.48550/arXiv.2210.01790, arXiv: 2210.01790 [cs] ; Denny Zhou et al., Least-to-Most Prompting Enables Complex Reasoning in Larg e Language Models , arXiv:2205.10625, April 2023, accessed July 18, 2023, https://doi.org/10.48550/arXiv.2205.10625, arXiv: 2205.10625 [cs] ; Daniel Ziegler et al., “Adversarial Training for High-Sta kes Reliability,”Advances in Neural Information Processing Systems 35 (December 2022): 9274–9286, accessed July 18, 2023; Mich ael K. Cohen, Marcus Hutter, and Michael A. Osborne, “Advanced Artiﬁcial Agents Intervene in the Provision of Reward,” AI Magazine 43, no. 3 (2022): 282–293, ISSN: 2371-9621, accessed July 18, 2023, https: //doi.org/10.1002/aaai.12064. 23.GPT-4 System Card . 24. ChrisHMSFT, Transparency Note for Azure OpenAI Azure Cognitive Servic es, https://learn.microsoft.com /en-us/legal/cognitiveservices/openai/transparency-note, May 2023, accessed July 18, 2023. 25. Page 19 GPT-4 System Card andMicrosoft Turing Academic Program (MS-TAP) , accessed July 18, 2023 26.Overview C2PA , https://c2pa.org/, accessed July 18, 2023; Paul England et al., AMP: Authentication of Media via Provenance , arXiv:2001.07886, June 2020, accessed July 18, 2023, https ://doi.org/10.48550/arXiv.2001.07886, arXiv: 2001.07886 [cs, eess] ; Evals , OpenAI, July 2023, accessed July 18, 2023. 27. For example, as part of its Content Authenticity Initiat ive (CAI), Adobe open-sourced JavaScript SDK and Rust SDK, w hich is “designed to let developers build functions displaying con tent credentials in browsers, or make custom desktop and mob ile apps that can create, verify and display content credentials.”Leigh Mc Gowran, Adobe Launches Open-Source Tools to Tackle Visual Misinfor mation , https://www.siliconrepublic.com /enterprise/adobe-digital-misinformation-cai-developer-tools, Ju ne 2022, accessed July 18, 2023 28.Core Views on AI Safety: When, Why, What, and How , https://www.anthropic.com /index/core-views-on-ai-safety, accessed July 28, 2023; Jan Leike, John Schulman, and Jeffrey Wu, Our Approach to Alignment Research , https://openai.com/blog/our-approach-to-alignmentresearch, August 2022, accessed July 28, 2023; Amelia Glaes e et al., Improving Alignment of Dialogue Agents via Targeted Human J udgements , arXiv:2209.14375, September 2022, accessed July 28, 2023, https://doi.org/10.48550/arXiv.2209.14375, arXiv: 2209.14375 [cs] . 29. For example, France’s Defence Ethics Committee advised the continuation of research into autonomy and weapons syst ems, citing, among other reasons, the need to “counter enemy development of LAW S; and. . . to be able to defend ourselves against this type of w eapon in the likely event of their use by an enemy State or terrorist group against our troops or population.” Opinion On The Integration Of Autonomy Into Lethal Weapon Systems , technical report (Ministére Des Armées Defence Ethics Com mittee, April 2021), 5–6 30.Repository of Military Conﬁdence-Building Measures – UNOD A, accessed July 18, 2023. 5Because most foundation model developers are non-governme nt entities, many CBMs will need to involve a wider stakeholder community . These measures can be implemented e ither by AI labs or by relevant government actors.31 Throughout the paper, we provide examples of adjacent techn ologies that have contributed to international crises, with the understanding that these examples can help us bette r anticipate the risks posed by foundation models, which are currently nascent or empirically unconﬁrmed. 1.4 Communication and Coordination Communication and coordination CBMs reduce misunderstand ings and misperceptions that, if left unaddressed, could escalate into conﬂict. The workshop identiﬁed two com munication and coordination challenges that could be remedied using communication and coordination CBMs: mis perceptions about authenticity of the content, and misperceptions concerning who authorized a decision. First, on the topic of content authenticity , several worksh op participants reiterated that foundation models and, speciﬁcally , generative AI, can be used to perpetuate ‘trut h decay ,’ or increased public distrust towards the informat ion reported by political leaders and other experts. That distr ust, in turn, complicates reporting on international event s and crises.32For example, in March 2022, a widely circulated deepfake vid eo on social media showed Ukrainian President Volodymyr Zelenskyy instructing soldiers to sur render to Russian forces.33Individuals may soon speak with interactive deepfakes, where the deepfake is both able to pause appropri ately for the other speaker and use predictive modeling and synthetic audio to carry on a conver sation.34And we could see the use of compositional deepfakes–not just one fake video or image, but many of them– released over time in between real events, to create a synthetic history that seems believable.35 Second, strong communication and coordination CBMs allow h uman actors to account for the ambiguity an AI injects into a system or team.36AI systems are often designed with the intention of supporti ng or augmenting human decision-making, making it challenging to disentangle the contributions of human operators. AI systems may also generate outputs or decisions that can be misinterpreted or misunderstood by human operators or other AI systems; in some cases the integration of AI in human-machine teams37can obfuscate whether AI was the (inadvertent, accidental, or intentional) cause of military escalation. A cas e in point is the 1988 tragedy of Iran Air Flight 655, which was targeted by an Aegis cruiser–the most sophisticated ant i-aircraft weapon system at the time–on the order of the USS Vincennes, killing 290 civilians. The accident was b lamed on a number of factors: the Aegis incorrectly identiﬁed the commercial airliner as a military aircraft; t he commander of the Vincennes was characterized as being unnecessarily aggressive in a high-pressure atmosphere pr one to misinterpretation; a nearby US navy ship, the USS Sides, had a human commander who correctly deduced that Iran Air Flight 655 was a civilian aircraft, but believed the Aegis’s identiﬁcation system to be technologically sup erior to his own human judgement and did not share his assessment with the Vincennes.38The Aegis radar system did eventually identify the Iran Air F light 655 as a civilian 31. We opted to exclude a discussion on cyber risks from the sc ope of this paper since legal advances published in the Talli nn Manuals, NATO announcements on what counts as ‘cyber war,’ and norm settin g at the UN Group of Governmental Experts on state behavior in cyberspace means that the topic deserves its own devoted forum. 32. Josh A. Goldstein et al., Generative Language Models and Automated Inﬂuence Operati ons: Emerging Threats and Potential Mitigations , arXiv:2301.04246, January 2023, accessed July 18, 2023, ht tps://doi.org/10.48550/arXiv.2301.04246, arXiv: 2301.04246 [cs] ; Philip Oltermann, “European Politicians Duped into Deepfake Vide o Calls with Mayor of Kyiv,” The Guardian , June 2022, chap. World news, ISSN: 0261-3077, accessed July 18, 2023. 33. Bobby Allyn, “Deepfake Video of Zelenskyy Could Be ’tip o f the Iceberg’ in Info War, Experts Warn,” NPR, March 2022, chap. Technology, accessed July 18, 2023. 34. Eric Horvitz, “On the Horizon: Interactive and Composit ional Deepfakes,” in INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (November 2022), 653–661, accessed July 18, 2023, https: //doi.org/10.1145/3536221.3558175, arXiv: 2209.01714 [cs] . 35.ibid. 36. Horowitz and Kahn, “Leading in Artiﬁcial Intelligence t hrough Conﬁdence Building Measures.” 37. We deﬁne ‘human machine’ team as “a relationship–one mad e up of at least three equally important elements: the human, the machine, and the interactions and interdependen cies between them.” Building Trust in Human-Machine Teams , https://www.brookings.edu /articles/building-trust-in-human-machine-teams /, accessed July 18, 2023 38.H-020-1: USS Vincennes Tragedy , http://public1.nhhcaws.local /content/history/nhhc/about-us/leadership/director/directors-corner/hgrams/h-gram-020/h-020-1-uss-vincennes-tragedy–.html, accessed July 28, 2023; Formal Investigation into the Circumstances Surrounding t he 6aircraft, but the human operators chose to accept the ﬁrst re ading. The Iran Air Flight 655 accident features many of the challenges that exist in today’s human-machine teams: o vertrust and a reluctance to contest the decisions made by the system, misunderstanding the threat due to existing g eopolitical hostilities, and cherry-picking evidence to support one’s interpretation of events. The introduction o f AI to this atmosphere, which promises to increase the speed of targeting and analysis using a black-boxed technol ogy , makes it even more necessary to identify communication pathways to prevent accidents. Hotlines The ability to interpret human intentions can become more ch allenging when communication integrates with or is supplanted by a non-human entity . Hotlines can assist with c larifying the ‘who’ or ‘what’ was responsible for military escalation, and for clarifying red lines39to avoid crossing them in the ﬁrst place.40Workshop participants noted that competitor states could establish communication link s to reduce friction during political crises, building on state-to-state hotlines that exist today for the managemen t of military crises. Despite their prominent role in mitigating nuclear crises, recent political events have underscored the reality that security norms will inform when parties use–or refuse–a pho ne call. This point was made especially evident during the February 2023 crisis involving a Chinese spy balloon tra veling across the United States, and the subsequent refusal by the People’s Liberation Army (PLA) to answer a hotline cal l from U.S Defense Secretary Lloyd Austin. Immediately following the crisis, researchers offered several explana tions for the PLA’s behavior that pointed to a discrepancy between how both military powers interpreted the threat lan dscape. Some stated that the PLA viewed CBMs and transparency as “disadvantageous” and a normalization “of increasingly brazen behavior.” Another researcher stated that U.S military norms prize the avoidance of military esca lation, while “[i]n the Chinese system, the impulse is to not be blamed for a mistake” or to be the person who reports the message to their political or military leaders.41It is worth noting that hotline usage becomes even more complicat ed in a world with three major military powers, where incentives could exist for one actor to exploit crisis commu nication between the two other states. The successful use of hotlines may require that parties shar e common values about the risks of foundation models and a mutual belief that CBMs reduce the risk of unnecessary m ilitary escalations. States routinely disagree about the severity of threats and will pursue technologies to keep their own borders safe, even at the expense of global security . Other CBMs listed in this document, such as collab orative red teaming, emergency-response tabletop games, and incident sharing, can supply the necessary data for asse ssing the risk landscape while reinforcing the idea that CBMs will not undermine any single state’s security . As an ar ea of future study , participants recommended research on understanding policymaker perceptions about foundatio n model risks to international security and ensuring that incentives for participating in CBMs address country-spec iﬁc social values. Incident Sharing Incident-sharing is a common practice across sectors where public safety is paramount, such as electric vehicles, cybersecurity , aviation, and healthcare. Information shari ng about security incidents or ‘near misses’ is used to impro ve safety and reduce the likelihood of new accidents. With rega rds to autonomy in military systems, Michael Horowitz and Paul Scharre have previously suggested that an “‘intern ational autonomous incidents agreement’ that focuses on military applications of autonomous systems, especiall y in the air and maritime environments...would reduce risks from accidental escalation by autonomous systems, as well as reduce ambiguity about the extent of human intention behind the behavior of autonomous systems”.42This problem is documented in technology modernization Downing of Iran Air Flight 655 on 3 July 1988 , Investigation Report 93-FOI-0184 (U.S. Department of Def ense, July 1988), 153. 39. Albert Wolf, Backing Down: Why Red Lines Matter in Geopolitics , https://mwi.westpoint.edu /geopolitical-costs-red-lines /, August 2016, accessed July 18, 2023. 40. For more information on how hotlines can clarify red line s, see: Bill Whitaker, When Russian Hackers Targeted the U.S. Election Infrastruc ture, https://www.cbsnews.com /news/when-russian-hackers-targeted-the-u-s-election-infr astructure/, July 2018, accessed July 18, 2023 41. Howard LaFranchi, “US-China Conundrum: Can Hotline Dip lomacy Work If Trust Isn’t a Goal?,” Christian Science Monitor , March 2023, ISSN: 0882-7729, accessed July 18, 2023. 42. Michael C. Horowitz and Paul Scharre, AI and International Stability: Risks and Conﬁdence-Build ing Measures , https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-conﬁdencebuilding-measures, January 2021, accessed 7for defense. For example, the introduction of touchscreen c ontrols to the USS John S McCain, combined with crew confusion about the different settings associated with the se controls, contributed to the largest maritime accident involving the US Navy in the last 40 years and left 10 sailors d ead.43 Open-source AI incident-sharing initiatives already exis t, such as the AI, Algorithmic, and Automation Incidents and Controversies (AIAAIC) and AI Incident Databases.44As of April 2023, these open-source databases primarily fea tured journalistic investigations, which sometimes include inc idents on generative AI and international security , like th e recent deepfake of the US Pentagon explosion.45Participants suggested a comparable database for internat ional security incidents caused by foundation models, with a poss ible focus on unusual vulnerabilities and emergent model behaviors. Workshop participants raised several questions that remai n unresolved. Namely , it is unclear which model behaviors and misuses would qualify as an “incident,” the incentives f or parties to participate in an incident-sharing agreement , and how those parties can assure accurate reporting while re specting intellectual property rights and user privacy . A distinction might exist between new and dangerous model ca pabilities versus the large-scale misuse of the model. The former category could include behaviors linked to impro vements of the model such as the ability to manipulate users or design a synthetic biochemical agent. The latter ca tegory could entail large-scale misuse campaigns, such as using models to create spam or disinformation. Other industries resolve these challenges through data ano nymization and encryption, trusted third-party agreements, access controls, NDAs, and security audits. Some typ es of incident sharing can leverage existing professional relationships between labs and could be as simple as h osting an informal meeting amongst parties. However, incident-sharing may require a multilateral entity that co ordinates incident collection across multiple parties. Wo rkshop participants noted that an AI incident-response entit y could be analogous to existing Computer Emergency Response Teams (CERT) found in the international cybersecu rity domain.46 2 Transparency AI systems may produce unintended outcomes due to biases in t raining data, algorithmic errors, or unforeseen interactions with other systems. To name a few examples: fou ndation models used to summarize ISR data can introduce artifacts into the data that impacts a military re sponse. Plausible outputs that are actually false, known as "hallucinations",47can be difﬁcult to detect in a fast-paced and high-pressure m ilitary environment. Moreover, labeling practices can contribute to bias by privileging so me worldviews over others, a serious risk for intelligence analysts conducting even routine tasks like report retriev al and summarization.48Compounding this problem is that models do not perform equally well across languages and it is seldom clear whose values should be reﬂected in model generations. Finally , prompt injection attacks, a type of d ata poisoning and security exploitation, can alter model July 18, 2023. 43.NTSB Accident Report on Fatal 2017 USS John McCain Collision off Singapore , August 2019, chap. Documents, accessed July 18, 2023. 44.AIAAIC , https://www.aiaaic.org/home, accessed July 18, 2023; AI Incidents Database , https://partnershiponai.org /workstream/aiincidents-database /, accessed July 18, 2023. 45.Incident 543: Deepfake of Explosion Near US Military Admini stration Building Reportedly Causes Stock Dip , https://incidentdatabase.ai /cite/543/, January 2020, accessed July 18, 2023. 46. For examples on national CERTs, please see: US-CERT (United States Computer Emergency Readiness Team) Glossary | CSRC , https://csrc.nist.gov/glossary/term/us_cert, accessed July 18, 2023; CERT-EU – Computer Emergency Response Team | European Union , https://european-union.europa.eu /institutions-law-budget /institutions-and-bodies /search-all-eu-institutions-and-bodies /computeremergency-response-team-eu-institutions-bodies-andagencies-cert-eu_en, accessed July 18, 2023 47. Adrian Tam, A Gentle Introduction to Hallucinations in Large Language M odels , June 2023, accessed July 18, 2023. 