Measuring the Algorithmic Efﬁciency of Neural NetworksDanny Hernandez⇤OpenAIdanny@openai.comTom B. BrownOpenAItom@openai.comAbstractThree factors drive the advance of AI: algorithmic innovation, data, and the amount ofcompute available for training. Algorithmic progress has traditionally been more difﬁcultto quantify than compute and data. In this work, we argue that algorithmic progress hasan aspect that is both straightforward to measure and interesting: reductions over timein the compute needed to reach past capabilities. We show that the number of ﬂoating-point operations required to train a classiﬁer to AlexNet-level performance on ImageNethas decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmicefﬁciency doubling every 16 months over a period of 7 years. Notably, this outpaces theoriginal Moore’s law rate of improvement in hardware efﬁciency (11x over this period).We observe that hardware and algorithmic efﬁciency gains multiply and can be on a similarscale over meaningful horizons, which suggests that a good model of AI progress shouldintegrate measures from both. ⇤Danny Hernandez led the research. Tom Brown paired on initial experiments, scoping, and debugging.Contents1 Introduction31.1 Measuring algorithmic progress in AI is critical to the ﬁeld, policymakers, and industry leaders31.2 Efﬁciency is the primary way we measure algorithmic progress on classic computer scienceproblems. We can apply the same lens to machine learning by holding performance constant32 Related Work42.1 Algorithmic progress had similar rate to Moore’s Law in some domains over decades . . . .42.2 Linear programming gains were well-deﬁned, steady, and faster than Moore’s Law for 21 years42.3 184x reduction in training cost (in dollars) to get to ResNet-50 performance since 2017 . . .52.4 We can estimate costly-to-observe algorithmic efﬁciency improvements through scaling laws52.5 Total investment in AI through private startups, public offerings, and mergers/acquisitionswent up 5x between 2012 and 2018 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63 Methods63.1 Main result primarily based on existing open source re-implementations of popular models .63.2 We made few hyperparameter adjustments between architectures and did minimal tuning . .64 Results74.1 Key Result: 44x less compute needed to get to AlexNet-level performance . . . . . . . . . .74.2 FLOPs based learning curves can help clarify comparisons between models . . . . . . . . .94.3 We observed a similar rate of progress for ResNet-50 level classiﬁcation performance andfaster rates of efﬁciency improvement in Go, Dota, and Machine Translation . . . . . . . . .95 Discussion105.1 We attribute the 44x efﬁciency gains to sparsity, batch normalization, residual connections,architecture search, and appropriate scaling . . . . . . . . . . . . . . . . . . . . . . . . . .105.2 It’s unclear the degree to which the observed efﬁciency trends generalize to other AI tasks .115.3 Why new capabilities are probably a larger portion of progress than observed efﬁciency gains125.4 We estimate a 7.5 million times increase in the effective training compute available to thelargest AI experiments between 2012 and 2018 . . . . . . . . . . . . . . . . . . . . . . . .125.5 It’s possible there’s an algorithmic Moore’s Law for optimization problems of interest . . . .145.6 Research provides leading indicators of the future economic impact of AI . . . . . . . . . .155.7 Major limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .156 Conclusion157 Acknowledgements 16A Calculations for efﬁciency improvements in Go, Dota, and Machine Translation 18B Calculations for efﬁciency improvements in image classiﬁcation 19C Accuracy achieved in relevant models 2021 Introduction1.1 Measuring algorithmic progress in AI is critical to the ﬁeld, policymakers, and industry leadersThere’s widespread agreement there’s been impressive progress in AI/ML in the domains of vision, naturallanguage, and game playing in the last decade [Krizhevsky et al., 2012, Xie et al., 2016, Silver et al., 2018].However, there’s massive disagreement as to how much progress in capabilities we should expect in the nearand long term [Grace et al., 2017]. For this reason, we believe measuring overall progress in AI/ML is acrucial question, because it can ground the discussion in evidence. Measuring AI progress is critical to poli-cymakers, economists, industry leaders, potential researchers, and others trying to navigate this disagreementand decide how much money and attention to invest in AI.For example, the compute used by the largest AI training runs per year grew at 300,000x between 2012 and2018 [Amodei & Hernandez, 2018]. Given the divergence from the past trend of approximately Moore’sLaw level growth for such training runs, [Sastry et al., 2019] suggests policymakers increase funding forcompute resources for academia, so they can continue to do the types of AI research that are becoming moreexpensive. Measurements of AI progress inform policymakers that are making such decisions.Hardware trends are relatively quantiﬁable. Moore’s Law explains much of the advance from mainframes, topersonal computers, to omnipresent smartphones [Moore, 1965]. Better measurement of scientiﬁc progresshas the potential for a lot of impact on a variety of fronts. Given the existing understanding of key hardwaretrends, we were primarily interested in measures that represented exclusively algorithmic improvement thatcould help paint a picture of the overall progress of the ﬁeld.We present measurements of algorithmic efﬁciency state of the arts over time that:1.Are informative to a wide audience of decision makers2.Help measure novel contributions produced with smaller amounts of compute1.2 Efﬁciency is the primary way we measure algorithmic progress on classic computer scienceproblems. We can apply the same lens to machine learning by holding performance constantIn a classic computer science problem like sorting, algorithmic quality is primarily measured in terms of howcost asymptotically increases with problem difﬁculty, generally denoted in Big O Notation. For example,quicksort [Hoare, 1962] hasO(nlogn)average cost in terms of operations to ﬁnd a perfect solution whereasmany sorting algorithms areO(n2)(wherenis the length of the list to be sorted). It’s impractical to performsimilar analysis for deep learning, because we’re looking for approximate solutions and don’t have as clear ameasure of problem difﬁculty. For these reasons, in machine learning, algorithmic progress is often presentedin terms of new states of the art, like a 1% absolute increase in top-5 accuracy on ImageNet, ignoring cost.It’s difﬁcult to reason about overall progress in terms of a large collection of such measures, because:1.Performance is often measured in different units (accuracy, BLEU, points, ELO, cross-entropy loss,etc) and gains on many of the metrics are hard to interpret. For instance going from 94.99% accuracyto 99.99% accuracy is much more impressive than going from 89% to 94%.2.The problems are unique and their difﬁculties aren’t comparable quantitively, so assessment requiresgaining an intuition for each problem.3.Most research focuses on reporting overall performance improvements rather than efﬁciency im-provements, so additional work is required to disentangle the gains due to algorithmic efﬁciencyfrom the gains due to additional computation.4.The benchmarks of interest are being solved more rapidly, which exacerbates 1) and 2). For instanceit took 15 years to get to human-level performance on MNIST [LeCun et al., 1998], 7 years onImageNet [Deng et al., 2009, Russakovsky et al., 2015], and GLUE [Wang et al., 2018] only lasted9 months [Devlin et al., 2018, Liu et al., 2019].We show that we can gain clear insights into efﬁciency trends by analyzing training costs while holdingperformance constant. We focused on training efﬁciency rather than inference efﬁciency, because we’re moreinterested in what systems are possible to produce than how much it costs to run those systems. Though wenote increased inference efﬁciency can have important economic implications [van den Oord et al., 2017]. Inthe research setting, we’ve typically found ourselves FLOPS bound rather than memory or communication3bound. So we measured total ﬂoating-point operations used in training rather than parameters or anothermeasure of efﬁciency.We focused on AlexNet-level performance, which we measured as 79.1% top-5 accuracy on ImageNet.AlexNet kicked off the wave of interest in neural networks and ImageNet is still a benchmark of wide in-terest, so this measure provided a long running trend to analyze.2 Related Work2.1 Algorithmic progress had similar rate to Moore’s Law in some domains over decadesGrace compared algorithmic progress to hardware progress looked at over several decades in the domains ofchess, go, physics simulations, mixed integer programming, and SAT solvers [Grace, 2013]. Grace’s overallconclusion wasMany of these areas appear to experience fast improvement, though the data are oftennoisy. For tasks in these areas, gains from algorithmic progress have been roughly ﬁfty toone hundred percent as large as those from hardware progress. Improvements tend to beincremental, forming a relatively smooth curve on the scale of yearsFor the most part, these estimates and their interpretation require substantial amounts of judgment. Forinstance, with chess and Go the approach was to use the available literature to estimate what kinds of returnscame from a hardware doubling and then attribute all ELO improvement not explained by Moore’s law tosoftware. Additionally, Grace suggests we treat these estimates as "optimistic" rather than representative,because of increased saliency of problems that are making fast progress, problems with good measures beinglikely to progress faster, and the potential motivations of authors. Regardless, we think this related workshows that hardware and algorithmic progress can be on a similar scale, and that even a relatively simplemodel of progress should consider integrating measures from both domains.Progress on mixed integer programming was particularly straightforward to measure, so we’ve extended theoriginal analysis of that domain below [Bixby, 2012].2.2 Linear programming gains were well-deﬁned, steady, and faster than Moore’s Law for 21 yearsUnlike some other optimization domains Grace looked at, linear programming was of commercial interestfor a long period. Progress is easy to track in this domain over this 21 year period because there were distinctreleases of commercial software (CPLEX and Gurobi) that can be compared with hardware held ﬁxed.The trend of a 2x speedup every 13 months observed in Figure 1 is surprisingly consistent over a long timehorizon. The smooth progress is partially explained by the measure being an aggregation of many problemsof varying difﬁculty. Over this time Moore’s Law yielded an efﬁciency gain of approximately 1500x.Caveats1.It’s notable that the benchmark was designed and the analysis was performed by the CEO of Gurobi(a commercial MIPS solver) and that he had an incentive to demonstrate large amounts of progress.2.It’s worth pointing out the implications of the maximum search time of 30,000s for the optimalsolution. When it took longer than 30,000s for the solver to ﬁnd the optimal solution, 30,000sis what would be recorded. It’s expected that the maximum search time would have been invokedmore for earlier, weaker solvers. Thus, the maximum search time made earlier solvers look relativelystronger, making the overall estimate conservative for this benchmark. We think using a maximumsearch time is reasonable, but we expect the overall speedup is sensitive to it. In this sense, thesemeasurements are a little different than the AlexNet accuracy measurements, where we waited forthe capability to be demonstrated before measuring progress.3.This is the related domain with highest amount of measured algorithmic efﬁciency progress we’reaware of for this period of time.4Cumulative speedup1101001000100001000001000000 1995200020052010500,000x Speedup in Mixed Integer Programming over 20 YearsFigure 1A 2x speedup every 13 months was observed on a benchmark of 1,892 mixed-integer problems(MIPs), a subset of linear programming. This benchmark was created by Bixby, he describes it as a set of"real-world problems that had been collected from academic and industry sources over 21 years." Progress isbased on the total time spent searching for the optimal solution for all problems in the benchmark. Progressis easy to track in this domain over this 21 year period because there were distinct releases of commercialsoftware (CPLEX and Gurobi) that can be compared with hardware held ﬁxed. A maximum search time of30,000 seconds (approximately 8 hours) per problem was used, so that’s what was recorded for instanceswhere the optimum wasn’t found. We clariﬁed the trend by graphing the trend by release date rather than byversion number [Bixby, 2012].2.3 184x reduction in training cost (in dollars) to get to ResNet-50 performance since 2017The eventual unit institutions generally care about for training cost is dollars. Earlier we observed a 10xefﬁciency improvement in terms of training FLOPs required to get ResNet-50 level accuracy (92.9% top-5accuracy target on ImageNet). On the same target, DawnBench submissions have surpassed the contest’soriginal benchmark cost, $2323, by a factor of 184x [Coleman et al., 2017]. This brought the cost of such atraining down to $12.60 in September 2017, less than a year after the competition was announced. Trainingcost in dollars is a useful overall measure, that aggregates:1.The efﬁciency gains from algorithmic progress we are most interested in within this paper.2.Moore’s Law’s effect on GPUs, TPUs, etc.3.Reduced cloud computing costs driven by modernization and increased competition.4.Hardware utilization. It’s not trivial to efﬁciently use the FLOPS capacity of GPUs, TPUs, etc.The DawnBench results make it clear that 3. and 4. can also be notable contributions to training efﬁciencythat are worth measuring. More targeted measurements, like training efﬁciency in terms of FLOPs, helpclarify the takeaway from measures like DawnBench that aggregate multiple effects.2.4 We can estimate costly-to-observe algorithmic efﬁciency improvements through scaling lawsWe’ve focused on algorithmic efﬁciency improvements that are observable empirically. [Kaplan McCandlish2020] showed that language model performance on cross-entropy had power-law scaling with the amount ofcompute over several orders of magnitude. Empirical scaling laws can be extrapolated to provide an estimateof how much we would have needed to scale up older models to reach current levels of performance. Through5this mechanism scaling laws provide insight on efﬁciency gains that may require prohibitively expensiveamounts of compute to observe directly.2.5 Total investment in AI through private startups, public offerings, and mergers/acquisitions wentup 5x between 2012 and 2018We’ve primarily considered algorithmic, hardware, and data as the inputs in progress in machine learning.Money spent would be another reasonable lens since that’s the lever available to decision-makers at thehighest level. [Bloom et al., 2017] looks into the relationship between scientiﬁc progress and spending:In many models, economic growth arises from people creating ideas, and the long-rungrowth rate is the product of two terms: the effective number of researchers and their re-search productivity... A good example is Moore’s Law. The number of researchers requiredtoday to achieve the famous doubling every two years of the density of computer chips ismore than 18 times larger than the number required in the early 1970s. Across a broadrange of case studies at various levels of (dis)aggregation, we ﬁnd that ideas – and theexponential growth they imply – are getting harder to ﬁnd. Exponential growth results fromlarge increases in research effort that offset its declining productivity.AI investment is also up substantially since 2012, and it seems likely this was important to maintainingalgorithmic progress at the observed level. [Raymond Perrault & Niebles, 2019] notes that:1.Private investment in AI startups rose from $7B in 2012 to $40B in 2018.2.Investment through public offerings and mergers/acquisitions grew from $5B in 2012 to $23B in2018.3.The DOD is projected to invest $4.0B on AI R&D in ﬁscal year 2020.4.Contract spending on AI by the US government has grown from about $150M to $728M between2012 and 2018.3 Methods3.1 Main result primarily based on existing open source re-implementations of popular modelsFor the majority of the architectures shown in Figure 3 [Szegedy et al., 2014, Simonyan & Zisserman,2014, He et al., 2015, Xie et al., 2016, Huang et al., 2016, Iandola et al., 2016, Zagoruyko & Komodakis,2016, Zhang et al., 2017, Howard et al., 2017, Sandler et al., 2018, Ma et al., 2018, Tan & Le, 2019] we usedPyTorch’s example models [Paszke et al., 2017] with Pytorch’s suggested hyperparameters. We mark ourdeviation from their hyperparameters in the next section. We supplemented PyTorch’s example models withexisting implementations of MobileNet, ShufﬂeNet [Xiao, 2017,Huang, 2017].Compute used is based on the product of the following:1.FLOPs per training image, which was counted by a PyTorch library [Zhu, 2019] that we checkedagainst other methods for several models2.The number of images per epoch3.The number of epochs it took an architecture to perform better than or equal to the AlexNet modelwe trained3.2 We made few hyperparameter adjustments between architectures and did minimal tuningWe largely followed the suggested hyperparameters from the PyTorch example models. For all points shownin ﬁgure 3 we trained using SGD with a batch size of 256, momentum of 0.9, and weight decay of 1e-4, for90 epochs. For pre-batch norm architectures, we began with the suggested learning rate of 0.01 (GoogleNetand VGG), for all other architectures we began with the suggested learning rate of 0.1.For AlexNet we followed the original paper’s learning rate schedule of decaying by a factor of 10 every30 epochs. For all other models, we followed the suggested 1000x total learning rate reduction. To sanitycheck that these were reasonable hyperparameters, we performed a scan on ResNet18 where we set the6initial learning rate to 0.0316, 0.1, and 0.316 and total decay to 250x, 1000x, and 2500x. The suggestedhyperparameters performed the best. For all models other than AlexNet we smoothed out the learning rateschedule, which was important for early learning as shown in Figure 2. EpochTop5 Accuracy0255075100 020406080SmoothPiece-wiseSmooth schedule improved early learning EpochLearning Rate0.0010.010.1 020406080SmoothPiece-wiseSmooth learning rate schedule Figure 2Smoothing out the learning rate improved early learning, which is the regime we were interestedin. ResNet-50 learning curves pictured.A natural concern would be that new models aren’t optimized well for compute in reaching AlexNet-levelperformance. Before smoothing the learning rate schedule, many models hit AlexNet performance at exactly31 epochs, when the learning rate was reduced by a factor of 10x. This adjustment often increased ourmeasured efﬁciency by 2-4x, but we didn’t observe meaningful differences in ﬁnal performance from thechange in learning rate schedule. So even though the change to the learning rate schedule could be consideredminimal, it has a large effect on our measurements. The more simple shape of the updated learning curve,suggests that optimizing for convergence might be relatively compatible with optimizing for lower levels ofperformance, like AlexNet-level accuracy.As context for the quality of these re-implementations we provide tables in Appendix C that compare theﬁnal accuracy we reached to the original paper results.4 Results4.1 Key Result: 44x less compute needed to get to AlexNet-level performanceIn ﬁgure 3 we show that between 2012 and 2019 the amount of compute that neural net architectures requireto be trained from scratch to AlexNet level performance has gone down by a factor of 44x (16-month doublingtime)Most researchers found the algorithmic efﬁciency gains to surprisingly high and regular. The progress isfaster than the original Moore’s Law rate (11x) over this period, where both trends made training models ofAlexNet-level performance cheaper. Moore’s Law is obviously a more general trend than what we observein Figure 3. We believe it’s quite interesting to see what we can say about algorithmic efﬁciency progress ingeneral given these types of measurement, and we explore this question in sections 4.2 and 5.4. 7Figure 3Lowest compute points at any given time shown in blue, all points measured shown in gray. Weobserved an efﬁciency doubling time of 16 months.We can split the progress in training efﬁciency into data efﬁciency (needing fewer epochs) and reductionsin the number of FLOPs required per epoch. Table 1 below shows this split for the models that were theefﬁciency state of the art for a time.We can see that both reductions in training epochs and FLOPs per training image play an important andvarying factor in the overall algorithmic efﬁciency gains. This type of analysis is somewhat sensitive to howfar the original work pushed towards convergence.2Other limitations are discussed in sections 5.4 and 5.7.Calculations for the ﬁgure 3 are provided in Appendix B. Relevant information for EfﬁcientNet training costwas provided through correspondence with authors. 2It only took 62 of the 90 epochs for AlexNet to train to 78.8% top 5 accuracy on ImageNet (99.6% of the 79.1%ﬁnal accuracy). So if the original AlexNet had only been trained for 62 epochs, we would have calculated the overallalgorithmic efﬁciency gain as 30x rather than 44x. We don’t think it’s tractable to mitigate this confounder without addinga lot of complexity to explaining the measurement, but it seemed important to ﬂag as a limitation of our approach.8Table 1Breakdown of total training efﬁciency gains in reaching AlexNet-level accuracy into reduction oftraining epochs and ﬂops per epochExperimentTraining epochs factorFLOPs per epoch factorTraining efﬁciency factorAlexNet1.01.01.0GoogleNet110.384.3MobileNet_v18.21.3511ShufﬂeNet_v1_1x3.85.521ShufﬂeNet_v2_1x4.55.525EfﬁcientNet-b0222.0444.2 FLOPs based learning curves can help clarify comparisons between modelsWe ﬁnd it noteworthy that in when we plot FLOPs based learning curves in ﬁgure 4 some architecturesdominate others. Teraflops/s-daysAccuracy0255075100 0.010.1110AlexNetGoogleNetMobileNet_v2Resnet-50ShuffleNet_v2_1xVgg-11FLOPs used to train vs top5 accuracy on ImageNet 