Supervising strong learners by amplifying weak experts Paul Christiano OpenAI paul@openai.comBuck Shlegeris bshlegeris@gmail.comDario Amodei OpenAI damodei@openai.com Abstract Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Ampliﬁcation, an alternative training strategy which progressively builds up a training signal for difﬁcult problems by combining solutions to easier subproblems. Iterated Ampliﬁcation is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017b), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Ampliﬁcation can efﬁciently learn complex behaviors. 1 Introduction If we want to train an ML system to perform a task, we need to be able to evaluate how well it is doing. Whether our training signal takes the form of labels, rewards, or something else entirely, we need some way to generate that signal. If our goal can be evaluated automatically, such as winning a game of Go, or if we have an algorithm that can generate examples of correct behavior, then generating a training signal is trivial. In these cases we might say that there is an “algorithmic” training signal. Unfortunately, most useful tasks don’t have an algorithmic training signal. So in current applications of machine learning, humans often provide the training signal. This can be done by having a human demonstrate the task, for example labeling an image or teleoperating a robot, or by learning a reward function from human judgments. For these classes of tasks, we could say there is a “human” training signal. However, there are harder tasks for which we can’t compute demonstrations or rewards even with human assistance, and for which we currently have no clear method to get a meaningful training signal. Consider making economic policy decisions, advancing the scientiﬁc frontier, or managing the security of a large network of computers. Some of these tasks are “beyond human scale” – a single human can’t perform them and can’t make sense of their massive observation space well enough to judge the behavior of an agent. It may be possible for a human to judge performance in the very long run (for example, by looking at economic growth over several years), but such long-term feedback is very slow to learn from. We currently have no way to learn how to perform such tasks much better than a human. The overall situation is depicted in Table 1, which shows six different combinations of training signal source and problem formulation (supervised learning or RL). The bulk of ML practice operates in the top center box (supervised learning from human labels), the bottom left box (RL with a scripted reward), and sometimes the top left box (supervised learning of algorithms). The bottom center box Work done while at OpenAI.arXiv:1810.08575v1  [cs.LG]  19 Oct 2018Table 1: Example problems which require different kinds of training signal. Training signal: Algorithmic Human Beyond human Supervised learningLearning data structures Image classiﬁcation Long-term prediction Reinforcement learningPlaying games Driving “well” Designing transit system (RL from a human training signal) is beginning to be explored, and includes inverse reinforcement learning (Ng and Russell, 2000; Abbeel and Ng, 2004; Finn et al., 2016) and RL from human feedback (Knox and Stone, 2009; Pilarski et al., 2011; MacGlashan et al., 2017; Christiano et al., 2017). At present there seems to be no general method to handle problems in the bottom right or top right. It seems desirable to expand the range of tasks for which we can get a training signal, for two reasons. First, it would enable ML systems to perform new tasks. SL and RL are very powerful methods when we can get a training signal, so making them applicable to tasks that humans can’t directly judge or perform could have a big impact. Second, better speciﬁcation of complex goals and targets may be vital to building robustly beneﬁcial AI systems. In practice, when an accurate training signal would be “beyond human scale,” we often instead ﬁnd a short-term proxy that is correlated with what we want. But aggressively optimizing that proxy can lead to pathological behavior(Lehman et al., 2018; Amodei and Clark, 2016; Amodei et al., 2016) , an example of Goodhart’s Law.2For example, we might ﬁnd that user-reported satisfaction (which we can easily measure) is a good proxy for long-term beneﬁt to society (which is very complicated), but if we maximize it with RL our agent may maintain fraudulent appearances or effectively manipulate users into providing high ratings. At large scales this kind of pathology could lead to systemic crashes, and a mismatch 