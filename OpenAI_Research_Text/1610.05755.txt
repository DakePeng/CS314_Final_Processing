Published as a conference paper at ICLR 2017
SEMI-SUPERVISED KNOWLEDGE TRANSFER
FOR DEEPLEARNING FROM PRIVATE TRAINING DATA
Nicolas Papernot
Pennsylvania State University
ngp5056@cse.psu.eduMart ¬¥ƒ±n Abadi
Google Brain
abadi@google.com¬¥Ulfar Erlingsson
Google
ulfar@google.com
Ian Goodfellow
Google Brainy
goodfellow@google.comKunal Talwar
Google Brain
kunal@google.com
ABSTRACT
Some machine learning applications involve training data that is sensitive, such
as the medical histories of patients in a clinical trial. A model may inadvertently
and implicitly store some of its training data; careful analysis of the model may
therefore reveal sensitive information.
To address this problem, we demonstrate a generally applicable approach to pro-
viding strong privacy guarantees for training data: Private Aggregation of Teacher
Ensembles (PATE). The approach combines, in a black-box fashion, multiple
models trained with disjoint datasets, such as records from different subsets of
users. Because they rely directly on sensitive data, these models are not pub-
lished, but instead used as ‚Äúteachers‚Äù for a ‚Äústudent‚Äù model. The student learns
to predict an output chosen by noisy voting among all of the teachers, and cannot
directly access an individual teacher or the underlying data or parameters. The
student‚Äôs privacy properties can be understood both intuitively (since no single
teacher and thus no single dataset dictates the student‚Äôs training) and formally, in
terms of differential privacy. These properties hold even if an adversary can not
only query the student but also inspect its internal workings.
Compared with previous work, the approach imposes only weak assumptions on
how teachers are trained: it applies to any model, including non-convex models
like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and
SVHN thanks to an improved privacy analysis and semi-supervised learning.
1 I NTRODUCTION
Some machine learning applications with great beneÔ¨Åts are enabled only through the analysis of
sensitive data, such as users‚Äô personal contacts, private photographs or correspondence, or even
medical records or genetic sequences (Alipanahi et al., 2015; Kannan et al., 2016; Kononenko, 2001;
Sweeney, 1997). Ideally, in those cases, the learning algorithms would protect the privacy of users‚Äô
training data, e.g., by guaranteeing that the output model generalizes away from the speciÔ¨Åcs of any
individual user. Unfortunately, established machine learning algorithms make no such guarantee;
indeed, though state-of-the-art algorithms generalize well to the test set, they continue to overÔ¨Åt on
speciÔ¨Åc training examples in the sense that some of these examples are implicitly memorized.
Recent attacks exploiting this implicit memorization in machine learning have demonstrated that
private, sensitive training data can be recovered from models. Such attacks can proceed directly, by
analyzing internal model parameters, but also indirectly, by repeatedly querying opaque models to
gather data for the attack‚Äôs analysis. For example, Fredrikson et al. (2015) used hill-climbing on the
output probabilities of a computer-vision classiÔ¨Åer to reveal individual faces from the training data.
Work done while the author was at Google.
yWork done both at Google Brain and at OpenAI.
1arXiv:1610.05755v4  [stat.ML]  3 Mar 2017Published as a conference paper at ICLR 2017
Because of those demonstrations‚Äîand because privacy guarantees must apply to worst-case out-
liers, not only the average‚Äîany strategy for protecting the privacy of training data should prudently
assume that attackers have unfettered access to internal model parameters.
To protect the privacy of training data, this paper improves upon a speciÔ¨Åc, structured application of
the techniques of knowledge aggregation and transfer (Breiman, 1994), previously explored by Nis-
sim et al. (2007), Pathak et al. (2010), and particularly Hamm et al. (2016). In this strategy, Ô¨Årst,
an ensemble (Dietterich, 2000) of teacher models is trained on disjoint subsets of the sensitive data.
Then, using auxiliary, unlabeled non-sensitive data, a student model is trained on the aggregate out-
put of the ensemble, such that the student learns to accurately mimic the ensemble. Intuitively, this
strategy ensures that the student does not depend on the details of any single sensitive training data
point (e.g., of any single user), and, thereby, the privacy of the training data is protected even if
attackers can observe the student‚Äôs internal model parameters.
This paper shows how this strategy‚Äôs privacy guarantees can be strengthened by restricting student
training to a limited number of teacher votes, and by revealing only the topmost vote after care-
fully adding random noise. We call this strengthened strategy PATE, for Private Aggregation of
Teacher Ensembles . Furthermore, we introduce an improved privacy analysis that makes the strat-
egy generally applicable to machine learning algorithms with high utility and meaningful privacy
guarantees‚Äîin particular, when combined with semi-supervised learning.
To establish strong privacy guarantees, it is important to limit the student‚Äôs access to its teachers,
so that the student‚Äôs exposure to teachers‚Äô knowledge can be meaningfully quantiÔ¨Åed and bounded.
Fortunately, there are many techniques for speeding up knowledge transfer that can reduce the rate
of student/teacher consultation during learning. We describe several techniques in this paper, the
most effective of which makes use of generative adversarial networks (GANs) (Goodfellow et al.,
2014) applied to semi-supervised learning, using the implementation proposed by Salimans et al.
(2016). For clarity, we use the term PATE-G when our approach is combined with generative, semi-
supervised methods. Like all semi-supervised learning methods, PATE-G assumes the student has
access to additional, unlabeled data, which, in this context, must be public or non-sensitive. This
assumption should not greatly restrict our method‚Äôs applicability: even when learning on sensitive
data, a non-overlapping, unlabeled set of data often exists, from which semi-supervised methods can
extract distribution priors. For instance, public datasets exist for text and images, and for medical
data.
It seems intuitive, or even obvious, that a student machine learning model will provide good privacy
when trained without access to sensitive training data, apart from a few, noisy votes from a teacher
quorum. However, intuition is not sufÔ¨Åcient because privacy properties can be surprisingly hard
to reason about; for example, even a single data item can greatly impact machine learning models
trained on a large corpus (Chaudhuri et al., 2011). Therefore, to limit the effect of any single sensitive
data item on the student‚Äôs learning, precisely and formally, we apply the well-established, rigorous
standard of differential privacy (Dwork & Roth, 2014). Like all differentially private algorithms, our
learning strategy carefully adds noise, so that the privacy impact of each data item can be analyzed
and bounded. In particular, we dynamically analyze the sensitivity of the teachers‚Äô noisy votes;
for this purpose, we use the state-of-the-art moments accountant technique from Abadi et al. (2016),
which tightens the privacy bound when the topmost vote has a large quorum. As a result, for MNIST
and similar benchmark learning tasks, our methods allow students to provide excellent utility, while
our analysis provides meaningful worst-case guarantees. In particular, we can bound the metric for
privacy loss (the differential-privacy ") to a range similar to that of existing, real-world privacy-
protection mechanisms, such as Google‚Äôs RAPPOR (Erlingsson et al., 2014).
Finally, it is an important advantage that our learning strategy and our privacy analysis do not depend
on the details of the machine learning techniques used to train either the teachers or their student.
Therefore, the techniques in this paper apply equally well for deep learning methods, or any such
learning methods with large numbers of parameters, as they do for shallow, simple techniques.
In comparison, Hamm et al. (2016) guarantee privacy only conditionally, for a restricted class of
student classiÔ¨Åers‚Äîin effect, limiting applicability to logistic regression with convex loss. Also,
unlike the methods of Abadi et al. (2016), which represent the state-of-the-art in differentially-
private deep learning, our techniques make no assumptions about details such as batch selection, the
loss function, or the choice of the optimization algorithm. Even so, as we show in experiments on
2Published as a conference paper at ICLR 2017
Data 1 
Data 2 
Data n Data 3 
...Teacher 1 
Teacher 2 
Teacher n Teacher 3 
...Aggregate 
Teacher Queries Student 
Training Accessible by adversary Not accessible by adversary 
Sensitive 
Data 
Incomplete 
Public Data 
Prediction Data feeding Predicted 
completion 
Figure 1: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of the
sensitive data, (2) a student model is trained on public data labeled using the ensemble.
MNIST and SVHN, our techniques provide a privacy/utility tradeoff that equals or improves upon
bespoke learning methods such as those of Abadi et al. (2016).
Section 5 further discusses the related work. Building on this related work, our contributions are as
follows:
We demonstrate a general machine learning strategy, the PATE approach, that provides dif-
ferential privacy for training data in a ‚Äúblack-box‚Äù manner, i.e., independent of the learning
algorithm, as demonstrated by Section 4 and Appendix C.
We improve upon the strategy outlined in Hamm et al. (2016) for learning machine models
that protect training data privacy. In particular, our student only accesses the teachers‚Äô top
vote and the model does not need to be trained with a restricted class of convex losses.
We explore four different approaches for reducing the student‚Äôs dependence on its teachers,
and show how the application of GANs to semi-supervised learning of Salimans et al.
(2016) can greatly reduce the privacy loss by radically reducing the need for supervision.
We present a new application of the moments accountant technique from Abadi et al. (2016)
for improving the differential-privacy analysis of knowledge transfer, which allows the
training of students with meaningful privacy bounds.
We evaluate our framework on MNIST and SVHN, allowing for a comparison of our results
with previous differentially private machine learning methods. Our classiÔ¨Åers achieve an
(";)differential-privacy bound of (2:04;10 5)for MNIST and (8:19;10 6)for SVHN,
respectively with accuracy of 98:00% and90:66%. In comparison, for MNIST, Abadi et al.
(2016) obtain a looser (8;10 5)privacy bound and 97% accuracy. For SVHN, Shokri &
Shmatikov (2015) report approx. 92% accuracy with ">2per each of 300,000model pa-
rameters, naively making the total ">600,000, which guarantees no meaningful privacy.
Finally, we show that the PATE approach can be successfully applied to other model struc-
tures and to datasets with different characteristics. In particular, in Appendix C PATE
protects the privacy of medical data used to train a model based on random forests.
Our results are encouraging, and highlight the beneÔ¨Åts of combining a learning strategy based on
semi-supervised knowledge transfer with a precise, data-dependent privacy analysis. However, the
most appealing aspect of this work is probably that its guarantees can be compelling to both an expert
and a non-expert audience. In combination, our techniques simultaneously provide both an intuitive
and a rigorous guarantee of training data privacy, without sacriÔ¨Åcing the utility of the targeted model.
This gives hope that users will increasingly be able to conÔ¨Ådently and safely beneÔ¨Åt from machine
learning models built from their sensitive data.
2 P RIVATE LEARNING WITH ENSEMBLES OF TEACHERS
In this section, we introduce the speciÔ¨Åcs of the PATE approach, which is illustrated in Figure 1.
We describe how the data is partitioned to train an ensemble of teachers, and how the predictions
made by this ensemble are noisily aggregated. In addition, we discuss how GANs can be used in
training the student, and distinguish PATE-G variants that improve our approach using generative,
semi-supervised methods.
3Published as a conference paper at ICLR 2017
2.1 T RAINING THE ENSEMBLE OF TEACHERS
Data partitioning and teachers: Instead of training a single model to solve the task associated with
dataset (X;Y ), whereXdenotes the set of inputs, and Ythe set of labels, we partition the data in n
disjoint sets (Xn;Yn)and train a model separately on each set. As evaluated in Section 4.1, assum-
ing thatnis not too large with respect to the dataset size and task complexity, we obtain nclassiÔ¨Åers
ficalled teachers. We then deploy them as an ensemble making predictions on unseen inputs xby
querying each teacher for a prediction fi(x)and aggregating these into a single prediction.
Aggregation: The privacy guarantees of this teacher ensemble stems from its aggregation. Let m
be the number of classes in our task. The label count for a given class j2[m]and an input ~ xis
the number of teachers that assigned class jto input~ x:nj(~ x) =jfi:i2[n];fi(~ x) =jgj. If we
simply apply plurality ‚Äîuse the label with the largest count‚Äîthe ensemble‚Äôs decision may depend
on a single teacher‚Äôs vote. Indeed, when two labels have a vote count differing by at most one, there
is a tie: the aggregated output changes if one teacher makes a different prediction. We add random
noise to the vote counts njto introduce ambiguity:
f(x) = arg max
j
nj(~ x) +Lap1

(1)
In this equation, is a privacy parameter and Lap(b)the Laplacian distribution with location 0and
scaleb. The parameter inÔ¨Çuences the privacy guarantee we can prove. Intuitively, a large leads
to a strong privacy guarantee, but can degrade the accuracy of the labels, as the noisy maximum f
above can differ from the true plurality.
While we could use an fsuch as above to make predictions, the noise required would increase as we
make more predictions, making the model useless after a bounded number of queries. Furthermore,
privacy guarantees do not hold when an adversary has access to the model parameters. Indeed,
as each teacher fiwas trained without taking into account privacy, it is conceivable that they have
sufÔ¨Åcient capacity to retain details of the training data. To address these limitations, we train another
model, the student, using a Ô¨Åxed number of labels predicted by the teacher ensemble.
2.2 S EMI-SUPERVISED TRANSFER OF THE KNOWLEDGE FROM AN ENSEMBLE TO A STUDENT
We train a student on nonsensitive and unlabeled data, some of which we label using the aggregation
mechanism. This student model is the one deployed, in lieu of the teacher ensemble, so as to Ô¨Åx the
privacy loss to a value that does not grow with the number of user queries made to the student model.
Indeed, the privacy loss is now determined by the number of queries made to the teacher ensemble
during student training and does not increase as end-users query the deployed student model. Thus,
the privacy of users who contributed to the original training dataset is preserved even if the student‚Äôs
architecture and parameters are public or reverse-engineered by an adversary.
We considered several techniques to trade-off the student model‚Äôs quality with the number of labels
it needs to access: distillation, active learning, semi-supervised learning (see Appendix B). Here, we
only describe the most successful one, used in PATE-G: semi-supervised learning with GANs.
Training the student with GANs: The GAN framework involves two machine learning models,
agenerator and a discriminator . They are trained in a competing fashion, in what can be viewed
as a two-player game (Goodfellow et al., 2014). The generator produces samples from the data
distribution by transforming vectors sampled from a Gaussian distribution. The discriminator is
trained to distinguish samples artiÔ¨Åcially produced by the generator from samples part of the real
data distribution. Models are trained via simultaneous gradient descent steps on both players‚Äô costs.
In practice, these dynamics are often difÔ¨Åcult to control when the strategy set is non-convex (e.g., a
DNN). In their application of GANs to semi-supervised learning, Salimans et al. (2016) made the
following modiÔ¨Åcations. The discriminator is extended from a binary classiÔ¨Åer (data vs. generator
sample) to a multi-class classiÔ¨Åer (one of kclasses of data samples, plus a class for generated
samples). This classiÔ¨Åer is then trained to classify labeled real samples in the correct class, unlabeled
real samples in any of the kclasses, and the generated samples in the additional class.
4Published as a conference paper at ICLR 2017
Although no formal results currently explain why yet, the technique was empirically demonstrated
to greatly improve semi-supervised learning of classiÔ¨Åers on several datasets, especially when the
classiÔ¨Åer is trained with feature matching loss (Salimans et al., 2016).
Training the student in a semi-supervised fashion makes better use of the entire data available to the
student, while still only labeling a subset of it. Unlabeled inputs are used in unsupervised learning
to estimate a good prior for the distribution. Labeled inputs are then used for supervised learning.
3 P RIVACY ANALYSIS OF THE APPROACH
We now analyze the differential privacy guarantees of our PATE approach. Namely, we keep track
of the privacy budget throughout the student‚Äôs training using the moments accountant (Abadi et al.,
2016). When teachers reach a strong quorum, this allows us to bound privacy costs more strictly.
3.1 D IFFERENTIAL PRIVACY PRELIMINARIES AND A SIMPLE ANALYSIS OF PATE
Differential privacy (Dwork et al., 2006b; Dwork, 2011) has established itself as a strong standard.
It provides privacy guarantees for algorithms analyzing databases, which in our case is a machine
learning training algorithm processing a training dataset. Differential privacy is deÔ¨Åned using pairs
of adjacent databases: in the present work, these are datasets that only differ by one training example.
Recall the following variant of differential privacy introduced in Dwork et al. (2006a).
DeÔ¨Ånition 1. A randomized mechanism Mwith domainDand rangeRsatisÔ¨Åes (";)-differential
privacy if for any two adjacent inputs d;d02D and for any subset of outputs SR it holds that:
Pr[M(d)2S]e"Pr[M(d0)2S] +: (2)
It will be useful to deÔ¨Åne the privacy loss and the privacy loss random variable . They capture the
differences in the probability distribution resulting from running Mondandd0.
DeÔ¨Ånition 2. LetM:D!R be a randomized mechanism and d;d0a pair of adjacent databases.
Letauxdenote an auxiliary input. For an outcome o2R, the privacy loss at ois deÔ¨Åned as:
c(o;M;aux;d;d0)= logPr[M(aux;d) =o]
Pr[M(aux;d0) =o]: (3)
The privacy loss random variable C(M;aux;d;d0)is deÔ¨Åned as c(M(d);M;aux;d;d0), i.e. the
random variable deÔ¨Åned by evaluating the privacy loss at an outcome sampled from M(d).
A natural way to bound our approach‚Äôs privacy loss is to Ô¨Årst bound the privacy cost of each label
queried by the student, and then use the strong composition theorem (Dwork et al., 2010) to derive
the total cost of training the student. For neighboring databases d;d0, each teacher gets the same
training data partition (that is, the same for the teacher with dand withd0, not the same across
teachers), with the exception of one teacher whose corresponding training data partition differs.
Therefore, the label counts nj(~ x)for any example ~ x, ondandd0differ by at most 1in at most two
locations. In the next subsection, we show that this yields loose guarantees.
3.2 T HE MOMENTS ACCOUNTANT : A BUILDING BLOCK FOR BETTER ANALYSIS
To better keep track of the privacy cost, we use recent advances in privacy cost accounting. The
moments accountant was introduced by Abadi et al. (2016), building on previous work (Bun &
Steinke, 2016; Dwork & Rothblum, 2016; Mironov, 2016).
DeÔ¨Ånition 3. LetM:D!R be a randomized mechanism and d;d0a pair of adjacent databases.
Letauxdenote an auxiliary input. The moments accountant is deÔ¨Åned as:
M()= max
aux;d;d0M(;aux;d;d0) (4)
whereM(;aux;d;d0)= log E[exp(C(M;aux;d;d0))]is the moment generating function of
the privacy loss random variable.
The following properties of the moments accountant are proved in Abadi et al. (2016).
5Published as a conference paper at ICLR 2017
Theorem 1. 1.[Composability] Suppose that a mechanism Mconsists of a sequence of adap-
tive mechanismsM1;:::;MkwhereMi:Qi 1
j=1RjD!R i. Then, for any output sequence
o1;:::;ok 1and any
M(;d;d0) =kX
i=1Mi(;o1;:::;oi 1;d;d0);
whereMis conditioned onMi‚Äôs output being oifori<k .
2.[Tail bound] For any">0, the mechanismMis(";)-differentially private for
= min
exp(M() "):
We write down two important properties of the aggregation mechanism from Section 2. The Ô¨Årst
property is proved in Dwork & Roth (2014), and the second follows from Bun & Steinke (2016).
Theorem 2. Suppose that on neighboring databases d;d0, the label counts njdiffer by at most 1
in each coordinate. Let Mbe the mechanism that reports arg maxjn
nj+Lap(1
)o
. ThenM
satisÔ¨Åes (2;0)-differential privacy. Moreover, for any l,aux,dandd0,
(l;aux;d;d0)22l(l+ 1) (5)
At each step, we use the aggregation mechanism with noise Lap(1
)which is (2;0)-DP. Thus over
Tsteps, we get (4T2+ 2q
2Tln1
;)-differential privacy. This can be rather large: plugging
in values that correspond to our SVHN result, = 0:05;T= 1000;= 1e 6gives us"26or
alternatively plugging in values that correspond to our MNIST result, = 0:05;T= 100;= 1e 5
gives us"5:80.
3.3 A PRECISE ,DATA -DEPENDENT PRIVACY ANALYSIS OF PATE
Our data-dependent privacy analysis takes advantage of the fact that when the quorum among the
teachers is very strong, the majority outcome has overwhelming likelihood, in which case the pri-
vacy cost is small whenever this outcome occurs. The moments accountant allows us analyze the
composition of such mechanisms in a uniÔ¨Åed framework.
The following theorem, proved in Appendix A, provides a data-dependent bound on the moments
of any differentially private mechanism where some speciÔ¨Åc outcome is very likely.
Theorem 3. LetMbe(2;0)-differentially private and qPr[M(d)6=o]for some outcome o.
Letl;0andq<e2 1
e4 1. Then for any auxand any neighbor d0ofd,MsatisÔ¨Åes
(l;aux;d;d0)log((1 q)1 q
1 e2ql
+qexp(2l)):
To upper bound qfor our aggregation mechanism, we use the following simple lemma, also proved
in Appendix A.
Lemma 4. Letnbe the label score vector for a database dwithnjnjfor allj. Then
Pr[M(d)6=j]X
j6=j2 +(nj nj)
4 exp((nj nj))
This allows us to upper bound qfor a speciÔ¨Åc score vector n, and hence bound speciÔ¨Åc moments. We
take the smaller of the bounds we get from Theorems 2 and 3. We compute these moments for a few
values of(integers up to 8). Theorem 1 allows us to add these bounds over successive steps, and
derive an (";)guarantee from the Ô¨Ånal . Interested readers are referred to the script that we used
to empirically compute these bounds, which is released along with our code: https://github.
com/tensorflow/models/tree/master/differential_privacy/multiple_teachers
6Published as a conference paper at ICLR 2017
Since the privacy moments are themselves now data dependent, the Ô¨Ånal "is itself data-dependent
and should not be revealed. To get around this, we bound the smooth sensitivity (Nissim et al.,
2007) of the moments and add noise proportional to it to the moments themselves. This gives us a
differentially private estimate of the privacy cost. Our evaluation in Section 4 ignores this overhead
and reports the un-noised values of ". Indeed, in our experiments on MNIST and SVHN, the scale
of the noise one needs to add to the released "is smaller than 0.5 and 1.0 respectively.
How does the number of teachers affect the privacy cost? Recall that the student uses a noisy label
computed in (1) which has a parameter . To ensure that the noisy label is likely to be the correct
one, the noise scale1
should be small compared to the the additive gap between the two largest
vales ofnj. While the exact dependence of on the privacy cost in Theorem 3 is subtle, as a general
principle, a smaller leads to a smaller privacy cost. Thus, a larger gap translates to a smaller
privacy cost. Since the gap itself increases with the number of teachers, having more teachers would
lower the privacy cost. This is true up to a point. With nteachers, each teacher only trains on a1
nfraction of the training data. For large enough n, each teachers will have too little training data to be
accurate.
To conclude, we note that our analysis is rather conservative in that it pessimistically assumes that,
even if just one example in the training set for one teacher changes, the classiÔ¨Åer produced by that
teacher may change arbitrarily. One advantage of our approach, which enables its wide applica-
bility, is that our analysis does not require any assumptions about the workings of the teachers.
Nevertheless, we expect that stronger privacy guarantees may perhaps be established in speciÔ¨Åc
settings‚Äîwhen assumptions can be made on the learning algorithm used to train the teachers.
4 E VALUATION
In our evaluation of PATE and its generative variant PATE-G, we Ô¨Årst train a teacher ensemble for
each dataset. The trade-off between the accuracy and privacy of labels predicted by the ensemble
is greatly dependent on the number of teachers in the ensemble: being able to train a large set of
teachers is essential to support the injection of noise yielding strong privacy guarantees while having
a limited impact on accuracy. Second, we minimize the privacy budget spent on learning the student
by training it with as few queries to the ensemble as possible.
Our experiments use MNIST and the extended SVHN datasets. Our MNIST model stacks two
convolutional layers with max-pooling and one fully connected layer with ReLUs. When trained on
the entire dataset, the non-private model has a 99:18% test accuracy. For SVHN, we add two hidden
layers.1The non-private model achieves a 92:8%test accuracy, which is shy of the state-of-the-art.
However, we are primarily interested in comparing the private student‚Äôs accuracy with the one of a
non-private model trained on the entire dataset, for different privacy guarantees. The source code
for reproducing the results in this section is available on GitHub.2
4.1 T RAINING AN ENSEMBLE OF TEACHERS PRODUCING PRIVATE LABELS
As mentioned above, compensating the noise introduced by the Laplacian mechanism presented in
Equation 1 requires large ensembles. We evaluate the extent to which the two datasets considered can
be partitioned with a reasonable impact on the performance of individual teachers. SpeciÔ¨Åcally, we
show that for MNIST and SVHN, we are able to train ensembles of 250teachers. Their aggregated
predictions are accurate despite the injection of large amounts of random noise to ensure privacy.
The aggregation mechanism output has an accuracy of 93:18% for MNIST and 87:79% for SVHN,
when evaluated on their respective test sets, while each query has a low privacy budget of "= 0:05.
Prediction accuracy: All other things being equal, the number nof teachers is limited by a trade-
off between the classiÔ¨Åcation task‚Äôs complexity and the available data. We train nteachers by
partitioning the training data n-way. Larger values of nlead to larger absolute gaps, hence poten-
tially allowing for a larger noise level and stronger privacy guarantees. At the same time, a larger
nimplies a smaller training dataset for each teacher, potentially reducing the teacher accuracy. We
empirically Ô¨Ånd appropriate values of nfor the MNIST and SVHN datasets by measuring the test
1The model is adapted from https://www.tensorflow.org/tutorials/deep_cnn
2https://github.com/tensorflow/models/tree/master/differential_privacy/multiple_teachers
7Published as a conference paper at ICLR 2017
" 
Figure 2: How much noise can be injected
to a query? Accuracy of the noisy aggrega-
tion for three MNIST and SVHN teacher en-
sembles and varying value per query. The
noise introduced to achieve a given scales
inversely proportionally to the value of :
small values of on the left of the axis corre-
spond to large noise amplitudes and large 
values on the right to small noise.
Figure 3: How certain is the aggregation of
teacher predictions? Gap between the num-
ber of votes assigned to the most and second
most frequent labels normalized by the num-
ber of teachers in an ensemble. Larger gaps
indicate that the ensemble is conÔ¨Ådent in as-
signing the labels, and will be robust to more
noise injection. Gaps were computed by av-
eraging over the test data.
set accuracy of each teacher trained on one of the npartitions of the training data. We Ô¨Ånd that even
forn= 250 , the average test accuracy of individual teachers is 83:86% for MNIST and 83:18% for
SVHN. The larger size of SVHN compensates its increased task complexity.
Prediction conÔ¨Ådence: As outlined in Section 3, the privacy of predictions made by an ensemble
of teachers intuitively requires that a quorum of teachers generalizing well agree on identical labels.
This observation is reÔ¨Çected by our data-dependent privacy analysis, which provides stricter privacy
bounds when the quorum is strong. We study the disparity of labels assigned by teachers. In other
words, we count the number of votes for each possible label, and measure the difference in votes
between the most popular label and the second most popular label, i.e., the gap. If the gap is small,
introducing noise during aggregation might change the label assigned from the Ô¨Årst to the second.
Figure 3 shows the gap normalized by the total number of teachers n. Asnincreases, the gap
remains larger than 60% of the teachers, allowing for aggregation mechanisms to output the correct
label in the presence of noise.
Noisy aggregation: For MNIST and SVHN, we consider three ensembles of teachers with varying
number of teachers n2f10;100;250g. For each of them, we perturb the vote counts with Laplacian
noise of inversed scale ranging between 0:01and1. This choice is justiÔ¨Åed below in Section 4.2.
We report in Figure 2 the accuracy of test set labels inferred by the noisy aggregation mechanism for
these values of ". Notice that the number of teachers needs to be large to compensate for the impact
of noise injection on the accuracy.
4.2 S EMI-SUPERVISED TRAINING OF THE STUDENT WITH PRIVACY
The noisy aggregation mechanism labels the student‚Äôs unlabeled training set in a privacy-preserving
fashion. To reduce the privacy budget spent on student training, we are interested in making as few
label queries to the teachers as possible. We therefore use the semi-supervised training approach de-
scribed previously. Our MNIST and SVHN students with (";)differential privacy of (2:04;10 5)
and(8:19;10 6)achieve accuracies of 98:00% and90:66%. These results improve the differential
privacy state-of-the-art for these datasets. Abadi et al. (2016) previously obtained 97% accuracy
with a (8;10 5)bound on MNIST, starting from an inferior baseline model without privacy. Shokri
& Shmatikov (2015) reported about 92% accuracy on SVHN with ">2per model parameter and a
model with over 300,000parameters. Naively, this corresponds to a total ">600,000.
8Published as a conference paper at ICLR 2017
Dataset " Queries Non-Private Baseline Student Accuracy
MNIST 2.04 10 5100 99.18% 98.00%
MNIST 8.03 10 51000 99.18% 98.10%
SVHN 5.04 10 6500 92.80% 82.72%
SVHN 8.19 10 61000 92.80% 90.66%
Figure 4: Utility and privacy of the semi-supervised students: each row is a variant of the stu-
dent model trained with generative adversarial networks in a semi-supervised way, with a different
number of label queries made to the teachers through the noisy aggregation mechanism. The last
column reports the accuracy of the student and the second and third column the bound "and failure
probabilityof the (";)differential privacy guarantee.
We apply semi-supervised learning with GANs to our problem using the following setup for each
dataset. In the case of MNIST, the student has access to 9,000samples, among which a subset
of either 100,500, or1,000samples are labeled using the noisy aggregation mechanism discussed
in Section 2.1. Its performance is evaluated on the 1,000remaining samples of the test set. Note
that this may increase the variance of our test set accuracy measurements, when compared to those
computed over the entire test data. For the MNIST dataset, we randomly shufÔ¨Çe the test set to ensure
that the different classes are balanced when selecting the (small) subset labeled to train the student.
For SVHN, the student has access to 10,000training inputs, among which it labels 500or1,000
samples using the noisy aggregation mechanism. Its performance is evaluated on the remaining
16,032samples. For both datasets, the ensemble is made up of 250teachers. We use Laplacian scale
of20to guarantee an individual query privacy bound of "= 0:05. These parameter choices are
motivated by the results from Section 4.1.
In Figure 4, we report the values of the (";)differential privacy guarantees provided and the cor-
responding student accuracy, as well as the number of queries made by each student. The MNIST
student is able to learn a 98% accurate model, which is shy of 1%when compared to the accuracy
of a model learned with the entire training set, with only 100label queries. This results in a strict
differentially private bound of "= 2:04for a failure probability Ô¨Åxed at 10 5. The SVHN stu-
dent achieves 90:66% accuracy, which is also comparable to the 92:80% accuracy of one teacher
learned with the entire training set. The corresponding privacy bound is "= 8:19, which is higher
than for the MNIST dataset, likely because of the larger number of queries made to the aggregation
mechanism.
We observe that our private student outperforms the aggregation‚Äôs output in terms of accuracy, with
or without the injection of Laplacian noise. While this shows the power of semi-supervised learning,
the student may not learn as well on different kinds of data (e.g., medical data), where categories are
not explicitly designed by humans to be salient in the input space. Encouragingly, as Appendix C
illustrates, the PATE approach can be successfully applied to at least some examples of such data.
5 D ISCUSSION AND RELATED WORK
Several privacy deÔ¨Ånitions are found in the literature. For instance, k-anonymity requires information
about an individual to be indistinguishable from at least k 1other individuals in the dataset (L.
Sweeney, 2002). However, its lack of randomization gives rise to caveats (Dwork & Roth, 2014), and
attackers can infer properties of the dataset (Aggarwal, 2005). An alternative deÔ¨Ånition, differential
privacy , established itself as a rigorous standard for providing privacy guarantees (Dwork et al.,
2006b). In contrast to k-anonymity, differential privacy is a property of the randomized algorithm
and not the dataset itself.
A variety of approaches and mechanisms can guarantee differential privacy. Erlingsson et al. (2014)
showed that randomized response, introduced by Warner (1965), can protect crowd-sourced data
collected from software users to compute statistics about user behaviors. Attempts to provide dif-
ferential privacy for machine learning models led to a series of efforts on shallow machine learning
models, including work by Bassily et al. (2014); Chaudhuri & Monteleoni (2009); Pathak et al.
(2011); Song et al. (2013), and Wainwright et al. (2012).
9Published as a conference paper at ICLR 2017
A privacy-preserving distributed SGD algorithm was introduced by Shokri & Shmatikov (2015). It
applies to non-convex models. However, its privacy bounds are given per-parameter, and the large
number of parameters prevents the technique from providing a meaningful privacy guarantee. Abadi
et al. (2016) provided stricter bounds on the privacy loss induced by a noisy SGD by introducing the
moments accountant. In comparison with these efforts, our work increases the accuracy of a private
MNIST model from 97% to98% while improving the privacy bound "from 8to1:9. Furthermore,
the PATE approach is independent of the learning algorithm, unlike this previous work. Support
for a wide range of architecture and training algorithms allows us to obtain good privacy bounds
on an accurate and private SVHN model. However, this comes at the cost of assuming that non-
private unlabeled data is available, an assumption that is not shared by (Abadi et al., 2016; Shokri &
Shmatikov, 2015).
Pathak et al. (2010) Ô¨Årst discussed secure multi-party aggregation of locally trained classiÔ¨Åers for a
global classiÔ¨Åer hosted by a trusted third-party. Hamm et al. (2016) proposed the use of knowledge
transfer between a collection of models trained on individual devices into a single model guaran-
teeing differential privacy. Their work studied linear student models with convex and continuously
differentiable losses, bounded and c-Lipschitz derivatives, and bounded features. The PATE ap-
proach of this paper is not constrained to such applications, but is more generally applicable.
Previous work also studied semi-supervised knowledge transfer from private models. For instance,
Jagannathan et al. (2013) learned privacy-preserving random forests. A key difference is that their
approach is tailored to decision trees. PATE works well for the speciÔ¨Åc case of decision trees, as
demonstrated in Appendix C, and is also applicable to other machine learning algorithms, including
more complex ones. Another key difference is that Jagannathan et al. (2013) modiÔ¨Åed the classic
model of a decision tree to include the Laplacian mechanism. Thus, the privacy guarantee does
not come from the disjoint sets of training data analyzed by different decision trees in the random
forest, but rather from the modiÔ¨Åed architecture. In contrast, partitioning is essential to the privacy
guarantees of the PATE approach.
6 C ONCLUSIONS
To protect the privacy of sensitive training data, this paper has advanced a learning strategy and a
corresponding privacy analysis. The PATE approach is based on knowledge aggregation and transfer
from ‚Äúteacher‚Äù models, trained on disjoint data, to a ‚Äústudent‚Äù model whose attributes may be made
public. In combination, the paper‚Äôs techniques demonstrably achieve excellent utility on the MNIST
and SVHN benchmark tasks, while simultaneously providing a formal, state-of-the-art bound on
users‚Äô privacy loss. While our results are not without limits‚Äîe.g., they require disjoint training
data for a large number of teachers (whose number is likely to increase for tasks with many output
classes)‚Äîthey are encouraging, and highlight the advantages of combining semi-supervised learn-
ing with precise, data-dependent privacy analysis, which will hopefully trigger further work. In
particular, such future work may further investigate whether or not our semi-supervised approach
will also reduce teacher queries for tasks other than MNIST and SVHN, for example when the
discrete output categories are not as distinctly deÔ¨Åned by the salient input space features.
A key advantage is that this paper‚Äôs techniques establish a precise guarantee of training data pri-
vacy in a manner that is both intuitive and rigorous. Therefore, they can be appealing, and easily
explained, to both an expert and non-expert audience. However, perhaps equally compelling are the
techniques‚Äô wide applicability. Both our learning approach and our analysis methods are ‚Äúblack-
box,‚Äù i.e., independent of the learning algorithm for either teachers or students, and therefore apply,
in general, to non-convex, deep learning, and other learning methods. Also, because our techniques
do not constrain the selection or partitioning of training data, they apply when training data is natu-
rally and non-randomly partitioned‚Äîe.g., because of privacy, regulatory, or competitive concerns‚Äî
or when each teacher is trained in isolation, with a different method. We look forward to such further
applications, for example on RNNs and other sequence-based models.
ACKNOWLEDGMENTS
Nicolas Papernot is supported by a Google PhD Fellowship in Security. The authors would like to
thank Ilya Mironov and Li Zhang for insightful discussions about early drafts of this document.
10Published as a conference paper at ICLR 2017
REFERENCES
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security . ACM, 2016.
Charu C Aggarwal. On k-anonymity and the curse of dimensionality. In Proceedings of the 31st
International Conference on Very Large Data Bases , pp. 901‚Äì909. VLDB Endowment, 2005.
Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the se-
quence speciÔ¨Åcities of DNA-and RNA-binding proteins by deep learning. Nature biotechnology ,
2015.
Dana Angluin. Queries and concept learning. Machine learning , 2(4):319‚Äì342, 1988.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Differentially private empirical risk minimiza-
tion: efÔ¨Åcient algorithms and tight error bounds. arXiv preprint arXiv:1405.7085 , 2014.
Eric B Baum. Neural net algorithms that learn in polynomial time from examples and queries. IEEE
Transactions on Neural Networks , 2(1):5‚Äì19, 1991.
Leo Breiman. Bagging predictors. Machine Learning , 24(2):123‚Äì140, 1994.
Jane Bromley, James W Bentz, L ¬¥eon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard
S¬®ackinger, and Roopak Shah. Signature veriÔ¨Åcation using a ‚ÄúSiamese‚Äù time delay neural network.
International Journal of Pattern Recognition and ArtiÔ¨Åcial Intelligence , 7(04):669‚Äì688, 1993.
Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceed-
ings of the 12th ACM International Conference on Knowledge Discovery and Data mining , pp.
535‚Äì541. ACM, 2006.
Mark Bun and Thomas Steinke. Concentrated differential privacy: simpliÔ¨Åcations, extensions, and
lower bounds. In Proceedings of TCC , 2016.
Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances in
Neural Information Processing Systems , pp. 289‚Äì296, 2009.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal of Machine Learning Research , 12(Mar):1069‚Äì1109, 2011.
Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multi-
ple classiÔ¨Åer systems , pp. 1‚Äì15. Springer, 2000.
Cynthia Dwork. A Ô¨Årm foundation for private data analysis. Communications of the ACM , 54(1):
86‚Äì95, 2011.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science , 9(3-4):211‚Äì407, 2014.
Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint
arXiv:1603.01887 , 2016.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: privacy via distributed noise generation. In Advances in Cryptology-EUROCRYPT
2006 , pp. 486‚Äì503. Springer, 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of Cryptography , pp. 265‚Äì284. Springer, 2006b.
Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In Pro-
ceedings of the 51st IEEE Symposium on Foundations of Computer Science , pp. 51‚Äì60. IEEE,
2010.
¬¥Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized aggregatable
privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security , pp. 1054‚Äì1067. ACM, 2014.
11Published as a conference paper at ICLR 2017
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit conÔ¨Å-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security , pp. 1322‚Äì1333. ACM, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems , pp. 2672‚Äì2680, 2014.
Jihun Hamm, Paul Cao, and Mikhail Belkin. Learning privately from multiparty data. arXiv preprint
arXiv:1602.03552 , 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2015.
Geetha Jagannathan, Claire Monteleoni, and Krishnan Pillaipakkamnatt. A semi-supervised learning
approach to differential privacy. In 2013 IEEE 13th International Conference on Data Mining
Workshops , pp. 841‚Äì848. IEEE, 2013.
Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins, Balint Miklos,
Greg Corrado, et al. Smart reply: Automated response suggestion for email. In Proceedings
of the ACM SIGKDD Conference on Knowledge Discovery and Data mining , volume 36, pp.
495‚Äì503, 2016.
Gregory Koch. Siamese neural networks for one-shot image recognition . PhD thesis, University of
Toronto, 2015.
Igor Kononenko. Machine learning for medical diagnosis: history, state of the art and perspective.
ArtiÔ¨Åcial Intelligence in medicine , 23(1):89‚Äì109, 2001.
L. Sweeney. k-anonymity: A model for protecting privacy. volume 10, pp. 557‚Äì570. World Scien-
tiÔ¨Åc, 2002.
Ilya Mironov. Renyi differential privacy. manuscript, 2016.
Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private
data analysis. In Proceedings of the 39th annual ACM Symposium on Theory of Computing , pp.
75‚Äì84. ACM, 2007.
Manas Pathak, Shantanu Rane, and Bhiksha Raj. Multiparty differential privacy via aggregation of
locally trained classiÔ¨Åers. In Advances in Neural Information Processing Systems , pp. 1876‚Äì1884,
2010.
Manas Pathak, Shantanu Rane, Wei Sun, and Bhiksha Raj. Privacy preserving probabilistic inference
with hidden markov models. In 2011 IEEE International Conference on Acoustics, Speech and
Signal Processing , pp. 5868‚Äì5871. IEEE, 2011.
Jason Poulos and Rafael Valle. Missing data imputation for supervised learning. arXiv preprint
arXiv:1610.09075 , 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. arXiv preprint arXiv:1606.03498 , 2016.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd
ACM SIGSAC Conference on Computer and Communications Security . ACM, 2015.
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with dif-
ferentially private updates. In Global Conference on Signal and Information Processing , pp.
245‚Äì248. IEEE, 2013.
Latanya Sweeney. Weaving technology and policy together to maintain conÔ¨Ådentiality. The Journal
of Law, Medicine & Ethics , 25(2-3):98‚Äì110, 1997.
Martin J Wainwright, Michael I Jordan, and John C Duchi. Privacy aware learning. In Advances in
Neural Information Processing Systems , pp. 1430‚Äì1438, 2012.
Stanley L Warner. Randomized response: A survey technique for eliminating evasive answer bias.
Journal of the American Statistical Association , 60(309):63‚Äì69, 1965.
12Published as a conference paper at ICLR 2017
A M ISSING DETAILS ON THE ANALYSIS
We provide missing proofs from Section 3.
Theorem 3. LetMbe(2;0)-differentially private and qPr[M(d)6=o]for some outcome o.
Letl;0andq<e2 1
e4 1. Then for any auxand any neighbor d0ofd,MsatisÔ¨Åes
(l;aux;d;d0)log((1 q)1 q
1 e2ql
+qexp(2l)):
Proof. SinceMis2-differentially private, for every outcome o,Pr[M(d)=o]
Pr[M(d0)=o]exp(2). Let
q0=Pr[M(d)6=o]. ThenPr[M(d0)6=o]exp(2)q0. Thus
exp((l;aux;d;d0)) =X
oPr[M(d) =o]Pr[M(d) =o]
Pr[M(d0) =o]l
= Pr[M(d) =o]Pr[M(d) =o]
Pr[M(d0) =o]l
+X
o6=oPr[M(d) =o]Pr[M(d) =o]
Pr[M(d0) =o]l
(1 q0)1 q0
1 e2q0l
+X
o6=oPr[M(d) =o](e2)l
(1 q0) 1 q0
1 e2q0l
+q0e2l:
Now consider the function
f(z) = (1 z)1 z
1 e2zl
+ze2l:
We next argue that this function is non-decreasing in (0;e2 1
e4 1)under the conditions of the lemma.
Towards this goal, deÔ¨Åne
g(z;w) = (1 z)1 w
1 e2wl
+ze2l;
and observe that f(z) =g(z;z). We can easily verify by differentiation that g(z;w)is increasing
individually in zand inwin the range of interest. This implies that f(q0)f(q)completing the
proof.
Lemma 4. Letnbe the label score vector for a database dwithnjnjfor allj. Then
Pr[M(d)6=j]X
j6=j2 +(nj nj)
4 exp((nj nj))
Proof. The probability that nj+Lap(1
)<nj+Lap(1
)is equal to the probability that the sum
of two independent Lap(1)random variables exceeds (nj nj). The sum of two independent
Lap(1)variables has the same distribution as the difference of two Gamma (2;1)random variables.
Recalling that the Gamma (2;1)distribution has pdf xe x, we can compute the pdf of the difference
via convolution as
Z1
y=0(y+jxj)e y jxjye ydy=1
ejxjZ1
y=0(y2+yjxj)e 2ydy=1 +jxj
4ejxj:
The probability mass in the tail can then be computed by integration as2+(nj nj)
4 exp((nj nj). Taking a
union bound over the various candidate j‚Äôs gives the claimed bound.
13Published as a conference paper at ICLR 2017
B A PPENDIX : TRAINING THE STUDENT WITH MINIMAL TEACHER QUERIES
In this appendix, we describe approaches that were considered to reduce the number of queries made
to the teacher ensemble by the student during its training. As pointed out in Sections 3 and 4, this
effort is motivated by the direct impact of querying on the total privacy cost associated with student
training. The Ô¨Årst approach is based on distillation , a technique used for knowledge transfer and
model compression (Hinton et al., 2015). The three other techniques considered were proposed
in the context of active learning , with the intent of identifying training examples most useful for
learning. In Sections 2 and 4, we described semi-supervised learning, which yielded the best results.
The student models in this appendix differ from those in Sections 2 and 4, which were trained using
GANs. In contrast, all students in this appendix were learned in a fully supervised fashion from
a subset of public, labeled examples. Thus, the learning goal was to identify the subset of labels
yielding the best learning performance.
B.1 T RAINING STUDENTS USING DISTILLATION
Distillation is a knowledge transfer technique introduced as a means of compressing large models
into smaller ones, while retaining their accuracy (Bucilua et al., 2006; Hinton et al., 2015). This is for
instance useful to train models in data centers before deploying compressed variants in phones. The
transfer is accomplished by training the smaller model on data that is labeled with probability vectors
produced by the Ô¨Årst model, which encode the knowledge extracted from training data. Distillation
is parameterized by a temperature parameterT, which controls the smoothness of probabilities
output by the larger model: when produced at small temperatures, the vectors are discrete, whereas
at high temperature, all classes are assigned non-negligible values. Distillation is a natural candidate
to compress the knowledge acquired by the ensemble of teachers, acting as the large model, into a
student, which is much smaller with ntimes less trainable parameters compared to the nteachers.
To evaluate the applicability of distillation, we consider the ensemble of n= 50 teachers for SVHN.
In this experiment, we do not add noise to the vote counts when aggregating the teacher predictions.
We compare the accuracy of three student models: the Ô¨Årst is a baseline trained with labels obtained
by plurality, the second and third are trained with distillation at T2f1;5g. We use the Ô¨Årst 10,000
samples from the test set as unlabeled data. Figure 5 reports the accuracy of the student model on
the last 16,032samples from the test set, which were not accessible to the model during training. It
is plotted with respect to the number of samples used to train the student (and hence the number of
queries made to the teacher ensemble). Although applying distillation yields classiÔ¨Åers that perform
more accurately, the increase in accuracy is too limited to justify the increased privacy cost of re-
vealing the entire probability vector output by the ensemble instead of simply the class assigned the
largest number of votes. Thus, we turn to an investigation of active learning.
B.2 A CTIVE LEARNING OF THE STUDENT
Active learning is a class of techniques that aims to identify and prioritize points in the student‚Äôs
training set that have a high potential to contribute to learning (Angluin, 1988; Baum, 1991). If the
label of an input in the student‚Äôs training set can be predicted conÔ¨Ådently from what we have learned
so far by querying the teachers, it is intuitive that querying it is not worth the privacy budget spent.
In our experiments, we made several attempts before converging to a simpler Ô¨Ånal formulation.
Siamese networks: Our Ô¨Årst attempt was to train a pair of siamese networks, introduced by Brom-
ley et al. (1993) in the context of one-shot learning and later improved by Koch (2015). The siamese
networks take two images as input and return 1if the images are equal and 0otherwise. They are
two identical networks trained with shared parameters to force them to produce similar represen-
tations of the inputs, which are then compared using a distance metric to determine if the images
are identical or not. Once the siamese models are trained, we feed them a pair of images where
the Ô¨Årst is unlabeled and the second labeled. If the unlabeled image is conÔ¨Ådently matched with a
known labeled image, we can infer the class of the unknown image from the labeled image. In our
experiments, the siamese networks were able to say whether two images are identical or not, but did
not generalize well: two images of the same class did not receive sufÔ¨Åciently conÔ¨Ådent matches. We
also tried a variant of this approach where we trained the siamese networks to output 1when the two
14Published as a conference paper at ICLR 2017
0 2000 4000 6000 8000 10000
Student share of samples in SVHN test set (out of 26032)60657075808590Rest of test set accuracy
Distilled Vectors
Labels only
Distilled Vectors at T=5
Figure 5: InÔ¨Çuence of distillation on the accuracy of the SVHN student trained with respect to the
initial number of training samples available to the student. The student is learning from n= 50
teachers, whose predictions are aggregated without noise: in case where only the label is returned,
we use plurality, and in case a probability vector is returned, we sum the probability vectors output
by each teacher before normalizing the resulting vector.
images are of the same class and 0otherwise, but the learning task proved too complicated to be an
effective means for reducing the number of queries made to teachers.
Collection of binary experts: Our second attempt was to train a collection of binary experts, one
per class. An expert for class jis trained to output 1if the sample is in class jand0otherwise.
We Ô¨Årst trained the binary experts by making an initial batch of queries to the teachers. Using
the experts, we then selected available unlabeled student training points that had a candidate label
score below 0:9and at least 4other experts assigning a score above 0:1. This gave us about 500
unconÔ¨Ådent points for 1700 initial label queries. After labeling these unconÔ¨Ådent points using the
ensemble of teachers, we trained the student. Using binary experts improved the student‚Äôs accuracy
when compared to the student trained on arbitrary data with the same number of teacher queries.
The absolute increases in accuracy were however too limited‚Äîbetween 1:5%and2:5%.
Identifying unconÔ¨Ådent points using the student: This last attempt was the simplest yet the most
effective. Instead of using binary experts to identify student training points that should be labeled by
the teachers, we used the student itself. We asked the student to make predictions on each unlabeled
training point available. We then sorted these samples by increasing values of the maximum proba-
bility assigned to a class for each sample. We queried the teachers to label these unconÔ¨Ådent inputs
Ô¨Årst and trained the student again on this larger labeled training set. This improved the accuracy of
the student when compared to the student trained on arbitrary data. For the same number of teacher
queries, the absolute increases in accuracy of the student trained on unconÔ¨Ådent inputs Ô¨Årst when
compared to the student trained on arbitrary data were in the order of 4% 10%.
15Published as a conference paper at ICLR 2017
C A PPENDIX : ADDITIONAL EXPERIMENTS ON THE UCI A DULT AND
DIABETES DATASETS
In order to further demonstrate the general applicability of our approach, we performed experiments
on two additional datasets. While our experiments on MNIST and SVHN in Section 4 used con-
volutional neural networks and GANs, here we use random forests to train our teacher and student
models for both of the datasets. Our new results on these datasets show that, despite the differing
data types and architectures, we are able to provide meaningful privacy guarantees.
UCI Adult dataset: The UCI Adult dataset is made up of census data, and the task is to predict
when individuals make over $50k per year. Each input consists of 13 features (which include the age,
workplace, education, occupation‚Äîsee the UCI website for a full list3). The only pre-processing we
apply to these features is to map all categorical features to numerical values by assigning an integer
value to each possible category. The model is a random forest provided by the scikit-learn
Python package. When training both our teachers and student, we keep all the default parameter
values, except for the number of estimators, which we set to 100. The data is split between a
training set of 32,562examples, and a test set of 16,282inputs.
UCI Diabetes dataset: The UCI Diabetes dataset includes de-identiÔ¨Åed records of diabetic patients
and corresponding hospital outcomes, which we use to predict whether diabetic patients were read-
mitted less than 30 days after their hospital release. To the best of our knowledge, no particular
classiÔ¨Åcation task is considered to be a standard benchmark for this dataset. Even so, it is valuable
to consider whether our approach is applicable to the likely classiÔ¨Åcation tasks, such as readmission,
since this dataset is collected in a medical environment‚Äîa setting where privacy concerns arise
frequently. We select a subset of 18input features from the 55available in the dataset (to avoid
features with missing values) and form a dataset balanced between the two output classes (see the
UCI website for more details4). In class 0, we include all patients that were readmitted in a 30-day
window, while class 1includes all patients that were readmitted after 30 days or never readmitted at
all. Our balanced dataset contains 34,104training samples and 12,702evaluation samples. We use
a random forest model identical to the one described above in the presentation of the Adult dataset.
Experimental results: We apply our approach described in Section 2. For both datasets, we train
ensembles of n= 250 random forests on partitions of the training data. We then use the noisy
aggregation mechanism, where vote counts are perturbed with Laplacian noise of scale 0:05to
privately label the Ô¨Årst 500test set inputs. We train the student random forest on these 500test set
inputs and evaluate it on the last 11,282test set inputs for the Adult dataset, and 6,352test set inputs
for the Diabetes dataset. These numbers deliberately leave out some of the test set, which allowed
us to observe how the student performance-privacy trade-off was impacted by varying the number
of private labels, as well as the Laplacian scale used when computing these labels.
For the Adult dataset, we Ô¨Ånd that our student model achieves an 83% accuracy for an (";) =
(2:66;10 5)differential privacy bound. Our non-private model on the dataset achieves 85% accu-
racy, which is comparable to the state-of-the-art accuracy of 86% on this dataset (Poulos & Valle,
2016). For the Diabetes dataset, we Ô¨Ånd that our privacy-preserving student model achieves a
93:94% accuracy for a (";) = (1:44;10 5)differential privacy bound. Our non-private model
on the dataset achieves 93:81% accuracy.
3https://archive.ics.uci.edu/ml/datasets/Adult
4https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008
16