Published as a conference paper at ICLR 2017 SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEPLEARNING FROM PRIVATE TRAINING DATA Nicolas Papernot Pennsylvania State University ngp5056@cse.psu.eduMart ¬¥ƒ±n Abadi Google Brain abadi@google.com¬¥Ulfar Erlingsson Google ulfar@google.com Ian Goodfellow Google Brainy goodfellow@google.comKunal Talwar Google Brain kunal@google.com ABSTRACT Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ‚Äúteachers‚Äù for a ‚Äústudent‚Äù model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student‚Äôs privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student‚Äôs training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning. 1 I NTRODUCTION Some machine learning applications with great beneÔ¨Åts are enabled only through the analysis of sensitive data, such as users‚Äô personal contacts, private photographs or correspondence, or even medical records or genetic sequences (Alipanahi et al., 2015; Kannan et al., 2016; Kononenko, 2001; Sweeney, 1997). Ideally, in those cases, the learning algorithms would protect the privacy of users‚Äô training data, e.g., by guaranteeing that the output model generalizes away from the speciÔ¨Åcs of any individual user. Unfortunately, established machine learning algorithms make no such guarantee; indeed, though state-of-the-art algorithms generalize well to the test set, they continue to overÔ¨Åt on speciÔ¨Åc training examples in the sense that some of these examples are implicitly memorized. Recent attacks exploiting this implicit memorization in machine learning have demonstrated that private, sensitive training data can be recovered from models. Such attacks can proceed directly, by analyzing internal model parameters, but also indirectly, by repeatedly querying opaque models to gather data for the attack‚Äôs analysis. For example, Fredrikson et al. (2015) used hill-climbing on the output probabilities of a computer-vision classiÔ¨Åer to reveal individual faces from the training data. Work done while the author was at Google. yWork done both at Google Brain and at OpenAI. 1arXiv:1610.05755v4  [stat.ML]  3 Mar 2017Published as a conference paper at ICLR 2017 Because of those demonstrations‚Äîand because privacy guarantees must apply to worst-case outliers, not only the average‚Äîany strategy for protecting the privacy of training data should prudently assume that attackers have unfettered access to internal model parameters. To protect the privacy of training data, this paper improves upon a speciÔ¨Åc, structured application of the techniques of knowledge aggregation and transfer (Breiman, 1994), previously explored by Nissim et al. (2007), Pathak et al. (2010), and particularly Hamm et al. (2016). In this strategy, Ô¨Årst, an ensemble (Dietterich, 2000) of teacher models is trained on disjoint subsets of the sensitive data. Then, using auxiliary, unlabeled non-sensitive data, a student model is trained on the aggregate output of the ensemble, such that the student learns to accurately mimic the ensemble. Intuitively, this strategy ensures that the student does not depend on the details of any single sensitive training data point (e.g., of any single user), and, thereby, the privacy of the training data is protected even if attackers can observe the student‚Äôs internal model parameters. This paper shows how this strategy‚Äôs privacy guarantees can be strengthened by restricting student training to a limited number of teacher votes, and by revealing only the topmost vote after carefully adding random noise. We call this strengthened strategy PATE, for Private Aggregation of Teacher Ensembles . Furthermore, we introduce an improved privacy analysis that makes the strategy generally applicable to machine learning algorithms with high utility and meaningful privacy guarantees‚Äîin particular, when combined with semi-supervised learning. To establish strong privacy guarantees, it is important to limit the student‚Äôs access to its teachers, so that the student‚Äôs exposure to teachers‚Äô knowledge can be meaningfully quantiÔ¨Åed and bounded. Fortunately, there are many techniques for speeding up knowledge transfer that can reduce the rate of student/teacher consultation during learning. We describe several techniques in this paper, the most effective of which makes use of generative adversarial networks (GANs) (Goodfellow et al., 2014) applied to semi-supervised learning, using the implementation proposed by Salimans et al. (2016). For clarity, we use the term PATE-G when our approach is combined with generative, semisupervised methods. Like all semi-supervised learning methods, PATE-G assumes the student has access to additional, unlabeled data, which, in this context, must be public or non-sensitive. This assumption should not greatly restrict our method‚Äôs applicability: even when learning on sensitive data, a non-overlapping, unlabeled set of data often exists, from which semi-supervised methods can extract distribution priors. For instance, public datasets exist for text and images, and for medical data. It seems intuitive, or even obvious, that a student machine learning model will provide good privacy when trained without access to sensitive training data, apart from a few, noisy votes from a teacher quorum. However, intuition is not sufÔ¨Åcient because privacy properties can be surprisingly hard to reason about; for example, even a single data item can greatly impact machine learning models trained on a large corpus (Chaudhuri et al., 2011). Therefore, to limit the effect of any single sensitive data item on the student‚Äôs learning, precisely and formally, we apply the well-established, rigorous standard of differential privacy (Dwork & Roth, 2014). Like all differentially private algorithms, our learning strategy carefully adds noise, so that the privacy impact of each data item can be analyzed and bounded. In particular, we dynamically analyze the sensitivity of the teachers‚Äô noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from Abadi et al. (2016), which tightens the privacy bound when the topmost vote has a large quorum. As a result, for MNIST and similar benchmark learning tasks, our methods allow students to provide excellent utility, while our analysis provides meaningful worst-case guarantees. In particular, we can bound the metric for privacy loss (the differential-privacy ") to a range similar to that of existing, real-world privacyprotection mechanisms, such as Google‚Äôs RAPPOR (Erlingsson et al., 2014). Finally, it is an important advantage that our learning strategy and our privacy analysis do not depend on the details of the machine learning techniques used to train either the teachers or their student. Therefore, the techniques in this paper apply equally well for deep learning methods, or any such learning methods with large numbers of parameters, as they do for shallow, simple techniques. In comparison, Hamm et al. (2016) guarantee privacy only conditionally, for a restricted class of student classiÔ¨Åers‚Äîin effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of Abadi et al. (2016), which represent the state-of-the-art in differentiallyprivate deep learning, our techniques make no assumptions about details such as batch selection, the loss function, or the choice of the optimization algorithm. Even so, as we show in experiments on 2Published as a conference paper at ICLR 2017 Data 1  Data 2  Data n Data 3  ...Teacher 1  Teacher 2  Teacher n Teacher 3  ...Aggregate  Teacher Queries Student  Training Accessible by adversary Not accessible by adversary  Sensitive  Data  Incomplete  Public Data  Prediction Data feeding Predicted  completion  Figure 1: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of the sensitive data, (2) a student model is trained on public data labeled using the ensemble. MNIST and SVHN, our techniques provide a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of Abadi et al. (2016). Section 5 further discusses the related work. Building on this related work, our contributions are as follows: We demonstrate a general machine learning strategy, the PATE approach, that provides differential privacy for training data in a ‚Äúblack-box‚Äù manner, i.e., independent of the learning algorithm, as demonstrated by Section 4 and Appendix C. We improve upon the strategy outlined in Hamm et al. (2016) for learning machine models that protect training data privacy. In particular, our student only accesses the teachers‚Äô top vote and the model does not need to be trained with a restricted class of convex losses. We explore four different approaches for reducing the student‚Äôs dependence on its teachers, and show how the application of GANs to semi-supervised learning of Salimans et al. (2016) can greatly reduce the privacy loss by radically reducing the need for supervision. We present a new application of the moments accountant technique from Abadi et al. (2016) for improving the differential-privacy analysis of knowledge transfer, which allows the training of students with meaningful privacy bounds. We evaluate our framework on MNIST and SVHN, allowing for a comparison of our results with previous differentially private machine learning methods. Our classiÔ¨Åers achieve an (";)differential-privacy bound of (2:04;10 5)for MNIST and (8:19;10 6)for SVHN, respectively with accuracy of 98:00% and90:66%. In comparison, for MNIST, Abadi et al. (2016) obtain a looser (8;10 5)privacy bound and 97% accuracy. For SVHN, Shokri & Shmatikov (2015) report approx. 92% accuracy with ">2per each of 300,000model parameters, naively making the total ">600,000, which guarantees no meaningful privacy. Finally, we show that the PATE approach can be successfully applied to other model structures and to datasets with different characteristics. In particular, in Appendix C PATE protects the privacy of medical data used to train a model based on random forests. Our results are encouraging, and highlight the beneÔ¨Åts of combining a learning strategy based on semi-supervised knowledge transfer with a precise, data-dependent privacy analysis. However, the most appealing aspect of this work is probably that its guarantees can be compelling to both an expert and a non-expert audience. In combination, our techniques simultaneously provide both an intuitive and a rigorous guarantee of training data privacy, without sacriÔ¨Åcing the utility of the targeted model. This gives hope that users will increasingly be able to conÔ¨Ådently and safely beneÔ¨Åt from machine learning models built from their sensitive data. 2 P RIVATE LEARNING WITH ENSEMBLES OF TEACHERS In this section, we introduce the speciÔ¨Åcs of the PATE approach, which is illustrated in Figure 1. We describe how the data is partitioned to train an ensemble of teachers, and how the predictions made by this ensemble are noisily aggregated. In addition, we discuss how GANs can be used in training the student, and distinguish PATE-G variants that improve our approach using generative, semi-supervised methods. 3Published as a conference paper at ICLR 2017 2.1 T RAINING THE ENSEMBLE OF TEACHERS Data partitioning and teachers: Instead of training a single model to solve the task associated with dataset (X;Y ), whereXdenotes the set of inputs, and Ythe set of labels, we partition the data in n disjoint sets (Xn;Yn)and train a model separately on each set. As evaluated in Section 4.1, assuming thatnis not too large with respect to the dataset size and task complexity, we obtain nclassiÔ¨Åers ficalled teachers. We then deploy them as an ensemble making predictions on unseen inputs xby querying each teacher for a prediction fi(x)and aggregating these into a single prediction. Aggregation: The privacy guarantees of this teacher ensemble stems from its aggregation. Let m be the number of classes in our task. The label count for a given class j2[m]and an input ~ xis the number of teachers that assigned class jto input~ x:nj(~ x) =jfi:i2[n];fi(~ x) =jgj. If we simply apply plurality ‚Äîuse the label with the largest count‚Äîthe ensemble‚Äôs decision may depend on a single teacher‚Äôs vote. Indeed, when two labels have a vote count differing by at most one, there is a tie: the aggregated output changes if one teacher makes a different prediction. We add random noise to the vote counts njto introduce ambiguity: f(x) = arg max j nj(~ x) +Lap1   (1) In this equation,  is a privacy parameter and Lap(b)the Laplacian distribution with location 0and scaleb. The parameter  inÔ¨Çuences the privacy guarantee we can prove. Intuitively, a large  leads to a strong privacy guarantee, but can degrade the accuracy of the labels, as the noisy maximum f above can differ from the true plurality. While we could use an fsuch as above to make predictions, the noise required would increase as we make more predictions, making the model useless after a bounded number of queries. Furthermore, privacy guarantees do not hold when an adversary has access to the model parameters. Indeed, as each teacher fiwas trained without taking into account privacy, it is conceivable that they have sufÔ¨Åcient capacity to retain details of the training data. To address these limitations, we train another model, the student, using a Ô¨Åxed number of labels predicted by the teacher ensemble. 2.2 S EMI-SUPERVISED TRANSFER OF THE KNOWLEDGE FROM AN ENSEMBLE TO A STUDENT We train a student on nonsensitive and unlabeled data, some of which we label using the aggregation mechanism. This student model is the one deployed, in lieu of the teacher ensemble, so as to Ô¨Åx the privacy loss to a value that does not grow with the number of user queries made to the student model. Indeed, the privacy loss is now determined by the number of queries made to the teacher ensemble during student training and does not increase as end-users query the deployed student model. Thus, the privacy of users who contributed to the original training dataset is preserved even if the student‚Äôs architecture and parameters are public or reverse-engineered by an adversary. We considered several techniques to trade-off the student model‚Äôs quality with the number of labels it needs to access: distillation, active learning, semi-supervised learning (see Appendix B). Here, we only describe the most successful one, used in PATE-G: semi-supervised learning with GANs. Training the student with GANs: The GAN framework involves two machine learning models, agenerator and a discriminator . They are trained in a competing fashion, in what can be viewed as a two-player game (Goodfellow et al., 2014). The generator produces samples from the data distribution by transforming vectors sampled from a Gaussian distribution. The discriminator is trained to distinguish samples artiÔ¨Åcially produced by the generator from samples part of the real data distribution. Models are trained via simultaneous gradient descent steps on both players‚Äô costs. In practice, these dynamics are often difÔ¨Åcult to control when the strategy set is non-convex (e.g., a DNN). In their application of GANs to semi-supervised learning, Salimans et al. (2016) made the following modiÔ¨Åcations. The discriminator is extended from a binary classiÔ¨Åer (data vs. generator sample) to a multi-class classiÔ¨Åer (one of kclasses of data samples, plus a class for generated samples). This classiÔ¨Åer is then trained to classify labeled real samples in the correct class, unlabeled real samples in any of the kclasses, and the generated samples in the additional class. 4Published as a conference paper at ICLR 2017 Although no formal results currently explain why yet, the technique was empirically demonstrated to greatly improve semi-supervised learning of classiÔ¨Åers on several datasets, especially when the classiÔ¨Åer is trained with feature matching loss (Salimans et al., 2016). Training the student in a semi-supervised fashion makes better use of the entire data available to the student, while still only labeling a subset of it. Unlabeled inputs are used in unsupervised learning to estimate a good prior for the distribution. Labeled inputs are then used for supervised learning. 3 P RIVACY ANALYSIS OF THE APPROACH We now analyze the differential privacy guarantees of our PATE approach. Namely, we keep track of the privacy budget throughout the student‚Äôs training using the moments accountant (Abadi et al., 2016). When teachers reach a strong quorum, this allows us to bound privacy costs more strictly. 3.1 D IFFERENTIAL PRIVACY PRELIMINARIES AND A SIMPLE ANALYSIS OF PATE Differential privacy (Dwork et al., 2006b; Dwork, 2011) has established itself as a strong standard. It provides privacy guarantees for algorithms analyzing databases, which in our case is a machine learning training algorithm processing a training dataset. Differential privacy is deÔ¨Åned using pairs of adjacent databases: in the present work, these are datasets that only differ by one training example. Recall the following variant of differential privacy introduced in Dwork et al. (2006a). DeÔ¨Ånition 1. A randomized mechanism Mwith domainDand rangeRsatisÔ¨Åes (";)-differential privacy if for any two adjacent inputs d;d02D and for any subset of outputs SR it holds that: Pr[M(d)2S]e"Pr[M(d0)2S] +: (2) It will be useful to deÔ¨Åne the privacy loss and the privacy loss random variable . They capture the differences in the probability distribution resulting from running Mondandd0. DeÔ¨Ånition 2. LetM:D!R be a randomized mechanism and d;d0a pair of adjacent databases. Letauxdenote an auxiliary input. For an outcome o2R, the privacy loss at ois deÔ¨Åned as: c(o;M;aux;d;d0)= logPr[M(aux;d) =o] Pr[M(aux;d0) =o]: (3) The privacy loss random variable C(M;aux;d;d0)is deÔ¨Åned as c(M(d);M;aux;d;d0), i.e. the random variable deÔ¨Åned by evaluating the privacy loss at an outcome sampled from M(d). A natural way to bound our approach‚Äôs privacy loss is to Ô¨Årst bound the privacy cost of each label queried by the student, and then use the strong composition theorem (Dwork et al., 2010) to derive the total cost of training the student. For neighboring databases d;d0, each teacher gets the same training data partition (that is, the same for the teacher with dand withd0, not the same across teachers), with the exception of one teacher whose corresponding training data partition differs. Therefore, the label counts nj(~ x)for any example ~ x, ondandd0differ by at most 1in at most two locations. In the next subsection, we show that this yields loose guarantees. 3.2 T HE MOMENTS ACCOUNTANT : A BUILDING BLOCK FOR BETTER ANALYSIS To better keep track of the privacy cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by Abadi et al. (2016), building on previous work (Bun & Steinke, 2016; Dwork & Rothblum, 2016; Mironov, 2016). DeÔ¨Ånition 3. LetM:D!R be a randomized mechanism and d;d0a pair of adjacent databases. Letauxdenote an auxiliary input. The moments accountant is deÔ¨Åned as: M()= max aux;d;d0M(;aux;d;d0) (4) whereM(;aux;d;d0)= log E[exp(C(M;aux;d;d0))]is the moment generating function of the privacy loss random variable. The following properties of the moments accountant are proved in Abadi et al. (2016). 5Published as a conference paper at ICLR 2017 Theorem 1. 1.[Composability] Suppose that a mechanism Mconsists of a sequence of adaptive mechanismsM1;:::;MkwhereMi:Qi 1 j=1RjD!R i. Then, for any output sequence o1;:::;ok 1and any M(;d;d0) =kX i=1Mi(;o1;:::;oi 1;d;d0); whereMis conditioned onMi‚Äôs output being oifori<k . 2.[Tail bound] For any">0, the mechanismMis(";)-differentially private for = min exp(M() "): We write down two important properties of the aggregation mechanism from Section 2. The Ô¨Årst property is proved in Dwork & Roth (2014), and the second follows from Bun & Steinke (2016). Theorem 2. Suppose that on neighboring databases d;d0, the label counts njdiffer by at most 1 in each coordinate. Let Mbe the mechanism that reports arg maxjn nj+Lap(1  )o . ThenM satisÔ¨Åes (2 ;0)-differential privacy. Moreover, for any l,aux,dandd0, (l;aux;d;d0)2 2l(l+ 1) (5) At each step, we use the aggregation mechanism with noise Lap(1  )which is (2 ;0)-DP. Thus over Tsteps, we get (4T 2+ 2 q 2Tln1 ;)-differential privacy. This can be rather large: plugging in values that correspond to our SVHN result,  = 0:05;T= 1000;= 1e 6gives us"26or alternatively plugging in values that correspond to our MNIST result,  = 0:05;T= 100;= 1e 5 gives us"5:80. 3.3 A PRECISE ,DATA -DEPENDENT PRIVACY ANALYSIS OF PATE Our data-dependent privacy analysis takes advantage of the fact that when the quorum among the teachers is very strong, the majority outcome has overwhelming likelihood, in which case the privacy cost is small whenever this outcome occurs. The moments accountant allows us analyze the composition of such mechanisms in a uniÔ¨Åed framework. The following theorem, proved in Appendix A, provides a data-dependent bound on the moments of any differentially private mechanism where some speciÔ¨Åc outcome is very likely. Theorem 3. LetMbe(2 ;0)-differentially private and qPr[M(d)6=o]for some outcome o. Letl; 0andq<e2  1 e4  1. Then for any auxand any neighbor d0ofd,MsatisÔ¨Åes (l;aux;d;d0)log((1 q)1 q 1 e2 ql +qexp(2 l)): To upper bound qfor our aggregation mechanism, we use the following simple lemma, also proved in Appendix A. Lemma 4. Letnbe the label score vector for a database dwithnjnjfor allj. Then Pr[M(d)6=j]X j6=j2 + (nj nj) 4 exp( (nj nj)) This allows us to upper bound qfor a speciÔ¨Åc score vector n, and hence bound speciÔ¨Åc moments. We take the smaller of the bounds we get from Theorems 2 and 3. We compute these moments for a few values of(integers up to 8). Theorem 1 allows us to add these bounds over successive steps, and derive an (";)guarantee from the Ô¨Ånal . Interested readers are referred to the script that we used to empirically compute these bounds, which is released along with our code: https://github. com/tensorflow/models/tree/master/differential_privacy/multiple_teachers 6Published as a conference paper at ICLR 2017 Since the privacy moments are themselves now data dependent, the Ô¨Ånal "is itself data-dependent and should not be revealed. To get around this, we bound the smooth sensitivity (Nissim et al., 2007) of the moments and add noise proportional to it to the moments themselves. This gives us a differentially private estimate of the privacy cost. Our evaluation in Section 4 ignores this overhead and reports the un-noised values of ". Indeed, in our experiments on MNIST and SVHN, the scale of the noise one needs to add to the released "is smaller than 0.5 and 1.0 respectively. How does the number of teachers affect the privacy cost? Recall that the student uses a noisy label computed in (1) which has a parameter  . To ensure that the noisy label is likely to be the correct one, the noise scale1  should be small compared to the the additive gap between the two largest vales ofnj. While the exact dependence of  on the privacy cost in Theorem 3 is subtle, as a general principle, a smaller  leads to a smaller privacy cost. Thus, a larger gap translates to a smaller privacy cost. Since the gap itself increases with the number of teachers, having more teachers would lower the privacy cost. This is true up to a point. With nteachers, each teacher only trains on a1 nfraction of the training data. For large enough n, each teachers will have too little training data to be accurate. To conclude, we note that our analysis is rather conservative in that it pessimistically assumes that, even if just one example in the training set for one teacher changes, the classiÔ¨Åer produced by that teacher may change arbitrarily. One advantage of our approach, which enables its wide applicability, is that our analysis does not require any assumptions about the workings of the teachers. Nevertheless, we expect that stronger privacy guarantees may perhaps be established in speciÔ¨Åc settings‚Äîwhen assumptions can be made on the learning algorithm used to train the teachers. 4 E VALUATION In our evaluation of PATE and its generative variant PATE-G, we Ô¨Årst train a teacher ensemble for each dataset. The trade-off between the accuracy and privacy of labels predicted by the ensemble is greatly dependent on the number of teachers in the ensemble: being able to train a large set of teachers is essential to support the injection of noise yielding strong privacy guarantees while having a limited impact on accuracy. Second, we minimize the privacy budget spent on learning the student by training it with as few queries to the ensemble as possible. Our experiments use MNIST and the extended SVHN datasets. Our MNIST model stacks two convolutional layers with max-pooling and one fully connected layer with ReLUs. When trained on the entire dataset, the non-private model has a 99:18% test accuracy. For SVHN, we add two hidden layers.1The non-private model achieves a 92:8%test accuracy, which is shy of the state-of-the-art. However, we are primarily interested in comparing the private student‚Äôs accuracy with the one of a non-private model trained on the entire dataset, for different privacy guarantees. The source code for reproducing the results in this section is available on GitHub.2 4.1 T RAINING AN ENSEMBLE OF TEACHERS PRODUCING PRIVATE LABELS As mentioned above, compensating the noise introduced by the Laplacian mechanism presented in Equation 1 requires large ensembles. We evaluate the extent to which the two datasets considered can be partitioned with a reasonable impact on the performance of individual teachers. SpeciÔ¨Åcally, we show that for MNIST and SVHN, we are able to train ensembles of 250teachers. Their aggregated predictions are accurate despite the injection of large amounts of random noise to ensure privacy. The aggregation mechanism output has an accuracy of 93:18% for MNIST and 87:79% for SVHN, when evaluated on their respective test sets, while each query has a low privacy budget of "= 0:05. Prediction accuracy: All other things being equal, the number nof teachers is limited by a tradeoff between the classiÔ¨Åcation task‚Äôs complexity and the available data. We train nteachers by partitioning the training data n-way. Larger values of nlead to larger absolute gaps, hence potentially allowing for a larger noise level and stronger privacy guarantees. At the same time, a larger nimplies a smaller training dataset for each teacher, potentially reducing the teacher accuracy. We empirically Ô¨Ånd appropriate values of nfor the MNIST and SVHN datasets by measuring the test 1The model is adapted from https://www.tensorflow.org/tutorials/deep_cnn 2https://github.com/tensorflow/models/tree/master/differential_privacy/multiple_teachers 7Published as a conference paper at ICLR 2017 "  Figure 2: How much noise can be injected to a query? Accuracy of the noisy aggregation for three MNIST and SVHN teacher ensembles and varying  value per query. The noise introduced to achieve a given  scales inversely proportionally to the value of  : small values of  on the left of the axis correspond to large noise amplitudes and large   values on the right to small noise. Figure 3: How certain is the aggregation of teacher predictions? Gap between the number of votes assigned to the most and second most frequent labels normalized by the number of teachers in an ensemble. Larger gaps indicate that the ensemble is conÔ¨Ådent in assigning the labels, and will be robust to more noise injection. Gaps were computed by averaging over the test data. set accuracy of each teacher trained on one of the npartitions of the training data. We Ô¨Ånd that even forn= 250 , the average test accuracy of individual teachers is 83:86% for MNIST and 83:18% for SVHN. The larger size of SVHN compensates its increased task complexity. Prediction conÔ¨Ådence: As outlined in Section 3, the privacy of predictions made by an ensemble of teachers intuitively requires that a quorum of teachers generalizing well agree on identical labels. This observation is reÔ¨Çected by our data-dependent privacy analysis, which provides stricter privacy bounds when the quorum is strong. We study the disparity of labels assigned by teachers. In other words, we count the number of votes for each possible label, and measure the difference in votes between the most popular label and the second most popular label, i.e., the gap. If the gap is small, introducing noise during aggregation might change the label assigned from the Ô¨Årst to the second. Figure 3 shows the gap normalized by the total number of teachers n. Asnincreases, the gap remains larger than 60% of the teachers, allowing for aggregation mechanisms to output the correct label in the presence of noise. Noisy aggregation: For MNIST and SVHN, we consider three ensembles of teachers with varying number of teachers n2f10;100;250g. For each of them, we perturb the vote counts with Laplacian noise of inversed scale  ranging between 0:01and1. This choice is justiÔ¨Åed below in Section 4.2. We report in Figure 2 the accuracy of test set labels inferred by the noisy aggregation mechanism for these values of ". Notice that the number of teachers needs to be large to compensate for the impact of noise injection on the accuracy. 4.2 S EMI-SUPERVISED TRAINING OF THE STUDENT WITH PRIVACY The noisy aggregation mechanism labels the student‚Äôs unlabeled training set in a privacy-preserving fashion. To reduce the privacy budget spent on student training, we are interested in making as few label queries to the teachers as possible. We therefore use the semi-supervised training approach described previously. Our MNIST and SVHN students with (";)differential privacy of (2:04;10 5) and(8:19;10 6)achieve accuracies of 98:00% and90:66%. These results improve the differential privacy state-of-the-art for these datasets. Abadi et al. (2016) previously obtained 97% accuracy with a (8;10 5)bound on MNIST, starting from an inferior baseline model without privacy. Shokri & Shmatikov (2015) reported about 92% accuracy on SVHN with ">2per model parameter and a model with over 300,000parameters. Naively, this corresponds to a total ">600,000. 8Published as a conference paper at ICLR 2017 Dataset " Queries Non-Private Baseline Student Accuracy MNIST 2.04 10 5100 99.18% 98.00% MNIST 8.03 10 51000 99.18% 98.10% SVHN 5.04 10 6500 92.80% 82.72% SVHN 8.19 10 61000 92.80% 90.66% Figure 4: Utility and privacy of the semi-supervised students: each row is a variant of the student model trained with generative adversarial networks in a semi-supervised way, with a different number of label queries made to the teachers through the noisy aggregation mechanism. The last column reports the accuracy of the student and the second and third column the bound "and failure probabilityof the (";)differential privacy guarantee. We apply semi-supervised learning with GANs to our problem using the following setup for each dataset. In the case of MNIST, the student has access to 9,000samples, among which a subset of either 100,500, or1,000samples are labeled using the noisy aggregation mechanism discussed in Section 2.1. Its performance is evaluated on the 1,000remaining samples of the test set. Note that this may increase the variance of our test set accuracy measurements, when compared to those computed over the entire test data. For the MNIST dataset, we randomly shufÔ¨Çe the test set to ensure that the different classes are balanced when selecting the (small) subset labeled to train the student. For SVHN, the student has access to 10,000training inputs, among which it labels 500or1,000 samples using the noisy aggregation mechanism. Its performance is evaluated on the remaining 16,032samples. For both datasets, the ensemble is made up of 250teachers. We use Laplacian scale of20to guarantee an individual query privacy bound of "= 0:05. These parameter choices are motivated by the results from Section 4.1. In Figure 4, we report the values of the (";)differential privacy guarantees provided and the corresponding student accuracy, as well as the number of queries made by each student. The MNIST student is able to learn a 98% accurate model, which is shy of 1%when compared to the accuracy of a model learned with the entire training set, with only 100label queries. This results in a strict differentially private bound of "= 2:04for a failure probability Ô¨Åxed at 10 5. The SVHN student achieves 90:66% accuracy, which is also comparable to the 92:80% accuracy of one teacher learned with the entire training set. The corresponding privacy bound is "= 8:19, which is higher than for the MNIST dataset, likely because of the larger number of queries made to the aggregation mechanism. We observe that our private student outperforms the aggregation‚Äôs output in terms of accuracy, with or without the injection of Laplacian noise. While this shows the power of semi-supervised learning, the student may not learn as well on different kinds of data (e.g., medical data), where categories are not explicitly designed by humans to be salient in the input space. Encouragingly, as Appendix C illustrates, the PATE approach can be successfully applied to at least some examples of such data. 5 D ISCUSSION AND RELATED WORK Several privacy deÔ¨Ånitions are found in the literature. For instance, k-anonymity requires information about an individual to be indistinguishable from at least k 1other individuals in the dataset (L. Sweeney, 2002). However, its lack of randomization gives rise to caveats (Dwork & Roth, 2014), and attackers can infer properties of the dataset (Aggarwal, 2005). An alternative deÔ¨Ånition, differential privacy , established itself as a rigorous standard for providing privacy guarantees (Dwork et al., 2006b). In contrast to k-anonymity, differential privacy is a property of the randomized algorithm and not the dataset itself. A variety of approaches and mechanisms can guarantee differential privacy. Erlingsson et al. (2014) showed that randomized response, introduced by Warner (1965), can protect crowd-sourced data collected from software users to compute statistics about user behaviors. Attempts to provide differential privacy for machine learning models led to a series of efforts on shallow machine learning models, including work by Bassily et al. (2014); Chaudhuri & Monteleoni (2009); Pathak et al. (2011); Song et al. (2013), and Wainwright et al. (2012). 9Published as a conference paper at ICLR 2017 A privacy-preserving distributed SGD algorithm was introduced by Shokri & Shmatikov (2015). It applies to non-convex models. However, its privacy bounds are given per-parameter, and the large number of parameters prevents the technique from providing a meaningful privacy guarantee. Abadi et al. (2016) provided stricter bounds on the privacy loss induced by a noisy SGD by introducing the moments accountant. In comparison with these efforts, our work increases the accuracy of a private MNIST model from 97% to98% while improving the privacy bound "from 8to1:9. Furthermore, the PATE approach is independent of the learning algorithm, unlike this previous work. Support for a wide range of architecture and training algorithms allows us to obtain good privacy bounds on an accurate and private SVHN model. However, this comes at the cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by (Abadi et al., 2016; Shokri & Shmatikov, 2015). Pathak et al. (2010) Ô¨Årst discussed secure multi-party aggregation of locally trained classiÔ¨Åers for a global classiÔ¨Åer hosted by a trusted third-party. Hamm et al. (2016) proposed the use of knowledge transfer between a collection of models trained on individual devices into a single model guaranteeing differential privacy. Their work studied linear student models with convex and continuously differentiable losses, bounded and c-Lipschitz derivatives, and bounded features. The PATE approach of this paper is not constrained to such applications, but is more generally applicable. Previous work also studied semi-supervised knowledge transfer from private models. For instance, Jagannathan et al. (2013) learned privacy-preserving random forests. A key difference is that their approach is tailored to decision trees. PATE works well for the speciÔ¨Åc case of decision trees, as demonstrated in Appendix C, and is also applicable to other machine learning algorithms, including more complex ones. Another key difference is that Jagannathan et al. (2013) modiÔ¨Åed the classic model of a decision tree to include the Laplacian mechanism. Thus, the privacy guarantee does not come from the disjoint sets of training data analyzed by different decision trees in the random forest, but rather from the modiÔ¨Åed architecture. In contrast, partitioning is essential to the privacy guarantees of the PATE approach. 6 C ONCLUSIONS To protect the privacy of sensitive training data, this paper has advanced a learning strategy and a corresponding privacy analysis. The PATE approach is based on knowledge aggregation and transfer from ‚Äúteacher‚Äù models, trained on disjoint data, to a ‚Äústudent‚Äù model whose attributes may be made public. In combination, the paper‚Äôs techniques demonstrably achieve excellent utility on the MNIST and SVHN benchmark tasks, while simultaneously providing a formal, state-of-the-art bound on users‚Äô privacy loss. While our results are not without limits‚Äîe.g., they require disjoint training data for a large number of teachers (whose number is likely to increase for tasks with many output classes)‚Äîthey are encouraging, and highlight the advantages of combining semi-supervised learning with precise, data-dependent privacy analysis, which will hopefully trigger further work. In particular, such future work may further investigate whether or not our semi-supervised approach will also reduce teacher queries for tasks other than MNIST and SVHN, for example when the discrete output categories are not as distinctly deÔ¨Åned by the salient input space features. A key advantage is that this paper‚Äôs techniques establish a precise guarantee of training data privacy in a manner that is both intuitive and rigorous. Therefore, they can be appealing, and easily explained, to both an expert and non-expert audience. However, perhaps equally compelling are the techniques‚Äô wide applicability. Both our learning approach and our analysis methods are ‚Äúblackbox,‚Äù i.e., independent of the learning algorithm for either teachers or students, and therefore apply, in general, to non-convex, deep learning, and other learning methods. Also, because our techniques do not constrain the selection or partitioning of training data, they apply when training data is naturally and non-randomly partitioned‚Äîe.g., because of privacy, regulatory, or competitive concerns‚Äî or when each teacher is trained in isolation, with a different method. We look forward to such further applications, for example on RNNs and other sequence-based models. ACKNOWLEDGMENTS Nicolas Papernot is supported by a Google PhD Fellowship in Security. The authors would like to thank Ilya Mironov and Li Zhang for insightful discussions about early drafts of this document. 10Published as a conference paper at ICLR 2017 