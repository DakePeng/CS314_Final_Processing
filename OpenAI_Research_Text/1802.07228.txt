Future of  Humanity  InstituteUniversity   of OxfordCentre for  the Study of  Existential  RiskUniversity of  CambridgeCenter for a  New American  SecurityElectronic  Frontier  FoundationOpenAI February 2018The Malicious Use   of Artificial Intelligence:  Forecasting, Prevention,   and Mitigation The Malicious Use of Artificial Intelligence:   Forecasting, Prevention, and Mitigation Authors are listed   in order of contributionDesign Direction   by Sankalp Bhatnagar   and Talia Cotton February 2018 1 Corresponding author  miles.brundage@philosophy. ox.ac.uk Future of Humanity Institute,  University of Oxford; Arizona  State University 2 Corresponding author,  sa478@cam.ac.uk Centre for the Study of  Existential Risk, University  of Cambridge 3 OpenAI 4 Open Philanthropy Project 5 Electronic Frontier  Foundation 6 Future of Humanity  Institute, University of  Oxford 7 Future of Humanity  Institute, University of  Oxford; Yale University 8 Center for a New American  Security 9 American University 10 Endgame 11 Endgame 12 University of Oxford/ Arizona State University/New  America Foundation 13 Center for a New American  Security 14 Stanford University 15 Future of Humanity  Institute, University of  Oxford 16 Centre for the Study  of Existential Risk and  Centre for the Future of  Intelligence, University of  Cambridge 17 Centre for the Study of  Existential Risk, University  of Cambridge18 Centre for the Study of  Existential Risk, University  of Cambridge 19 Future of Humanity  Institute, University of  Oxford 20 Future of Humanity  Institute, University of  Oxford 21 Information Society  Project, Yale University 22 Future of Humanity  Institute, University of  Oxford 23 OpenAI 24 University of Bath 25 University of Louisville 26 OpenAI Miles Brundage   Shahar Avin   Jack Clark Helen Toner Peter Eckersley Ben Garfinkel Allan Dafoe Paul Scharre Thomas Zeitzoff Bobby Filar Hyrum Anderson Heather Roff Gregory C. Allen Jacob Steinhardt Carrick Flynn Seán Ó hÉigeartaigh Simon Beard Haydn Belfield   Sebastian Farquhar   Clare Lyle Rebecca Crootof Owain Evans Michael Page  Joanna Bryson  Roman Yampolskiy Dario Amodei Artificial intelligence and machine learning capabilities are growing  at an unprecedented rate. These technologies have many widely  beneficial applications, ranging from machine translation to medical  image analysis. Countless more such applications are being  developed and can be expected over the long term. Less attention  has historically been paid to the ways in which artificial intelligence  can be used maliciously.  This report surveys the landscape of  potential security threats from malicious uses of artificial intelligence  technologies, and proposes ways to better forecast, prevent, and  mitigate these threats. We analyze, but do not conclusively resolve,  the question of what the long-term equilibrium between attackers and  defenders will be. We focus instead on what sorts of attacks we are  likely to see soon if adequate defenses are not developed. p.3Executive Summary p.4 Executive Summary The Malicious Use of Artificial IntelligenceIn response to the changing threat landscape we make four high-level  recommendations : 1. Policymakers should collaborate closely with technical  researchers to investigate, prevent, and mitigate potential  malicious uses of AI. 2. Researchers and engineers in artificial intelligence should take  the dual-use nature of their work seriously, allowing misuserelated considerations to influence research priorities and  norms, and proactively reaching out to relevant actors when  harmful applications are foreseeable. 3. Best practices should be identified in research areas with more  mature methods for addressing dual-use concerns, such as  computer security, and imported where applicable to the case  of AI. 4. Actively seek to expand the range of stakeholders and domain  experts involved in discussions of these challenges.p.5 Executive Summary The Malicious Use of Artificial IntelligenceAs AI capabilities become more powerful and widespread, we expect  the growing use of AI systems to lead to the following changes in the  landscape of threats:   • Expansion of existing threats . The costs of attacks may be  lowered by the scalable use of AI systems to complete tasks  that would ordinarily require human labor, intelligence and  expertise. A natural effect would be to expand the set of actors  who can carry out particular attacks, the rate at which they can  carry out these attacks, and the set of potential targets.  • Introduction of new threats . New attacks may arise through the  use of AI systems to complete tasks that would be otherwise  impractical for humans. In addition, malicious actors may  exploit the vulnerabilities of AI systems deployed by defenders.  • Change to the typical character of threats . We believe there is  reason to expect attacks enabled by the growing use of AI to  be especially effective, finely targeted, difficult to attribute,  and likely to exploit vulnerabilities in AI systems. p.6 Executive Summary The Malicious Use of Artificial IntelligenceWe structure our analysis by separately considering three security  domains, and illustrate possible changes to threats within these  domains through representative examples: • Digital security . The use of AI to automate tasks involved in  carrying out cyberattacks will alleviate the existing tradeoff  between the scale and efficacy of attacks. This may expand  the threat associated with labor-intensive cyberattacks  (such as spear phishing). We also expect novel attacks that  exploit human vulnerabilities (e.g. through the use of speech  synthesis for impersonation), existing software vulnerabilities  (e.g. through automated hacking), or the vulnerabilities  of AI systems (e.g. through adversarial examples and data  poisoning). • Physical security . The use of AI to automate tasks involved in  carrying out attacks with drones and other physical systems  (e.g. through the deployment of autonomous weapons  systems) may expand the threats associated with these  attacks. We also expect novel attacks that subvert cyberphysical systems (e.g. causing autonomous vehicles to crash)  or involve physical systems that it would be infeasible to direct  remotely (e.g. a swarm of thousands of micro-drones). • Political security . The use of AI to automate tasks involved in  surveillance (e.g. analysing mass-collected data), persuasion  (e.g. creating targeted propaganda), and deception (e.g.  manipulating videos) may expand threats associated with  privacy invasion and social manipulation. We also expect novel  attacks that take advantage of an improved capacity to analyse  human behaviors, moods, and beliefs on the basis of available  data.  These concerns are most significant in the context of  authoritarian states, but may also undermine the ability of  democracies to sustain truthful public debates. p.7 Executive Summary The Malicious Use of Artificial IntelligenceIn addition to the high-level recommendations listed above, we also  propose the exploration of several open questions and potential  interventions within four priority research areas: • Learning from and with the cybersecurity community . At the  intersection of cybersecurity and AI attacks, we highlight  the need to explore and potentially implement red teaming,  formal verification, responsible disclosure of AI vulnerabilities,  security tools, and secure hardware. • Exploring different openness models . As the dual-use nature  of AI and ML becomes apparent, we highlight the need to  reimagine norms and institutions around the openness of  research, starting with pre-publication risk assessment in  technical areas of special concern, central access licensing  models, sharing regimes that favor safety and security, and  other lessons from other dual-use technologies.  • Promoting a culture of responsibility . AI researchers and the  organisations that employ them are in a unique position to  shape the security landscape of the AI-enabled world. We  highlight the importance of education, ethical statements and  standards, framings, norms, and expectations. • Developing technological and policy solutions . In addition to  the above, we survey a range of promising technologies, as  well as policy interventions, that could help build a safer future  with AI. High-level areas for further research include privacy  protection, coordinated use of AI for public-good security,  monitoring of AI-relevant resources, and other legislative and  regulatory responses.   The proposed interventions require attention and action not just from  AI researchers and companies but also from legislators, civil servants,  regulators, security researchers and educators. The challenge is  daunting and the stakes are high.Executive Summary Introduction   Scope   Related Literature General Framework   for AI and Security Threats   AI Capabilities   Security-Relevant Properties of AI   General Implications Scenarios   Digital Security   Physical Security   Political Security Security Domains   Digital Security   Physical Security   Political Security Interventions   Recommendations   Priority Areas for Further Research Strategic Analysis   Factors Affecting the Equilibrium of AI and Security   Overall Assessment Conclusion Acknowledgements   