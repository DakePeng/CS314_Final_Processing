Published as a conference paper at ICLR 2017 ON THE QUANTITATIVE ANALYSIS OF DECODER BASED GENERATIVE MODELS Yuhuai Wu Department of Computer Science University of Toronto ywu@cs.toronto.eduYuri Burda OpenAI yburda@openai.comRuslan Salakhutdinov School of Computer Science Carnegie Mellon University rsalakhu@cs.cmu.edu Roger Grosse Department of Computer Science University of Toronto rgrosse@cs.toronto.edu ABSTRACT The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that deﬁnes a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difﬁcult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at https:// github.com/tonywu95/eval_gen . Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overﬁtting, and the degree to which these models miss important modes of the data distribution. 1 I NTRODUCTION In recent years, deep generative models have dramatically pushed forward the state-of-the-art in generative modelling by generating convincing samples of images (Radford et al., 2016), achieving state-of-the-art semi-supervised learning results (Salimans et al., 2016), and enabling automatic image manipulation (Zhu et al., 2016). Many of the most successful approaches are deﬁned in terms of a process which samples latent variables from a simple ﬁxed distribution (such as Gaussian or uniform) and then applies a learned deterministic mapping which we will refer to as a decoder network. Important examples include variational autoencoders (V AEs) (Kingma & Welling, 2014; Rezende et al., 2014), generative adversarial networks (GANs) (Goodfellow et al., 2014), generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015), and nonlinear independent components estimation (Dinh et al., 2014). We refer to this set of models collectively as decoder-based models, also known as density networks (MacKay & Gibbs, 1998). While many decoder-based models are able to produce convincing samples (Denton et al., 2015; Radford et al., 2016), rigorous evaluation remains a challenge. Comparing models by inspecting samples is labor-intensive, and potentially misleading (Theis et al., 2016). While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model’s performance. Unfortunately, unless the decoder is designed to be reversible (Dinh et al., 2014; 2016), log-likelihood estimation in decoder-based models is typically intractable. In the case of V AE-based models, a learned encoder network gives a tractable lower bound, but for GANs and GMMNs it is not obvious how even to compute a good lower bound. Even when lower bounds are available, their accuracy may be hard to determine. Because of the difﬁculty of log-likelihood 1 arXiv:1611.04273v2  [cs.LG]  6 Jun 2017Published as a conference paper at ICLR 2017 (a) GAN-10; LLD: 328.7 (b) GAN-50, epoch 200; LLD: 543.5 (c) GAN-50, epoch 1000; LLD: 625.5 Figure 1: (a) samples from a GAN with 10 latent dimensions, (b) and (c) samples from a GAN with 50 latent dimensions at different epochs of training. While it is difﬁcult to visually discern differences between these three models, their log-likelihood (LLD) values span almost 300 nats. evaluation, it is hard to answer basic questions such as whether the networks are simply memorizing training examples, or whether they are missing important modes of the data distribution. The most widely used estimator of log-likelihood for GANs and GMMNs is the Kernel Density Estimator (KDE) (Parzen, 1962). It estimates the likelihood under an approximation to the model’s distribution obtained by simulating from the model and convolving the set of samples with a kernel (typically Gaussian). Unfortunately, KDE is notoriously inaccurate for estimating likelihood in high dimensions, because it is hard to tile a high-dimensional manifold with spherical Gaussians (Theis et al., 2016). In this paper, we propose to use annealed importance sampling (AIS; (Neal, 2001)) to estimate log-likelihoods of decoder-based generative models and to obtain approximate posterior samples. Importantly, we validate this approach using Bidirectional Monte Carlo (BDMC) (Grosse et al., 2015), which provably bounds the log-likelihood estimation error and the KL divergence from the true posterior distribution for data simulated from a model. For most models we consider, we ﬁnd that AIS is two orders of magnitude more accurate than KDE, and is accurate enough to perform ﬁne-grained comparisons between generative models. In the case of V AEs, we show that AIS can be further sped up by using the recognition network to determine the initial distribution; this yields an estimator which is fast enough to be run repeatedly during training. Using the proposed method, we analyze several scientiﬁc questions central to understanding decoderbased generative models. First, we measure the accuracy of KDE and of the importance weighting bound which is commonly used to evaluate V AEs. We ﬁnd that the KDE error is larger than the (quite signiﬁcant) log-likelihood differences between different models, and that KDE can lead to misleading conclusions. The importance weighted bound, while reasonably accurate, can also yield misleading results in some cases. Second, we compare the log-likelihoods of V AEs, GANs, and GMMNs, and ﬁnd that V AEs achieve log-likelihoods several hundred nats higher than the other models (even though KDE considers all three models to have roughly the same log-likelihood). Third, we analyze the degree of overﬁtting in V AEs, GANs, and GMMNs. Contrary to a commonly proposed hypothesis, we ﬁnd that GANs and GMMNs are notsimply memorizing their training data; in fact, their log-likelihood gaps between training and test data are much smaller relative to comparably-sized V AEs. Finally, by visualizing (approximate) posterior samples obtained from AIS, we observe that GANs miss important modes of the data distribution, even ones which are represented in the training data. We emphasize that none of the above phenomena can be measured using KDE or the importance weighted bound, or by inspecting samples. (See Fig. 1 for an example where it is tricky to compare models based on samples.) While log-likelihood is by no means a perfect measure, we ﬁnd that the ability to accurately estimate log-likelihoods of decoder-based models yields crucial insight into their behavior and suggests directions for improving them. 2 B ACKGROUND 2.1 D ECODER -BASED GENERATIVE MODELS In generative modelling, a decoder network is often used to deﬁne a generative distribution by transforming samples from some simple distribution (e.g. normal) to the data manifold. In this 2Published as a conference paper at ICLR 2017 paper, we consider three kinds of decoder-based generative models: Variational Autoencoder (V AE) (Kingma & Welling, 2014), Generative Adversarial Network (GAN) (Goodfellow et al., 2014), and Generative Moment Matching Network (GMMN) (Li & Swersky, 2015; Dziugaite et al., 2015). 2.1.1 V ARIATIONAL AUTOENCODER A variational autoencoder (V AE) (Kingma & Welling, 2014) is a probabilistic directed graphical model. It is deﬁned by a joint distribution over a set of latent random variables zand the observed variablesx:p(x,z) =p(x|z)p(z). The prior over the latent random variables, p(z), is usually chosen to be a standard Gaussian distribution. The data likelihood p(x|z)is usually a Gaussian or Bernoulli distribution whose parameters depend on zthrough a deep neural network, known as the decoder network. It also uses an approximate inference model called an encoder or recognition network, that serves as a variational approximation q(z|x)to the posterior p(z|x). The decoder network and the encoder networks are jointly trained to maximize the evidence lower bound (ELBO): logp(x)≥Eq(z|x)[logp(x|z)]−KL(q(z|x)||p(z)) (1) In addition, the reparametrization trick is used to reduce the variance of the gradient estimate. 2.1.2 G ENERATIVE ADVERSARIAL NETWORK (GAN) A generative adversarial network (GAN) (Goodfellow et al., 2014) is a generative model trained by a game between a decoder network and a discriminator network. It deﬁnes the generative model by sampling the latent variable zfrom some simple prior distribution p(z)(e.g., Gaussian) followed through the decoder network. The discriminator network D(·)outputs a probability of a given sample coming from the data distribution. Its task is to distinguish samples from the generator distribution from real data. The decoder network, on the other hand, tries to produce samples as realistic as possible, in order to fool the discriminator into accepting its outputs as being real. The competition between the two networks results in the following minimax problem: min Gmax DEx∼pdata[logD(x)] +Ez∼p(z)[log(1−D(G(z))] (2) Unlike V AE, the objective is not explicitly related to the log-likelihood of the data. Moreover, the generative distribution is a deterministic mapping, i.e., p(x|z)is a Dirac delta distribution, parametrized by the deterministic decoder. This can make data likelihood ill-deﬁned, as the probability density of any particular point xcan be either inﬁnite, or exactly zero. 2.1.3 G ENERATIVE MOMENT MATCHING NETWORK (GMMN) Generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015) adopt maximum mean discrepancy (MMD) as the training objective, a moment matching criterion where kernel mean embedding techniques are used to avoid unnecessary assumptions of the distributions. It has the same issue as GAN in that the log-likelihood is undeﬁned. 2.2 A NNEALED IMPORTANCE SAMPLING We are interested in estimating the probability p(x) =/integraltext p(z)p(x|z) dza model assigns to an observation x. This is equivalent to computing the normalizing constant of the unnormalized distribution f(z) =p(z,x). One naïve approach is likelihood weighting, where one samples {z(k)}K k=1∼p(z)and averages the conditional likelihoods p(x|z(k)). This is justiﬁed by the following identity: p(x) =/integraldisplayp(x,z) p(z)p(z) dz=Ez∼p(z)[p(x|z)] (3) Likelihood weighting can be viewed as simple importance sampling, where the proposal distribution is the priorp(z)and the target distribution is the posterior p(z|x). Unfortunately, importance sampling works well only when the proposal distribution is a good match for the target distribution. For the models considered in this paper, the (very broad) prior can be drastically different than the (highly concentrated) posterior, leading to inaccurate estimates of the likelihood. Annealed importance sampling (AIS; Neal, 2001) is a Monte Carlo algorithm commonly used to estimate (ratios of) normalizing constants. Roughly speaking, it computes a sequence of importance 3Published as a conference paper at ICLR 2017 sampling based estimates, each of which is stable because it involves two distributions which are very similar. In particular, suppose one is interested in estimating the normalizing constant Z=/integraltext f(z) dz of an unnormalized distribution f(z). (In the likelihood estimation setting, f(z) =p(z,x)and Z=p(x).) One must specify a sequence of distributions q1,...,qT, whereqt=ft/Zt, andfT=f is the target distribution. It is required that one can obtain one or more exact samples from the initial distributionq1. One must also specify a sequence of reversible MCMC transition operators T1,...,TT, whereTtleavesqtinvariant. AIS produces a (nonnegative) unbiased estimate of the ratio ZT/Z1as follows: ﬁrst, we sample a random initial state z1∼q1and set the initial weight w1= 1. For every stage t≥2we update the weightwand sample the state ztaccording to wt←wt−1ft(zt−1) ft−1(zt−1)zt∼Tt(z|zt−1) (4) As demonstrated by Neal (2001), this procedure produces a nonnegative weight wTsuch that E[wT] =ZT/Z1. Typically,Z1is known, so one computes multiple independent AIS weights {w(K) T}K k=1and obtains the unbiased estimate ˆZT=Z11 K/summationtextK k=1w(K) T. In the likelihood estimation setting,Z1= 1andZT=p(x), so we denote this estimator as ˆp(x). Typically, the unnormalized intermediate distributions are simply deﬁned to be geometric averages ft(z) =f1(z)1−βtfT(z)βt, where theβtare monotonically increasing parameters with β1= 0and βT= 1. Forf1(z) =p(z)andfT(z) =p(z,x), this gives ft(z) =p(z)p(x|z)βt. (5) As shown by Neal (2001), under certain regularity conditions, the variance of ˆZTtends to zero as the number of intermediate distributions is increased. AIS is very effective in practice, and has been used to estimate normalizing constants of complex high-dimensional distributions (Salakhutdinov & Murray, 2008). 2.3 B IDIRECTIONAL MONTE CARLO AIS provides a nonnegative unbiased estimate ˆp(x)ofp(x). However, it is often more practical to estimatep(x)in the log space, i.e. logp(x), because of underﬂow problem of dealing with many products of probability measure. In general, we note that logarithm of a nonnegative unbiased estimate is astochastic lower bound of the log estimand (Grosse et al., 2015). In particular, log ˆp(x)is a stochastic lower bound on logp(x), satisfying E[log ˆp(x)]≤logp(x)andPr(log ˆp(x)>logp(x) + b)<e−b. Grosse et al. (2015) pointed out that if AIS is run in reverse starting from an exact posterior sample, it yields an unbiased estimate of 1/p(x), which (by the above argument) can be seen as a stochastic upper bound on logp(x). The combination of lower and upper bounds from forward and reverse AIS is known as bidirectional Monte Carlo (BDMC). In many cases, the combination of bounds can pinpoint the true value quite precisely. While posterior sampling is just as hard as log-likelihood estimation (Jerrum et al., 1986), in the case of log-likelihood estimation for simulated data, one has available a single exact posterior sample: the parameters and/or latent variables which generated the data. Because this trick is only applicable to simulated data, BDMC is most useful for measuring the accuracy of a log-likelihood estimator on simulated data. Grosse et al. (2016) observed that BDMC can also be used to validate posterior inference algorithms, as the gap between upper and lower bounds is itself a bound on the KL divergence of approximate samples from the true posterior distribution. 3 M ETHODOLOGY For a given generative distribution p(x,z) =p(z)p(x|z), our task is to measure the log-likelihood of test examples logp(xtest). We ﬁrst discuss how we deﬁne the generative distribution for decoderbased networks. For V AE, the generative distribution is deﬁned in the standard way, where p(z)is a standard normal distribution and p(x|z)is a normal distribution parametrized by mean µθ(z)and σθ(z), predicted by the generator given the latent code. However, the observation distribution for GANs and GMMNs is typically taken to be a delta function, so that the model’s distribution covers 4Published as a conference paper at ICLR 2017 only a submanifold of the space of observables. In order for the likelihood to be well-deﬁned, we follow the same assumption made when evaluating using Kernel Density Estimator (Parzen, 1962): we assume a Gaussian observation model with a ﬁxed variance hyperparameter σ2. We will refer to the distribution deﬁned by this Gaussian observation model as pσ. Observe that the KDE estimate is given by ˆpσ(x) =1 KK/summationdisplay k=1pσ(x|z(k)), (6) where{z(k)}K k=1are samples from the prior p(z). This is equivalent to likelihood weighting for the distribution pσ, which is an instance of simple importance sampling (SIS). Because SIS is an unbiased estimator of the likelihood, log ˆpσ(x)is a stochastic lower bound on logpσ(x)(Grosse et al., 2015). Unfortunately, SIS can result in very poor estimates when the evidence has low prior probability (i.e. the posterior is very dissimilar to the prior). This suggests that AIS might be able to yield much more accurate log-likelihood estimates under pσ. We note that KDE can be viewed as a special case of AIS where the number of intermediate distributions is set to 0. We now describe speciﬁcally how we carry out evaluation using AIS. In most of our experiments, we choose the initial distribution of AIS to be p(z), the same prior distribution used in training decoderbased models. If the model provides an encoder network (e.g., V AE), we can take the approximated distribution predicted by the encoder q(z|x)as the initial distribution of the AIS chain. For continuous data, we deﬁne the unnormalized density of target distribution to be the joint generative distribution with the Gaussian noise model, pσ(x,z) =pσ(x|z)p(z). For the small subset of experiments done on the binary data, we deﬁne the observation model to be a Bernoulli model with mean predicted by the decoder. Our intermediate distributions are geometric averages of the prior and posterior, as in Eqn. 5. Since all of our experiments are done using continuous latent space, we use Hamiltonian Monte Carlo (Neal, 2010) as the transition operator for sampling latent samples along annealing. The evaluation code is provided at https://github.com/tonywu95/eval_gen . 4 R ELATED WORK AIS is known to be a powerful technique of estimating the partition function of the model. One inﬂuential example was the use of AIS to evaluate deep belief networks (Salakhutdinov & Murray, 2008). Although we used the same technique, the problem we consider is completely different. First of all, the model they consider is undirected graphical models, whereas decoder-based models are directed graphical models. Secondly, their model has a well-deﬁned probabilistic density function in terms of energy function, whereas we need to consider different probabilistic model for one in which the the likelihood is ill-deﬁned. In addition, we validate our estimates using BDMC. Theis et al. (2016) give an in-depth analysis of issues that might come up in evaluating generative models. They also point out that a model that completely fails at modelling the proportion of modes of the distribution might still achieve a high likelihood score. Salimans et al. (2016) propose an image-quality measure which they ﬁnd to be highly correlated with human visual judgement. They propose to feed the samples xof the model to the “inception” model to obtain a conditional label distributionp(y|x), and evaluate the score deﬁned by expExKL(p(y|x)||p(y)), which is motivated by having a low entropy of p(y|x)but a large entropy of p(y). However, the measure is largely based on visual quality of the sample, and we argue that the visual quality can be a misleading way to evaluate a model. 5 E XPERIMENTS 5.1 D ATASETS All of our experiments were performed on the MNIST dataset of images of handwritten digits (LeCun et al., 1998). For consistency with prior work on evaluating decoder-based models, most of our experiments used the continuous inputs. We dequantized the data following Uria et al. (2013), by adding a uniform noise of1 256to the data and rescaling it to be in [0,1]Dafter dequantization. We use the standard split of MNIST into 60,000 training and 10,000 test examples, and used 50,000 images from the training set for training, and remaining 10,000 images for validation. In addition, 5Published as a conference paper at ICLR 2017 some of our experiments used the binarized MNIST dataset with a Bernoulli observation model (Salakhutdinov & Murray, 2008). 5.2 M ODELS For most of our experiments, we considered two decoder architectures: a small one with 10 latent dimensions, and a larger one with 50 latent dimensions. We use standard Normal distribution as prior for training all of our models. All layers were fully connected, and the number of units in each layer was 10–64–256–256-1024–784 for the smaller architecture and 50–1024–1024–1024–784 for the larger one. We trained both architectures using the V AE, GAN, and GMMN objectives, resulting in six networks which we refer to as V AE-10, V AE-50, etc. In general, the larger architecture performed substantially better on both the training and test sets, but we analyze the smaller architecture as well because it better highlights some of the differences between the training criteria. Additional architectural details are given in Appendix A.1. In order to enable a direct comparison between training criteria, all models used a spherical Gaussian observation model with ﬁxed variance. This is consistent with previous protocols for evaluating GANs and GMMNs. However, we note that this observation model is a nontrivial constraint on the V AEs, which could instead be trained with a more ﬂexible diagonal Gaussian observation model where the variances depend on the latent state. Such observation models can easily achieve much higher log-likelihood scores, for instance by noticing that boundary pixels are always close to 0. (E.g., we trained a V AE with the more general observation model which achieved a log-likelihood of at least 2200 nats on continuous MNIST.) Therefore, the log-likelihood values we report should not be compared directly against networks which have a more ﬂexible observation model. 5.3 V ALIDATION OF LOG -LIKELIHOOD ESTIMATES Before we analyze the performance of the trained networks, we must ﬁrst determine the accuracy of the log-likelihood estimators. In this section, we validate the accuracy of our AIS-based estimates using BDMC. We then analyze the error in the KDE and IWAE estimates and highlight some cases where these measures miss important phenomena. 5.3.1 V ALIDATION OF AIS We used AIS to estimate log-likelihoods for all models under consideration. Except where otherwise speciﬁed, all AIS estimates were obtained using 16 independent chains, 10,000 intermediate distributions of the form in Eqn. 5, and a transition operator consisting of one proposed HMC trajectory with 10 leapfrog steps.1Following Ranzato et al. (2010), the HMC stepsize was tuned to achieve an acceptance rate of 0.65 (as recommended by Neal (2010)). For all six models, we evaluated the accuracy of this estimation procedure using BDMC on data sampled from the model’s distribution on 1000 simulated examples. The gap between the loglikelihood estimates produced by forward AIS (which gives a lower bound) and reverse AIS (which gives an upper bound) bounds the error of the AIS estimates on simulated data. We refer to this gap as the BDMC gap . For ﬁve of the six networks under consideration, we found the BDMC gap to be less than 1 nat. For the remaining model (GAN-50), the gap was about 10 nats. Both gaps are much smaller than our measured log-likelihood differences between models. If these gaps are representative of the true error in the estimates on the real data, then this indicates AIS is accurate enough to make ﬁne-grained comparisons between models and to benchmark other log-likelihood estimators. (The BDMC gap is not guaranteed to hold for the real data, although Grosse et al. (2016) found the behavior of AIS to match closely between real and simulated data.) 5.3.2 H OW ACCURATE IS KERNEL DENSITY ESTIMATION ? Kernel density estimation (KDE) (Parzen, 1962) is widely used to evaluate decoder-based models (Goodfellow et al., 2014; Li & Swersky, 2015), and a variant was proposed in the setting of evaluating Boltzmann machines (Bengio et al., 2013). Papers reporting KDE estimates often caution that the 1We used the HMC implementation from http://deeplearning.net/tutorial/ deeplearning.pdf 6Published as a conference paper at ICLR 2017 0.005 0.010 0.015 0.020 0.025 Variance−400−2000200400600Log-likelihoodGAN50 with varying variance Train AIS Valid AIS Train KDE Valid KDE 101 102 103 Seconds50100150200250300350Log-likelihoodAIS vs: KDE KDE AIS forward AIS backward 101 102 103 104 Seconds−88.0−87.5−87.0−86.5−86.0−85.5Log-likelihoodAIS vs: IWAE IWAE AIS AIS+encoder (a) GAN-50: LLD vs. Variance (b) GMMN-10: LLD vs. Evaluation time (c) IWAE: LLD vs. Evaluation time Figure 2: (a) Log-likelihood of GAN-50, under different choices of variance parameter. (b) Log-likelihood of GMMN-10 on 100 simulated examples evaluated by AIS and KDE vs. the corresponding running time. We show the BDMC gap converges to almost zero as we increase the running time. (c) Log-likelihood of IWAE on 10,000 test examples evaluated by AIS and IWAE bound vs. running time. (a), (b) are results on continuous MNIST, and (c) is on binarized MNIST. Note that AIS/AIS+encoder dominates the other estimate in both estimation accuracy and running time. (Nats) AIS AIS+encoder IWAE bound # dist AIS # dist AIS+encoder # samples IWAE -85.679 -85.754 -86.902 1000 100 10000 -85.619 -85.621 -86.464 10000 1000 100000 Table 1: AIS vs. IWAE bound on 10,000 test examples of binarized MNIST. “# dist” denotes the number of intermediate distributions used for evalution. We ﬁnd AIS estimate is consistently 1 nat higher than IWAE bound; AIS+encoder can achieve about the same estimate as AIS, but with 1 order of magnitude less number of intermediate distributions. KDE is not meant to be applied in high-dimensional spaces and that the results might therefore be inaccurate. Nevertheless, KDE remains the standard protocol for evaluating decoder-based models. We analyzed the accuracy of the KDE estimates by comparing against AIS. Both estimates are stochastic lower bounds on the true log-likelihood (see Section 3), so larger values are guaranteed (with high probability) to be more accurate. For each estimator, we varied one parameter inﬂuencing the computational budget; for AIS, this was the number of intermediate distributions (chosen from {100,500,1000,2000,10000}), and for KDE, it was the number of samples (chosen from {10000,100000,500000,1000000,2000000}). Using GMMN-10 for illustration, we plot both log-likelihood estimates 100 simulated examples as a function of evaluation time in Fig. 2(b). We also plot the upper bound of likelihood given by running AIS in reverse direction. We see that the BDMC gap approaches to zero, validating the accuracy of AIS. We also see that the AIS estimator achieves much more accurate estimates during similar evaluation time. Furthermore, the KDE estimates appear to level off, suggesting one cannot obtain accurate results even using orders of magnitude more samples. The KDE estimation error also impacts the estimate of the observation noise σ, since a large value of σis needed for the samples to cover the full distribution. We compared the log-likelihoods estimated by AIS and KDE with varying choices of σon 100 training and validation examples of MNIST. We used 1 million simulated samples for KDE evaluation, which takes almost the same time as running AIS estimation. In Fig. 2(a), we show the log-likelihood of GAN-50 estimated by KDE and AIS as a function ofσ. Because the accuracy of KDE declines sharply for small σvalues, it creates a strong bias towards large σ. 5.3.3 H OW ACCURATE IS THE IWAE BOUND ? In principle, one could estimate V AE likelihoods using the V AE objective function (which is a lower bound on the true log-likelihood). However, it is more common to use importance weighting, where the proposal distribution is computed by the recognition network. This is provably more accurate than the V AE bound (Burda et al., 2016). Because the importance weighted estimate corresponds to the objective function used by the Importance Weighted Autoencoder (IWAE) (Burda et al., 2016), we will refer to it as the IWAE bound . On continuous MNIST, the IWAE bound underestimated the true log-likelihoods by at least 33.2 nats on the training set and 187.4 nats on the test set. While this is considerably more accurate than KDE, the error is still signiﬁcant. Interestingly, this result also suggests that the recognition network overﬁts the training data. 7Published as a conference paper at ICLR 2017 (Nats) AIS Test AIS Train BDMC gap KDE Test IWAE Test V AE-50 991.435 ±6.477 1298.830±0.863 1.540 351.213 826.325 GAN-50 627.297 ±8.813 648.283±21.115 10.045 300.331 / GMMN-50 593.472 ±8.591 607.272±1.451 1.146 277.193 / V AE-10 705.375 ±7.411 791.029±0.810 0.832 408.659 486.466 GAN-10 328.772 ±5.538 346.640±4.260 0.934 259.673 / GMMN-10 346.679 ±5.860 358.943±6.485 0.605 262.73 / Table 2: Model comparisons on 1000 test and training examples of continuous MNIST. Conﬁdence intervals reﬂect the variability from the choice of training or test examples (which appears to be the dominant source of error for the AIS values). AIS, KDE, and IWAE are all stochastic lower bounds on the log-likelihood. Since V AE and IWAE results have customarily been reported on binarized MNIST, we additionally trained an IWAE in this setting. The training details are given in Appendix A.2. To show the practicality of our method, we evaluated the IWAE on the full 10000 test using AIS and IWAE bound, with different choices of intermediate distribution and number of simulated samples, shown in Table 1. We also evaluate AIS with the initial distribution deﬁned by encoders of V AEs, denoted as AIS+encoder. We ﬁnd that the IWAE bound underestimates the true value by at least 1 nat, which is a large difference by the standards of binarized MNIST. (E.g., it represents about half of the gap between a state-of-the-art permutation-invariant model (Tran et al., 2016) and one which exploits structure (van den Oord et al., 2016).) The AIS and IWAE estimates are compared in terms of evaluation time in Fig. 2 (c). 5.4 S CIENTIFIC FINDINGS Having validated the accuracy of AIS, we now use it to analyze the effectiveness of various training criteria. We also highlight phenomena which would not be observable using existing log-likelihood estimators or by inspecting samples. For all experiments in this section, we used 10,000 intermediate distributions for AIS, 1 million simulated samples for KDE, and 200,000 importance samples for the IWAE bound. (These settings resulted in similar computation time for all three estimators.) 5.4.1 M ODEL LIKELIHOOD COMPARISON We evaluated the trained models using AIS and KDE on 1000 test examples of MNIST; results are shown in Table 2. We ﬁnd that for all three training criteria, the larger architectures consistently outperformed the smaller ones. We also ﬁnd that for both the 10and 50-dimensional architectures, the V AEs achieved substantially higher log-likelihoods than GANs or GMMNs. It is not surprising that the V AEs achieved higher likelihood, because they were trained using a likelihood-based objective while the GANs and GMMNs were not. However, it is interesting that the difference in log-likelihoods was so large; in the rest of this section, we attempt to analyze what exactly is causing this large difference. We note that the KDE errors were of the same order of magnitude as the differences between models, indicating that it cannot be used reliably to compare log-likelihoods. Furthermore, KDE did not identify the correct ordering of models; for instance, it estimated a lower log-likelihood for V AE50 than for V AE-10, even though its true log-likelihood was almost 300 nats higher. KDE also underestimated by an order of magnitude the log-likelihood improvements that resulted from using the larger architectures. (E.g., it estimated a 15 nat difference between GMMN-10 and GMMN-50, even though the true difference was 247 nats as estimated by AIS.) These differences are also hard to observe simply by looking at samples; for instance, we were unable to visually distinguish the quality of samples for GAN-10 and GAN-50 (see Fig. 1), even though their log-likelihoods differed by almost 300 nats on both the training and test sets. 5.4.2 M EASURING THE DEGREE OF OVERFITTING One question that arises in evaluation of decoder-based generative models is whether they memorize parts of the training dataset. One cannot test this by looking only at model samples. The commonly reported nearest-neighbors from the training set can be misleading (Theis et al., 2016), and interpolation in the latent space between different samples can be visually appealing, but does not provide a quantitative measure of the degree of generalization. 8Published as a conference paper at ICLR 2017 100200 400 600 800 1000 number of Epochs200250300350400450500550600650Log-likelihoodGAN50 training curves Train AIS Valid AISTrain KDE Valid KDE 100200 400 600 800 1000 number of Epochs40060080010001200Log-likelihoodVAE50 training curves Train AIS Valid AIS Train KDEValid KDE Train IWAE Valid IWAE 2000 4000 6000 800010000 number of Epochs200300400500600Log-likelihoodGMMN50 training curves Train AIS Valid AIS Train KDE Valid KDE (a) GAN-50: LLD vs. Num epochs (b) V AE-50: LLD vs. Num epochs (c) GMMN-50: LLD vs. Num epochs Figure 3: Training curves for (a) GAN-50, (b) V AE-50, and (c) GMMN-10, as measured by AIS, KDE, and (if applicable) the IWAE lower bound. All estimates shown here are lower bounds. In (c), the gap between training and validation log-likelihoods is not fairly small (see Table 2). To analyze the degree of overﬁtting, Fig. 3 shows training curves for three networks as measured by AIS, KDE, and the IWAE bound. We observe that GAN-50’s training and test log-likelihoods are nearly identical throughout training, disconﬁrming the hypothesis that it was memorizing training data. Both GAN-50 and GMMN-50 overﬁt less than V AE-50. We also observed two phenomena which could not be measured using existing techniques. First, in the case of V AE-50, the IWAE lower bound starts to decline after 200 epochs, while the AIS estimates hold steady, suggesting it is the recognition network rather than the generative network which is overﬁtting most. Second, the GMMN-50 training and validation error continue to improve at 10,000 epochs, even though KDE erroneously indicates that performance has leveled off. 5.4.3 H OW APPROPRIATE IS THE OBSERVATION MODEL ? Appendix B addresses the questions of whether the spherical Gaussian observation model is a good ﬁt and whether the log-likelihood differences could be an artifact of the observation model. We ﬁnd that all of the models can be substantially improved by accounting for non-Gaussianity, but that this effect is insufﬁcient to explain the gap between the V AEs and the other models. 5.4.4 A RE THE NETWORKS MISSING MODES ? It was previously observed that one of the potential failure modes of Boltzmann machines is to fail to generate one or more modes of a distribution or to drastically misallocate probability mass between modes (Salakhutdinov & Murray, 2008). Here we analyze this for decoder-based models. First, we ask a coarse-grained version of this question: do the networks allocate probability mass correctly between the 10 digit classes, and if not, can this explain the difference in log-likelihood scores? In Fig. 1, we see that GAN-50’s distribution of digit classes was heavily skewed: out of 100 samples, it generated 37 images of 1’s, but only a single 2. This appears to be a large effect, but it does not explain the magnitude of the log-likelihood difference from V AEs. In particular, if the allocation of digit classes were off by a factor of 10, this effect by itself could cost at most log 10≈2.3nats of log-likelihood. Since V AE-50 outperformed GAN-50 by 364 nats, this effect cannot explain the difference. However, MNIST has many factors of variability beyond simply the 10 digit classes. In order to determine whether any of the models missed more ﬁne-grained modes, we visualized posterior samples for each model conditioned on training and test images. In particular, for each image x under consideration, we used AIS to approximately sample zfrom the posterior distribution p(z|x), and then ran the decoder on z. While these samples are approximate, Grosse et al. (2016) point out that the BDMC gap also bounds the KL divergence of approximate samples from the true posterior. With the exception of GAN-50, our BDMC gaps were on the order of 1 nat, suggesting our approximate posterior samples are fairly representative. The results are shown in Fig. 4. Further posterior visualizations for digit class 2 (the most difﬁcult for the models we considered) are shown in Appendix C. Both V AEs’ posterior samples match the observations almost perfectly. (We observed a few poorly reconstructed examples on the test set, but not on the training set.) The GANs and GMMNs fail to 9Published as a conference paper at ICLR 2017 Data GAN 10 V AE 10 GMMN 10 GAN 50 V AE 50 GMMN 50 (a) The visualization of posterior of 10 training examples (b)The visualization of posterior of 10 validation examples (c)The visualization of posterior of 10 examples of digit “2" of training set Figure 4: (a) and (b) show visualization of posterior samples of 10 training/validation examples. (c) shows visualization of posterior samples of 10 training examples of digit “2". Each column of 10 digits comes from true data and the six models. The order of visualization is: True data, GAN-10, V AE-10, GMMN-10, GAN-50, V AE-50, GMMN-50. reconstruct some of the examples on both the training and validation sets, suggesting that they failed to learn some modes of the distribution. ACKNOWLEDGMENTS We like to thank Yujia Li for providing his original GMMN model and codebase, and thank Jimmy Ba for advice on training GANs. Ruslan Salakhutdinov is supported in part by Disney and ONR grant N000141310721. We also thank the developers of Lasagne (Battenberg et al., 2014) and Theano (Al-Rfou et al., 2016). 