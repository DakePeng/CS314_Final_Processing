#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning Haoran Tang1∗, Rein Houthooft34∗, Davis Foote2, Adam Stooke2, Xi Chen2†, Yan Duan2†, John Schulman4, Filip De Turck3, Pieter Abbeel2† 1UC Berkeley, Department of Mathematics 2UC Berkeley, Department of Electrical Engineering and Computer Sciences 3Ghent University – imec, Department of Information Technology 4OpenAI Abstract Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty orintrinsic motivation . In this work, we describe a surprising ﬁnding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various highdimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We ﬁnd that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domaindependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration. 1 Introduction Reinforcement learning (RL) studies an agent acting in an initially unknown environment, learning through trial and error to maximize rewards. It is impossible for the agent to act near-optimally until it has sufﬁciently explored the environment and identiﬁed all of the opportunities for high reward, in all scenarios. A core challenge in RL is how to balance exploration—actively seeking out novel states and actions that might yield high rewards and lead to long-term gains; and exploitation—maximizing short-term rewards using the agent’s current knowledge. While there are exploration techniques for ﬁnite MDPs that enjoy theoretical guarantees, there are no fully satisfying techniques for highdimensional state spaces; therefore, developing more general and robust exploration techniques is an active area of research. ∗These authors contributed equally. Correspondence to: Haoran Tang <hrtang@math.berkeley.edu>, Rein Houthooft <rein.houthooft@openai.com> †Work done at OpenAI 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1611.04717v3  [cs.AI]  5 Dec 2017Most of the recent state-of-the-art RL results have been obtained using simple exploration strategies such as uniform sampling  and i.i.d./correlated Gaussian noise [ 19,30]. Although these heuristics are sufﬁcient in tasks with well-shaped rewards, the sample complexity can grow exponentially (with state space size) in tasks with sparse rewards . Recently developed exploration strategies for deep RL have led to signiﬁcantly improved performance on environments with sparse rewards. Bootstrapped DQN  led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma’s Revenge, an extremely challenging Atari 2600 game . Variational Information Maximizing Exploration (VIME, ) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains. Some of the classic, theoretically-justiﬁed exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of  chooses the action atat timetthat maximizes ˆr(at) +/radicalBig 2 logt n(at)where ˆr(at)is the estimated reward, and n(at)is the number of times action atwas previously chosen. In the MDP setting, some of the algorithms have similar structure, for example, Model Based Interval Estimation–Exploration Bonus (MBIE-EB) of  counts state-action pairs with a table n(s,a)and adding a bonus reward of the formβ√ n(s,a)to encourage exploring less visited pairs.  show that the inverse-square-root dependence is optimal. MBIE and related algorithms assume that the augmented MDP is solved analytically at each timestep, which is only practical for small ﬁnite state spaces. This paper presents a simple approach for exploration, which extends classic counting-based methods to high-dimensional, continuous state spaces. We discretize the state space with a hash function and apply a bonus based on the state-visitation count. The hash function can be chosen to appropriately balance generalization across states, and distinguishing between states. We select problems from rllab  and Atari 2600  featuring sparse rewards, and demonstrate near state-of-the-art performance on several games known to be hard for naïve exploration strategies. The main strength of the presented approach is that it is fast, ﬂexible and complementary to most existing RL algorithms. In summary, this paper proposes a generalization of classic count-based exploration to highdimensional spaces through hashing (Section 2); demonstrates its effectiveness on challenging deep RL benchmark problems and analyzes key components of well-designed hash functions (Section 4). 2 Methodology 2.1 Notation This paper assumes a ﬁnite-horizon discounted Markov decision process (MDP), deﬁned by (S,A,P,r,ρ 0,γ,T ), in whichSis the state space, Athe action space,Pa transition probability distribution, r:S×A→ Ra reward function, ρ0an initial state distribution, γ∈(0,1]a discount factor, and Tthe horizon. The goal of RL is to maximize the total expected discounted reward Eπ,P/bracketleftBig/summationtextT t=0γtr(st,at)/bracketrightBig over a policy π, which outputs a distribution over actions given a state. 2.2 Count-Based Exploration via Static Hashing Our approach discretizes the state space with a hash function φ:S→Z. An exploration bonus r+:S→Ris added to the reward function, deﬁned as r+(s) =β/radicalbig n(φ(s)), (1) whereβ∈R≥0is the bonus coefﬁcient. Initially the counts n(·)are set to zero for the whole range ofφ. For every state stencountered at time step t,n(φ(st))is increased by one. The agent is trained with rewards (r+r+), while performance is evaluated as the sum of rewards without bonuses. 2Algorithm 1: Count-based exploration through static hashing, using SimHash 1Deﬁne state preprocessor g:S→RD 2(In case of SimHash) Initialize A∈Rk×Dwith entries drawn i.i.d. from the standard Gaussian distributionN(0,1) 3Initialize a hash table with values n(·)≡0 4foreach iteration jdo 5 Collect a set of state-action samples {(sm,am)}M m=0with policyπ 6 Compute hash codes through any LSH method, e.g., for SimHash, φ(sm) = sgn(Ag(sm)) 7 Update the hash table counts ∀m: 0≤m≤Masn(φ(sm))←n(φ(sm)) + 1 8 Update the policy πusing rewards/braceleftbigg r(sm,am) +β√ n(φ(sm))/bracerightbiggM m=0with any RL algorithm Note that our approach is a departure from count-based exploration methods such as MBIE-EB since we use a state-space count n(s)rather than a state-action count n(s,a). State-action counts n(s,a)are investigated in the Supplementary Material, but no signiﬁcant performance gains over state counting could be witnessed. A possible reason is that the policy itself is sufﬁciently random to try most actions at a novel state. Clearly the performance of this method will strongly depend on the choice of hash function φ. One important choice we can make regards the granularity of the discretization: we would like for “distant” states to be be counted separately while “similar” states are merged. If desired, we can incorporate prior knowledge into the choice of φ, if there would be a set of salient state features which are known to be relevant. A short discussion on this matter is given in the Supplementary Material. Algorithm 1 summarizes our method. The main idea is to use locality-sensitive hashing (LSH) to convert continuous, high-dimensional data to discrete hash codes. LSH is a popular class of hash functions for querying nearest neighbors based on certain similarity metrics . A computationally efﬁcient type of LSH is SimHash , which measures similarity by angular distance. SimHash retrieves a binary code of state s∈Sas φ(s) = sgn(Ag(s))∈{− 1,1}k, (2) whereg:S→RDis an optional preprocessing function and Ais ak×Dmatrix with i.i.d. entries drawn from a standard Gaussian distribution N(0,1). The value for kcontrols the granularity: higher values lead to fewer collisions and are thus more likely to distinguish states. 2.3 Count-Based Exploration via Learned Hashing When the MDP states have a complex structure, as is the case with image observations, measuring their similarity directly in pixel space fails to provide the semantic similarity measure one would desire. Previous work in computer vision [ 7,20,36] introduce manually designed feature representations of images that are suitable for semantic tasks including detection and classiﬁcation. More recent methods learn complex features directly from data by training convolutional neural networks [ 12, 17,31]. Considering these results, it may be difﬁcult for a method such as SimHash to cluster states appropriately using only raw pixels. Therefore, rather than using SimHash, we propose to use an autoencoder (AE) to learn meaningful hash codes in one of its hidden layers as a more advanced LSH method. This AE takes as input statessand contains one special dense layer comprised of Dsigmoid functions. By rounding the sigmoid activations b(s)of this layer to their closest binary number ⌊b(s)⌉∈{ 0,1}D, any states can be binarized. This is illustrated in Figure 1 for a convolutional AE. A problem with this architecture is that dissimilar inputs si,sjcan map to identical hash codes ⌊b(si)⌉=⌊b(sj)⌉, but the AE still reconstructs them perfectly. For example, if b(si)andb(sj)have values 0.6 and 0.7 at a particular dimension, the difference can be exploited by deconvolutional layers in order to reconstruct siandsjperfectly, although that dimension rounds to the same binary value. One can imagine replacing the bottleneck layer b(s)with the hash codes ⌊b(s)⌉, but then gradients cannot be back-propagated through the rounding function. A solution is proposed by Gregor et al.  and Salakhutdinov & Hinton  is to inject uniform noise U(−a,a)into the sigmoid 36×6 6×6 6×6 6×6 6×6 6×6⌊·⌉ codedownsample softmax linear 64×52×52 1×52×5296×24×2496×10×1096×5×5 2400b(·) 512 102496×5×5 96×11×11 96×24×24 1×52×52 Figure 1: The autoencoder (AE) architecture for ALE; the solid block represents the dense sigmoidal binary code layer, after which noise U(−a,a)is injected. Algorithm 2: Count-based exploration using learned hash codes 1Deﬁne state preprocessor g:S→{ 0,1}Das the binary code resulting from the autoencoder (AE) 2InitializeA∈Rk×Dwith entries drawn i.i.d. from the standard Gaussian distribution N(0,1) 3Initialize a hash table with values n(·)≡0 4foreach iteration jdo 5 Collect a set of state-action samples {(sm,am)}M m=0with policyπ 6 Add the state samples {sm}M m=0to a FIFO replay pool R 7 ifjmodjupdate = 0then 8 Update the AE loss function in Eq. (3) using samples drawn from the replay pool {sn}N n=1∼R , for example using stochastic gradient descent 9 Computeg(sm) =⌊b(sm)⌉, theD-dim rounded hash code for smlearned by the AE 10 Projectg(sm)to a lower dimension kvia SimHash as φ(sm) = sgn(Ag(sm)) 11 Update the hash table counts ∀m: 0≤m≤Masn(φ(sm))←n(φ(sm)) + 1 12 Update the policy πusing rewards/braceleftbigg r(sm,am) +β√ n(φ(sm))/bracerightbiggM m=0with any RL algorithm activations. By choosing uniform noise with a>1 4, the AE is only capable of (always) reconstructing distinct state inputs si/negationslash=sj, if it has learned to spread the sigmoid outputs sufﬁciently far apart, |b(si)−b(sj)|>/epsilon1, in order to counteract the injected noise. As such, the loss function over a set of collected states {si}N i=1is deﬁned as L/parenleftbig {sn}N n=1/parenrightbig =−1 NN/summationdisplay n=1/bracketleftBig logp(sn)−λ K/summationtextD i=1min/braceleftBig (1−bi(sn))2,bi(sn)2/bracerightBig/bracketrightBig , (3) withp(sn)the AE output. This objective function consists of a negative log-likelihood term and a term that pressures the binary code layer to take on binary values, scaled by λ∈R≥0. The reasoning behind this latter term is that it might happen that for particular states, a certain sigmoid unit is never used. Therefore, its value might ﬂuctuate around1 2, causing the corresponding bit in binary code ⌊b(s)⌉to ﬂip over the agent lifetime. Adding this second loss term ensures that an unused bit takes on an arbitrary binary value. For Atari 2600 image inputs, since the pixel intensities are discrete values in the range [0,255], we make use of a pixel-wise softmax output layer  that shares weights between all pixels. The architectural details are described in the Supplementary Material and are depicted in Figure 1. Because the code dimension often needs to be large in order to correctly reconstruct the input, we apply a downsampling procedure to the resulting binary code ⌊b(s)⌉, which can be done through random projection to a lower-dimensional space via SimHash as in Eq. (2). On the one hand, it is important that the mapping from state to code needs to remain relatively consistent over time, which is nontrivial as the AE is constantly updated according to the latest data (Algorithm 2 line 8). A solution is to downsample the binary code to a very low dimension, or by slowing down the training process. On the other hand, the code has to remain relatively unique 4for states that are both distinct and close together on the image manifold. This is tackled both by the second term in Eq. (3)and by the saturating behavior of the sigmoid units. States already well represented by the AE tend to saturate the sigmoid activations, causing the resulting loss gradients to be close to zero, making the code less prone to change. 3 Related Work Classic count-based methods such as MBIE , MBIE-EB and  solve an approximate Bellman equation as an inner loop before the agent takes an action . As such, bonus rewards are propagated immediately throughout the state-action space. In contrast, contemporary deep RL algorithms propagate the bonus signal based on rollouts collected from interacting with environments, with value-based  or policy gradient-based [ 22,30] methods, at limited speed. In addition, our proposed method is intended to work with contemporary deep RL algorithms, it differs from classical count-based method in that our method relies on visiting unseen states ﬁrst, before the bonus reward can be assigned, making uninformed exploration strategies still a necessity at the beginning. Filling the gaps between our method and classic theories is an important direction of future research. A related line of classical exploration methods is based on the idea of optimism in the face of uncertainty  but not restricted to using counting to implement “optimism”, e.g., R-Max , UCRL , and E3. These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods [ 9,11,16,35], which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by  and . Another type of exploration is curiosity-based exploration. These methods try to capture the agent’s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to  and  for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently.  propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus.  propose to use the prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through bootstrapping is proposed by , using bootstrapped Q-functions. The most related exploration strategy is proposed by , in which an exploration bonus is added inversely proportional to the square root of a pseudo-count quantity. A state pseudo-count is derived from its log-probability improvement according to a density model over the state space, which in the limit converges to the empirical count. Our method is similar to pseudo-count approach in the sense that both methods are performing approximate counting to have the necessary generalization over unseen states. The difference is that a density model has to be designed and learned to achieve good generalization for pseudo-count whereas in our case generalization is obtained by a wide range of simple hash functions (not necessarily SimHash). Another interesting connection is that our method also implies a density model ρ(s) =n(φ(s)) Nover all visited states, where Nis the total number of states visited. Another method similar to hashing is proposed by , which clusters states and counts cluster centers instead of the true states, but this method has yet to be tested on standard exploration benchmark problems. 4 Experiments Experiments were designed to investigate and answer the following research questions: 1.Can count-based exploration through hashing improve performance signiﬁcantly across different domains? How does the proposed method compare to the current state of the art in exploration for deep RL? 2.What is the impact of learned or static state preprocessing on the overall performance when image observations are used? 5To answer question 1, we run the proposed method on deep RL benchmarks (rllab and ALE) that feature sparse rewards, and compare it to other state-of-the-art algorithms. Question 2 is answered by trying out different image preprocessors on Atari 2600 games. Trust Region Policy Optimization (TRPO, ) is chosen as the RL algorithm for all experiments, because it can handle both discrete and continuous action spaces, can conveniently ensure stable improvement in the policy performance, and is relatively insensitive to hyperparameter changes. The hyperparameters settings are reported in the Supplementary Material. 4.1 Continuous Control The rllab benchmark  consists of various control tasks to test deep RL algorithms. We selected several variants of the basic and locomotion tasks that use sparse rewards, as shown in Figure 2, and adopt the experimental setup as deﬁned in —a description can be found in the Supplementary Material. These tasks are all highly difﬁcult to solve with naïve exploration strategies, such as adding Gaussian noise to the actions. Figure 2: Illustrations of the rllab tasks used in the continuous control experiments, namely MountainCar, CartPoleSwingup, SimmerGather, and HalfCheetah; taken from . (a) MountainCar  (b) CartPoleSwingup  (c) SwimmerGather  (d) HalfCheetah Figure 3: Mean average return of different algorithms on rllab tasks with sparse rewards. The solid line represents the mean average return, while the shaded area represents one standard deviation, over 5seeds for the baseline and SimHash (the baseline curves happen to overlap with the axis). Figure 3 shows the results of TRPO (baseline), TRPO-SimHash, and VIME  on the classic tasks MountainCar and CartPoleSwingup, the locomotion task HalfCheetah, and the hierarchical task SwimmerGather. Using count-based exploration with hashing is capable of reaching the goal in all environments (which corresponds to a nonzero return), while baseline TRPO with Gaussia n control noise fails completely. Although TRPO-SimHash picks up the sparse reward on HalfCheetah, it does not perform as well as VIME. In contrast, the performance of SimHash is comparable with VIME on MountainCar, while it outperforms VIME on SwimmerGather. 4.2 Arcade Learning Environment The Arcade Learning Environment (ALE, ), which consists of Atari 2600 video games, is an important benchmark for deep RL due to its high-dimensional state space and wide variety of games. In order to demonstrate the effectiveness of the proposed exploration strategy, six games are selected featuring long horizons while requiring signiﬁcant exploration: Freeway, Frostbite, Gravitar, Montezuma’s Revenge, Solaris, and Venture. The agent is trained for 500iterations in all experiments, with each iteration consisting of 0.1 Msteps (the TRPO batch size, corresponds to 0.4 Mframes). Policies and value functions are neural networks with identical architectures to . Although the policy and baseline take into account the previous four frames, the counting algorithm only looks at the latest frame. 6Table 1: Atari 2600: average total reward after training for 50 M time steps. Boldface numbers indicate best results. Italic numbers are the best among our methods. Freeway Frostbite Gravitar Montezuma Solaris Venture TRPO (baseline) 16.5 2869 486 0 2758 121 TRPO-pixel-SimHash 31.6 4683 468 0 2897 263 TRPO-BASS-SimHash 28.4 3150 604 238 1201 616 TRPO-AE-SimHash 33.5 5214 482 75 4467 445 Double-DQN 33.3 1683 412 0 3068 98 .0 Dueling network 0.0 4672 588 0 2251 497 Gorila 11.7 605 1054 4 N/A 1245 DQN Pop-Art 33.4 3469 483 0 4544 1172 A3C+ 27.3 507 246 142 2175 0 pseudo-count 29.2 1450 – 3439 – 369 BASS To compare with the autoencoder-based learned hash code, we propose using Basic Abstraction of the ScreenShots (BASS, also called Basic; see ) as a static preprocessing function g. BASS is a hand-designed feature transformation for images in Atari 2600 games. BASS builds on the following observations speciﬁc to Atari: 1) the game screen has a low resolution, 2) most objects are large and monochrome, and 3) winning depends mostly on knowing object locations and motions. We designed an adapted version of BASS3, that divides the RGB screen into square cells, computes the average intensity of each color channel inside a cell, and assigns the resulting values to bins that uniformly partition the intensity range [0,255]. Mathematically, let Cbe the cell size (width and height),Bthe number of bins, (i,j)cell location, (x,y)pixel location, and zthe channel, then feature(i,j,z ) =/floorleftBig B 255C2/summationtext (x,y)∈cell(i,j)I(x,y,z )/floorrightBig . (4) Afterwards, the resulting integer-valued feature tensor is converted to an integer hash code ( φ(st)in Line 6 of Algorithm 1). A BASS feature can be regarded as a miniature that efﬁciently encodes object locations, but remains invariant to negligible object motions. It is easy to implement and introduces little computation overhead. However, it is designed for generic Atari game images and may not capture the structure of each speciﬁc game very well. We compare our results to double DQN , dueling network , A3C+ , double DQN with pseudo-counts , Gorila , and DQN Pop-Art  on the “null op” metric4. We show training curves in Figure 4 and summarize all results in Table 1. Surprisingly, TRPO-pixel-SimHash already outperforms the baseline by a large margin and beats the previous best result on Frostbite. TRPOBASS-SimHash achieves signiﬁcant improvement over TRPO-pixel-SimHash on Montezuma’s Revenge and Venture, where it captures object locations better than other methods.5TRPO-AESimHash achieves near state-of-the-art performance on Freeway, Frostbite and Solaris. As observed in Table 1, preprocessing images with BASS or using a learned hash code through the AE leads to much better performance on Gravitar, Montezuma’s Revenge and Venture. Therefore, a static or adaptive preprocessing step can be important for a good hash function. In conclusion, our count-based exploration method is able to achieve remarkable performance gains even with simple hash functions like SimHash on the raw pixel space. If coupled with domaindependent state preprocessing techniques, it can sometimes achieve far better results. A reason why our proposed method does not achieve state-of-the-art performance on all games is that TRPO does not reuse off-policy experience, in contrast to DQN-based algorithms [ 4,23,38]), and is 3The original BASS exploits the fact that at most 128colors can appear on the screen. Our adapted version does not make this assumption. 4The agent takes no action for a random number (within 30) of frames at the beginning of each episode. 5We provide videos of example game play and visualizations of the difference bewteen Pixel-SimHash and BASS-SimHash at https://www.youtube.com/playlist?list=PLAd-UMX6FkBQdLNWtY8nH1-pzYJA_1T55 70 100 200 300 400 500−505101520253035(a) Freeway 0 100 200 300 400 5000200040006000800010000 (b) Frostbite 0 100 200 300 400 5001002003004005006007008009001000 TRPO-AE-SimHash TRPO TRPO-BASS-SimHash TRPO-pixel-SimHash (c) Gravitar 0 100 200 300 400 5000100200300400500 (d) Montezuma’s Revenge 0 100 200 300 400 500−100001000200030004000500060007000 (e) Solaris 0 100 200 300 400 500−200020040060080010001200 (f) Venture Figure 4: Atari 2600 games: the solid line is the mean average undiscounted return per iteration, while the shaded areas represent the one standard deviation, over 5seeds for the baseline, TRPOpixel-SimHash, and TRPO-BASS-SimHash, while over 3seeds for TRPO-AE-SimHash. hence less efﬁcient in harnessing extremely sparse rewards. This explanation is corroborated by the experiments done in , in which A3C+ (an on-policy algorithm) scores much lower than DQN (an off-policy algorithm), while using the exact same exploration bonus. 5 Conclusions This paper demonstrates that a generalization of classical counting techniques through hashing is able to provide an appropriate signal for exploration, even in continuous and/or high-dimensional MDPs using function approximators, resulting in near state-of-the-art performance across benchmarks. It provides a simple yet powerful baseline for solving MDPs that require informed exploration. Acknowledgments We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Adam Stooke gratefully acknowledges funding from a Fannie and John Hertz Foundation fellowship. Rein Houthooft was supported by a Ph.D. Fellowship of the Research Foundation Flanders (FWO). 