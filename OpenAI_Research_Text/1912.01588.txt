Leveraging Procedural Generation to Benchmark Reinforcement Learning Karl Cobbe1Christopher Hesse1Jacob Hilton1John Schulman1 Abstract We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efﬁciency and generalization in reinforcement learning. We believe that the community will beneﬁt from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, ﬁnding that larger models signiﬁcantly improve both sample efﬁciency and generalization. 1. Introduction Generalization remains one of the most fundamental challenges in deep reinforcement learning. In several recent studies (Zhang et al., 2018c; Cobbe et al., 2019; Justesen et al., 2018; Juliani et al., 2019), agents exhibit the capacity to overﬁt to remarkably large training sets. This evidence raises the possibility that overﬁtting pervades classic benchmarks like the Arcade Learning Environment (ALE) (Bellemare et al., 2013), which has long served as a gold standard in RL. While the diversity between games in the ALE is one of the benchmark’s greatest strengths, the low emphasis on generalization presents a signiﬁcant drawback. Previous work has sought to alleviate overﬁtting in the ALE by introducing sticky actions (Machado et al., 2018) or by embedding natural videos as backgrounds (Zhang et al., 2018b), but these methods only superﬁcially address the underlying problem — that agents perpetually encounter near-identical states. For each game the question must be asked: are agents robustly learning a relevant skill, 1OpenAI, San Francisco, CA, USA. Correspondence to: Karl Cobbe<karl@openai.com >. Proceedings of the 37thInternational Conference on Machine Learning , Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).or are they approximately memorizing speciﬁc trajectories? There have been several investigations of generalization in RL (Farebrother et al., 2018; Packer et al., 2018; Zhang et al., 2018a; Lee et al., 2019), but progress has largely proved elusive. Arguably one of the principal setbacks has been the lack of environments well-suited to measure generalization. While previously mentioned studies (Zhang et al., 2018c; Cobbe et al., 2019; Justesen et al., 2018; Juliani et al., 2019) reveal intriguing trends, it is hard to draw general conclusions from so few environments. We seek the best of both worlds: a benchmark with overall diversity comparable to the ALE, comprised of environments that fundamentally require generalization. We have created Procgen Benchmark to fulﬁll this need. This benchmark is ideal for evaluating generalization, as distinct training and test sets can be generated for each environment. This benchmark is also well-suited to evaluate sample efﬁciency, as all environments pose diverse and compelling challenges for RL agents. The environments’ intrinsic diversity demands that agents learn robust policies; overﬁtting to narrow regions in state space will not sufﬁce. Put differently, the ability to generalize becomes an integral component of success when agents are faced with everchanging levels. All environments are open-source and can be found at https://github.com/openai/procgen. 2. Procgen Benchmark Procgen Benchmark consists of 16 unique environments designed to measure both sample efﬁciency and generalization in reinforcement learning. These environments greatly beneﬁt from the use of procedural content generation, the algorithmic creation of a near-inﬁnite supply of highly randomized content. In these environments, employing procedural generation is far more effective than relying on ﬁxed, human-designed content. Procedural generation logic governs the level layout (Johnson et al., 2010), the selection of game assets, the location and spawn times of entities, and other game-speciﬁc details. To master any one of these environments, agents must learn a policy that is robust across all axes of variation. Learning such a policy is both more challenging and more relevant than overﬁtting to a handful of ﬁxed levels.arXiv:1912.01588v2  [cs.LG]  26 Jul 2020Leveraging Procedural Generation to Benchmark Reinforcement Learning Figure 1. Screenshots from each game in Procgen Benchmark. Screenshots from each environment are shown in Figure 1. We note that the state transition function is deterministic in all environments.1 2.1. Environment Desiderata We designed all environments to satisfy the following criteria. High Diversity: Procedural generation logic is given maximal freedom, subject to basic design constraints. The diversity in the resulting level distributions presents agents with meaningful generalization challenges. Fast Evaluation: Environment difﬁculty is calibrated such that baseline agents make signiﬁcant progress training over 200M timesteps. Moreover, the environments are optimized to perform thousands of steps per second on a single CPU core, including the time required to render observations. This enables a fast experimental pipeline. Tunable Difﬁculty: All environments support two wellcalibrated difﬁculty settings: easy and hard. This difﬁculty refers to the level distribution and not to individual levels; in both settings, the difﬁculty of individual levels has high variance. Unless otherwise speciﬁed, we report results using the hard difﬁculty setting. We make the easy 1Although the Chaser environment is deterministic, the enemy AI will make pseudorandom decisions conditioned on the level seed.difﬁculty setting available for those with limited access to compute power, as it reduces the resources required to train agents by roughly a factor of 8. Level Solvability: The procedural generation in each environment strives to make all levels solvable, but this is not strictly guaranteed. For each environment, greater than 99% of levels are believed to be solvable. Emphasis on Visual Recognition and Motor Control: In keeping with precedent, environments mimic the style of many Atari and Gym Retro (Pfau et al., 2018) games. Performing well primarily depends on identifying critical assets in the observation space and enacting appropriate low level motor responses. Shared Action and Observation Space: To support a uniﬁed training pipeline, all environments use a discrete 15 dimensional action space and produce 64643RGB observations. Some environments include no-op actions to accommodate the shared action space. Tunable Dependence on Exploration: These environments were designed to be tractable for baseline RL agents without the need for custom exploratory rewards. However, many of these environments can be made into more challenging exploration tasks if desired. See Appendix B.1 for a discussion on evaluating exploration capability. Tunable Dependence on Memory: These environments were designed to require minimal use of memory, in order to isolate the challenges in RL. However, several environments include variants that do test the use of memory, as we discuss in Appendix B.2. By satisfying these requirements, we believe Procgen Benchmark will be a valuable tool in RL research. Descriptions of each speciﬁc environment can be found in Appendix A. 2.2. Experimental Protocols By default, we train agents using Proximal Policy Optimization (Schulman et al., 2017) for 200M timesteps. While this timestep choice is arbitrary, it follows the precedent set by the ALE. It is also experimentally convenient: training for 200M timesteps with PPO on a single Procgen environment requires approximately 24 GPU-hrs and 60 CPU-hrs. We consider this a reasonable and practical computational cost. To further reduce training time at the cost of experimental complexity, environments can be set toLeveraging Procedural Generation to Benchmark Reinforcement Learning 68CoinRun 1020StarPilot 2.55.07.510.0CaveFlyer 510Dodgeball 01020FruitBot 468Chaser 101520Miner 2468Jumper 2.55.07.510.0Leaper 2.55.07.510.0Maze 10210310410501020BigFish 1021031041052.55.07.510.0Heist 1021031041055.07.510.0Climber 1021031041051020Plunder 102103104105468Ninja 10210310410591011BossFight Number of LevelsScore Train Test Figure 2. Generalization performance in each environment as a function of training set size. We report the mean raw episodic return, where each episode includes a single level. The mean and standard deviation is shown across 4 seeds. the easy difﬁculty. We recommend training easy difﬁculty environments for 25M timesteps, which requires approximately 3 GPU-hrs with our implementation of PPO. When evaluating sample efﬁciency, we train and test agents on the full distribution of levels in each environment. When evaluating generalization, we train on a ﬁnite set of levels and we test on the full distribution of levels. Unless otherwise speciﬁed, we use a training set of 500 levels to evaluate generalization in each environment. For easy difﬁculty environments, we recommend using training sets of 200 levels. We report results on easy difﬁculty environments in Appendix I. When it is necessary to report a single score across Procgen Benchmark, we calculate the mean normalized return. For each environment, we deﬁne the normalized return to beRnorm = (R Rmin)=(Rmax Rmin), where Ris the raw expected return and RminandRmax are constants chosen to approximately bound R. Under this deﬁnition, the normalized return will almost always fall between 0 and 1. We use the mean normalized return as it provides a better signal than the median, and since there is no need to be robust to outliers. We designed all environments to have similar difﬁculties in order to prevent a small subset from dominating this signal. See Appendix C for a list of normalization constants and a discussion on their selection.2.3. Hyperparameter Selection In deep RL, hyperparameter tuning is often the difference between great and mediocre results. Unfortunately, this process can be costly in both time and computation. For those who are more comfortable with the existing ALE benchmark, minimal hyperparameter tuning should be required to train on Procgen environments. This is partially by design, as Procgen Benchmark heavily draws inspiration from the ALE and Gym Retro. To provide a point of comparison, we evaluate our Procgen-tuned implementation of PPO on the ALE, and we achieve competitive performance. Detailed results are shown in Appendix F. As a convenience, we choose not to use any frame stacking in Procgen experiments, as we ﬁnd this only minimally impacts performance. See Appendix H for further discussion. By default, we train agents with the convolutional architecture found in IMPALA (Espeholt et al., 2018), as we ﬁnd this architecture strikes a reasonable balance between performance and compute requirements. We note that smaller architectures often struggle to train when faced with the high diversity of Procgen environments, a trend we explore further in Section 4.Leveraging Procedural Generation to Benchmark Reinforcement Learning 03006009001200 CoinRun 0150300450 StarPilot 0150300450600 CaveFlyer 060120180240 Dodgeball 250 0250500750 FruitBot 04080120160 Chaser 080160240 Miner 080160240 Jumper 0255075 Leaper 060120180 Maze 0 2000150300450 BigFish 0 20008162432 Heist 0 200060120180240 Climber 0 200060120180240 Plunder 0 2000255075100 Ninja 0 2000255075100 BossFight Timesteps (M)Score  Train  Test Figure 3. Train and test performance when training with a deterministic sequence of levels. We report the mean raw episodic return, where each episode may include many sequential levels. The mean and standard deviation is shown across 4 seeds. 3. Generalization Experiments 3.1. Level Requirements We ﬁrst evaluate the impact of training set size on generalization. For each environment, we construct several training sets ranging in size from 100 to 100,000 levels. We train agents for 200M timesteps on each training set using PPO, and we measure performance on held out levels. Results are shown in Figure 2. See Appendix D for a list of hyperparameters and Appendix E for test curves from each training set. We ﬁnd that agents strongly overﬁt to small training sets in almost all cases. To close the generalization gap, agents need access to as many as 10,000 levels. A peculiar trend emerges in many environments: past a certain threshold, training performance improves as the training set grows. This runs counter to trends found in supervised learning, where training performance commonly decreases with the size of the training set. We attribute this trend to the implicit curriculum provided by the distribution of levels. A larger training set can improve training performance if the agent learns to generalize even across levels in the training set. This effect was previously reported by (Cobbe et al., 2019), and we now corroborate those results with a larger number of environments.3.2. An Ablation with Deterministic Levels To fully emphasize the signiﬁcance of procedural generation, we conduct a simple ablation study. Instead of resampling a new level at the start of every episode, we train agents on a ﬁxed sequence of levels. In each episode, the agent begins on the ﬁrst level. When the agent successfully completes a level, it progresses to the next level. If the agent fails at any point, the episode terminates. With this setup, the agent can reach arbitrarily many levels, though in practice it rarely progresses beyond the 20thlevel in any environment. This approximately mimics the training setup of the ALE. To make training more tractable in this setting, we use the easy environment difﬁculty. At test time, we simply remove the determinism in the level sequence, instead choosing level sequences at random. Results are shown in Figure 3. We ﬁnd that agents become competent over the ﬁrst several training levels in most environments, giving an illusion of meaningful progress. However, test performance demonstrates that the agents have in fact learned almost nothing about the underlying level distribution. We believe this vast gap between train and test performance is worth highlighting. It reveals a crucial hidden ﬂaw in training on environments that follow a ﬁxed sequence of levels. These results emphasize the importance of both training and evaluating with diverse environment distributions.Leveraging Procedural Generation to Benchmark Reinforcement Learning 45678 CoinRun 3691215 StarPilot 1234 CaveFlyer 2468 Dodgeball 051015 FruitBot 2468 Chaser 481216 Miner 246 Jumper 2468 Leaper 46 Maze 036912 BigFish 12345 Heist 0 2002468 Climber 0 20034567 Plunder 0 2002468 Ninja 0 2000369 BossFight Timesteps (M)Score  Train  Test 0 25 50 75 100 125 150 175 200 Timesteps (M)0.00.10.20.30.40.5Mean Normalized Score  Train  Test Figure 4. Generalization performance from 500 levels in each environment. The mean and standard deviation is shown across 3 seeds. 3.3. 500 Level Generalization Due to the high computational cost, it is impractical to regularly run the experiments described in Section 3.1. To benchmark generalization, we recommend training on 500 levels from each environment and testing on held out levels, as in (Cobbe et al., 2019). We choose this training set size to be near the region where generalization begins to take effect, as seen in Figure 2. At test time, we measure agents’ zero-shot performance averaged over unseen levels. When evaluating generalization, we do not explicitly restrict the duration of training, though in practice we still train for 200M timesteps. Baseline results are shown in Figure 4. We see a high amount of overﬁtting in most environments. In some environments, the generalization gap is relatively small only because both training and test performance are poor, as discussed in Section 3.1. In any case, we expect to see signiﬁcant improvement on test performance as we develop agents more capable of generalization. 4. Scaling Model Size We now investigate how scaling model size impacts both sample efﬁciency and generalization in RL. We conduct these experiments to demonstrate the usefulness of Procgen Benchmark metrics, and because this is a compelling topic in its own right. We follow the experimental protocols described in Section 2.2, evaluating the performance of 4 different models on both sample efﬁciency and generalization.The ﬁrst 3 models use the convolutional architecture found in IMPALA (Espeholt et al., 2018) with the number of convolutional channels at each layer scaled by 1,2or4. Note that scaling the number of channels by kresults in scaling the total parameter count by approximately k2. The ﬁnal model uses the smaller and more basic convolutional architecture found in (Mnih et al., 2015), which we call NatureCNN. We include this architecture as it is often used to train agents in the ALE. We train the Nature-CNN model with the same learning rate as the smallest IMPALA model. When we scale the number of IMPALA channels by k, we also scale the learning rate by1p kto match the scaling of the weights, initialized with the method from (Glorot and Bengio, 2010). The learning rate is the only hyperparameter we vary between architectures. We performed sweeps over other hyperparameters, including the batch size and the number of epochs per rollout, and we found no other obvious gains. Results are shown in Figure 5. We ﬁnd that larger architectures signiﬁcantly improve both sample efﬁciency and generalization. It is notable that the small Nature-CNN model almost completely fails to train. These results align with the results from (Cobbe et al., 2019), and we now establish that this trend holds across many diverse environments. Although larger models offer fairly consistent improvements, we note that some environments beneﬁt from the larger models to a greater extent. See Appendix G for detailed training curves from each environment.Leveraging Procedural Generation to Benchmark Reinforcement Learning 0 25 50 75 100 125 150 175 200 Timesteps (M)0.00.20.40.60.81.0Mean Normalized ScoreIMPALA-CNN × 1 IMPALA-CNN × 2 IMPALA-CNN × 4 Nature-CNN 0 25 50 75 100 125 150 175 200 Timesteps (M)0.00.20.40.60.81.0Mean Normalized ScoreTrain TestIMPALA-CNN × 1 IMPALA-CNN × 2 IMPALA-CNN × 4 Nature-CNN Figure 5. Performance of different model sizes, measuring both sample efﬁciency (left) and generalization (right). The mean and standard deviation is shown across 3 seeds. 5. Comparing Algorithms We next compare our implementation of PPO to our implementation of Rainbow (Hessel et al., 2018) on Procgen Benchmark. We evaluate sample efﬁciency, training and testing on the full distribution of levels in each environment. As with PPO, we train Rainbow agents using the IMPALA convolutional architecture, collecting experience from 64 parallel environment copies into a single replay buffer. We ﬁrst experimented with the default Rainbow hyperparameters (with an appropriate choice for distributional min/max values), but we found that agents struggled to learn any non-trivial behaviour. We hypothesize that the diversity of our environments can lead to high variance gradients that promote instability. We therefore reduced gradient variance by running the algorithm on 8 parallel workers, using shared model parameters and averaging gradients between workers. This greatly improved performance. To improve wall-clock time for Rainbow, we also increased the batch size and decreased the update frequency each by a factor of 16, while increasing the learning rate by a factor of 4. While this change signiﬁcantly reduced wall-clock training time, it did not adversely impact performance. We conﬁrmed that the new learning rate was roughly optimal by sweeping over nearby learning rates. See Appendix D for a full list of Rainbow hyperparameters. Results are shown in Figure 6. PPO performs much more consistently across the benchmark, though Rainbow offers a signiﬁcant improvement in several environments. We’re not presently able to diagnose the instability that leads toRainbow’s low performance in some environments, though we consider this an interesting avenue for further research. 6. Related Work Many recent RL benchmarks grapple with generalization in different ways. The Sonic benchmark (Nichol et al., 2018) was designed to measure generalization in RL by separating levels of the Sonic the HedgehogTMvideo game into training and test sets. However, RL agents struggled to generalize from the few available training levels, and progress was hard to measure. The CoinRun environment (Cobbe et al., 2019) addressed this concern by procedurally generating large training and test sets to better measure generalization. CoinRun serves as the inaugural environment in Procgen Benchmark. The General Video Game AI (GVG-AI) framework (PerezLiebana et al., 2018) has also encouraged the use of procedural generation in deep RL. Using 4 procedurally generated environments based on classic video games, (Justesen et al., 2018) measured generalization across different level distributions, ﬁnding that agents strongly overﬁt to their particular training set. Environments in Procgen Benchmark are designed in a similar spirit, with two of the environments (Miner and Leaper) drawing direct inspiration from this work. The Obstacle Tower environment (Juliani et al., 2019) attempts to measure generalization in vision, control, and planning using a 3D, 3rd person, procedurally generated environment. Success requires agents to solve both low-Leveraging Procedural Generation to Benchmark Reinforcement Learning 2468 CoinRun 08162432 StarPilot 369 CaveFlyer 036912 Dodgeball 06121824 FruitBot 2468 Chaser 0481216 Miner 02468 Jumper 0369 Leaper 369 Maze 0 100 20008162432 BigFish 0 100 2000369 Heist 0 100 2000369 Climber 0 100 20005101520 Plunder 0 100 2000369 Ninja 0 100 200036912 BossFight Timesteps (M)Score PPO Rainbow Figure 6. A comparison between Rainbow and PPO. In both cases, we train and test on the full distribution of levels from each environment. The mean and standard deviation is shown across 3 seeds. level control and high-level planning problems. While studying generalization in a single complex environment offers certain advantages, we opted to design many heterogeneous environments for Procgen Benchmark. bsuite (Osband et al., 2019) is a set of simple environments designed to serve as “an MNIST for reinforcement learning.” Each environment targets a small number of core RL capabilities, including the core capability of generalization. bsuite includes a single environment that requires visual generalization in the form of an MNIST contextual bandit, whereas visual generalization is a primary source of difﬁculty across all Procgen environments. Safety Gym (Achiam et al., 2019) provides a suite of benchmark environments designed for studying safe exploration and constrained RL. While generalization is not an explicit focus of this benchmark, all Safety Gym environments perform extensive randomization to prevent agents from overﬁtting to speciﬁc environment layouts. In doing so, these environments enforce a need for generalization. The Animal-AI Environment (Beyret et al., 2019) uses tasks inspired by the animal cognition literature to evaluate agent intelligence. Since these tests are not encountered during training, high performance depends on generalizing well from the speciﬁc training conﬁgurations. The use of a single uniﬁed environment makes the prospect of generalization signiﬁcantly more plausible. Meta-World (Yu et al., 2019) proposes several metalearning benchmarks, using up to 50 unique continuous control environments for training and testing. As with theAnimal-AI Environment, the shared physics and mechanics between train and test environments gives rise to the plausible expectation of generalization, even when the details of the test task are novel. 7. Conclusion Training agents capable of generalizing across environments remains one of the greatest challenges in reinforcement learning. We’ve designed Procgen Benchmark to help the community to contend with this challenge. The intrinsic diversity within level distributions makes this benchmark ideal for evaluating both generalization and sample efﬁciency in RL. We expect many insights gleaned from this benchmark to apply in more complex settings, and we look forward to leveraging these environments to design more capable and efﬁcient algorithms. 