Published as a conference paper at ICLR 2018 CONTINUOUS ADAPTATION VIA META-LEARNING IN NONSTATIONARY AND COMPETITIVE ENVIRONMENTS Maruan Al-Shedivat CMUTrapit Bansal UMass AmherstYura Burda OpenAIIlya Sutskever OpenAI Igor Mordatch OpenAIPieter Abbeel UC Berkeley ABSTRACT The ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo , and deﬁne iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables signiﬁcantly more efﬁcient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the ﬁttest. 1 I NTRODUCTION Recent progress in reinforcement learning (RL) has achieved very impressive results ranging from playing games (Mnih et al., 2015; Silver et al., 2016), to applications in dialogue systems (Li et al., 2016), to robotics (Levine et al., 2016). Despite the progress, the learning algorithms for solving many of these tasks are designed to deal with stationary environments. On the other hand, real-world is often nonstationary either due to complexity (Sutton et al., 2007), changes in the dynamics or the objectives in the environment over the life-time of a system (Thrun, 1998), or presence of multiple learning actors (Lowe et al., 2017; Foerster et al., 2017a). Nonstationarity breaks the standard assumptions and requires agents to continuously adapt, both at training and execution time, in order to succeed. Learning under nonstationary conditions is challenging. The classical approaches to dealing with nonstationarity are usually based on context detection (Da Silva et al., 2006) and tracking (Sutton et al., 2007), i.e., reacting to the already happened changes in the environment by continuously ﬁne-tuning the policy. Unfortunately, modern deep RL algorithms, while able to achieve super-human performance on certain tasks, are known to be sample inefﬁcient. Nevertheless, nonstationarity allows only for limited interaction before the properties of the environment change. Thus, it immediately puts learning into the few-shot regime and often renders simple ﬁne-tuning methods impractical. A nonstationary environment can be seen as a sequence of stationary tasks, and hence we propose to tackle it as a multi-task learning problem (Caruana, 1998). The learning-to-learn (or meta-learning) approaches (Schmidhuber, 1987; Thrun & Pratt, 1998) are particularly appealing in the few-shot regime, as they produce ﬂexible learning rules that can generalize from only a handful of examples. Meta-learning has shown promising results in the supervised domain and have gained a lot of attention from the research community recently (e.g., Santoro et al., 2016; Ravi & Larochelle, 2016). In this paper, we develop a gradient-based meta-learning algorithm similar to (Finn et al., 2017b) and suitable for continuous adaptation of RL agents in nonstationary environments. More concretely, our agents meta-learn to anticipate the changes in the environment and update their policies accordingly. While virtually any changes in an environment could induce nonstationarity (e.g., changes in the physics or characteristics of the agent), environments with multiple agents are particularly challenging Correspondence: maruan.alshedivat.com . Work done while MA and TB interned at OpenAI. 1arXiv:1710.03641v2  [cs.LG]  23 Feb 2018Published as a conference paper at ICLR 2018 due to complexity of the emergent behavior and are of practical interest with applications ranging from multiplayer games (Peng et al., 2017) to coordinating self-driving ﬂeets Cao et al. (2013). Multi-agent environments are nonstationary from the perspective of any individual agent since all actors are learning and changing concurrently (Lowe et al., 2017). In this paper, we consider the problem of continuous adaptation to a learning opponent in a competitive multi-agent setting. To this end, we design RoboSumo —a 3D environment with simulated physics that allows pairs of agents to compete against each other. To test continuous adaptation, we introduce iterated adaptation games —a new setting where a trained agent competes against the same opponent for multiple rounds of a repeated game, while both are allowed to update their policies and change their behaviors between the rounds. In such iterated games, from the agent’s perspective, the environment changes from round to round, and the agent ought to adapt in order to win the game. Additionally, the competitive component of the environment makes it not only nonstationary but also adversarial, which provides a natural training curriculum and encourages learning robust strategies (Bansal et al., 2018). We evaluate our meta-learning agents along with a number of baselines on a (single-agent) locomotion task with handcrafted nonstationarity and on iterated adaptation games in RoboSumo . Our results demonstrate that meta-learned strategies clearly dominate other adaptation methods in the few-shot regime in both singleand multi-agent settings. Finally, we carry out a large-scale experiment where we train a diverse population of agents with different morphologies, policy architectures, and adaptation methods, and make them interact by competing against each other in iterated games. We evaluate the agents based on their TrueSkills (Herbrich et al., 2007) in these games, as well as evolve the population as whole for a few generations—the agents that lose disappear, while the winners get duplicated. Our results suggest that the agents with meta-learned adaptation strategies end up being the ﬁttest. Videos that demonstrate adaptation behaviors are available at https://goo.gl/tboqaN . 2 R ELATED WORK The problem of continuous adaptation considered in this work is a variant of continual learning (Ring, 1994; 1997) and is related to lifelong (Thrun & Pratt, 1998; Silver et al., 2013) and neverending (Mitchell et al., 2015) learning. Life-long learning systems aim at solving multiple tasks sequentially by efﬁciently transferring and utilizing knowledge from already learned tasks to new tasks while minimizing the effect of catastrophic forgetting (McCloskey & Cohen, 1989). Neverending learning is concerned with mastering a ﬁxed set of tasks in iterations, where the set keeps growing and the performance on all the tasks in the set keeps improving from iteration to iteration. The scope of continuous adaptation is narrower and more precise. While life-long and never-ending learning settings are deﬁned as general multi-task problems (Silver et al., 2013; Mitchell et al., 2015), continuous adaptation targets to solve a single but nonstationary task or environment. The nonstationarity in the former two problems exists and is dictated by the selected sequence of tasks. In the latter case, we assume that nonstationarity is caused by some underlying dynamics in the properties of a given task in the ﬁrst place (e.g., changes in the behavior of other agents in a multiagent setting). Finally, in the life-long and never-ending scenarios the boundary between training and execution is blurred as such systems constantly operate in the training regime. Continuous adaptation, on the other hand, expects a (potentially trained) agent to adapt to the changes in the environment at execution time under the pressure of limited data or interaction experience between the changes1. Nonstationarity of multi-agent environments is a well known issue that has been extensively studied in the context of learning in simple multi-player iterated games (such as rock-paper-scissors) where each episode is one-shot interaction (Singh et al., 2000; Bowling, 2005; Conitzer & Sandholm, 2007). In such games, discovering and converging to a Nash equilibrium strategy is a success for the learning agents. Modeling and exploiting opponents (Zhang & Lesser, 2010; Mealing & Shapiro, 2013) or even their learning processes (Foerster et al., 2017b) is advantageous as it improves convergence or helps to discover equilibria of certain properties (e.g., leads to cooperative behavior). In contrast, each episode in RoboSumo consists of multiple steps, happens in continuous time, and requires learning a good intra-episodic controller. Finding Nash equilibria in such a setting is hard. Thus, fast adaptation becomes one of the few viable strategies against changing opponents. 1The limited interaction aspect of continuous adaptation makes the problem somewhat similar to the recently proposed life-long few-shot learning (Finn et al., 2017a). 2Published as a conference paper at ICLR 2018    T (a)i 1 i 1 Ti 1i i Tii+1 i+1 Ti+1 ::: :::::: ::: (b) +Policy Loss Trajectory +Intermediate steps deterministic stochastic gradient (c) Fig. 1: (a) A probabilistic model for MAML in a multi-task RL setting. The task, T, the policies, , and the trajectories, , are all random variables with dependencies encoded in the edges of the given graph. (b) Our extended model suitable for continuous adaptation to a task changing dynamically due to non-stationarity of the environment. Policy and trajectories at a previous step are used to construct a new policy for the current step. (c) Computation graph for the meta-update from itoi+1. Boxes represent replicas of the policy graphs with the speciﬁed parameters. The model is optimized via truncated backpropagation through time starting from LTi+1. Our proposed method for continuous adaptation follows the general meta-learning paradigm (Schmidhuber, 1987; Thrun & Pratt, 1998), i.e., it learns a high-level procedure that can be used to generate a good policy each time the environment changes. There is a wealth of work on meta-learning, including methods for learning update rules for neural models that were explored in the past (Bengio et al., 1990; 1992; Schmidhuber, 1992), and more recent approaches that focused on learning optimizers for deep networks (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2016; Ravi & Larochelle, 2016), generating model parameters (Ha et al., 2016; Edwards & Storkey, 2016; Al-Shedivat et al., 2017), learning task embeddings (Vinyals et al., 2016; Snell et al., 2017) including memory-based approaches (Santoro et al., 2016), learning to learn implicitly via RL (Wang et al., 2016; Duan et al., 2016), or simply learning a good initialization (Finn et al., 2017b). 3 M ETHOD The problem of continuous adaptation in nonstationary environments immediately puts learning into the few-shot regime: the agent must learn from only limited amount of experience that it can collect before its environment changes. Therefore, we build our method upon the previous work on gradient-based model-agnostic meta-learning (MAML) that has been shown successful in the fewshot settings (Finn et al., 2017b). In this section, we re-derive MAML for multi-task reinforcement learning from a probabilistic perspective ( cf.Grant et al., 2018), and then extend it to dynamically changing tasks. 3.1 A PROBABILISTIC VIEW OF MODEL -AGNOSTIC META -LEARNING (MAML) Assume that we are given a distribution over tasks, D(T), where each task, T, is a tuple: T:= (LT;PT(x);PT(xt+1jxt;at);H) (1) LTis a task-speciﬁc loss function that maps a trajectory, := (x0;a1;x1;R1;:::;aH;xH;RH)2 T, to a loss value, i.e., LT:T 7!R;PT(x)andPT(xt+1jxt;at)deﬁne the Markovian dynamics of the environment in task T;Hdenotes the horizon; observations, xt, and actions, at, are elements (typically, vectors) of the observation space, X, and action space, A, respectively. The loss of a trajectory, , is the negative cumulative reward, LT() := PH t=1Rt. The goal of meta-learning is to ﬁnd a procedure which, given access to a limited experience on a task sampled from D(T), can produce a good policy for solving it. More formally, after querying Ktrajectories from a task TD(T)under policy , denoted 1:K , we would like to construct a new, task-speciﬁc policy, , that would minimize the expected subsequent loss on the task T. In particular, MAML constructs parameters of the task-speciﬁc policy, , using gradient of LTw.r.t.: := rLT  1:K  ;whereLT  1:K  :=1 KKX k=1LT(k );andk PT(j)(2) 3Published as a conference paper at ICLR 2018 Algorithm 1 Meta-learning at training time. input Distribution over pairs of tasks, P(Ti;Ti+1), learning rate, . 1: Randomly initialize and. 2:repeat 3: Sample a batch of task pairs, f(Ti;Ti+1)gn i=1. 4: for all task pairs (Ti;Ti+1)in the batch do 5: Sample traj. 1:K fromTiusing. 6: Compute=(1:K ;; )as given in (7). 7: Sample traj. fromTi+1using. 8: end for 9: ComputerLTi;Ti+1andrLTi;Ti+1using 1:K andas given in (8). 10: Update  +rLT(;). 11: Update  +rLT(;). 12:until Convergence output Optimaland.Algorithm 2 Adaptation at execution time. input A stream of tasks, T1;T2;T3;:::. 1: Initialize =. 2:while there are new incoming tasks do 3: Get a new task, Ti, from the stream. 4: SolveTiusingpolicy. 5: While solving Ti, collect trajectories, 1:K i;. 6: Update  (1:K i;;;)using importance-corrected meta-update as in (9). 7:end while Policy parameter space We call (2)theadaptation update with a step. The adaptation update is parametrized by , which we optimize by minimizing the expected loss over the distribution of tasks, D(T)—the meta-loss : min ETD(T)[LT()];whereLT() :=E1:K PT(j) EPT(j) LT()j1:K ; (3) where andare trajectories obtained under and, respectively. In general, we can think of the task, trajectories, and policies, as random variables (Fig. 1a), where  is generated from some conditional distribution PT(j;1:k). The meta-update (2)is equivalent to assuming the delta distribution, PT(j;1:k) :=  r1 KPK k=1LT(k) 2. To optimize (3), we can use the policy gradient method (Williams, 1992), where the gradient of LTis as follows: rLT() =E1:K PT(j) PT(j)" LT()" rlog() +rKX k=1log(k )## (4) The expected loss on a task, LT, can be optimized with trust-region policy (TRPO) (Schulman et al., 2015a) or proximal policy (PPO) (Schulman et al., 2017) optimization methods. For details and derivations please refer to Appendix A. 3.2 C ONTINUOUS ADAPTATION VIA META -LEARNING In the classical multi-task setting, we make no assumptions about the distribution of tasks, D(T). When the environment is nonstationary, we can see it as a sequence of stationary tasks on a certain timescale where the tasks correspond to different dynamics of the environment. Then, D(T)is deﬁned by the environment changes, and the tasks become sequentially dependent. Hence, we would like to exploit this dependence between consecutive tasks and meta-learn a rule that keeps updating the policy in a way that minimizes the total expected loss encountered during the interaction with the changing environment. For instance, in the multi-agent setting, when playing against an opponent that changes its strategy incrementally (e.g., due to learning), our agent should ideally meta-learn to anticipate the changes and update its policy accordingly. In the probabilistic language, our nonstationary environment is equivalent to a distribution of tasks represented by a Markov chain (Fig. 1b). The goal is to minimize the expected loss over the chain of tasks of some length L: min EP(T0);P(Ti+1jTi)"LX i=1LTi;Ti+1()# (5) 2Grant et al. (2018) similarly reinterpret adaptation updates (in non-RL settings) as Bayesian inference. 4Published as a conference paper at ICLR 2018 Here,P(T0)andP(Ti+1jTi)denote the initial and the transition probabilities in the Markov chain of tasks. Note that (i) we deal with Markovian dynamics on two levels of hierarchy, where the upper level is the dynamics of the tasks and the lower level is the MDPs that represent particular tasks, and (ii) the objectives, LTi;Ti+1, will depend on the way the meta-learning process is deﬁned. Since we are interested in adaptation updates that are optimal with respect to the Markovian transitions between the tasks, we deﬁne the meta-loss on a pair of consecutive tasks as follows: LTi;Ti+1() :=E1:K i;PTi(j)h Ei+1;PTi+1(j) LTi+1(i+1;)j1:K i;;i (6) The principal difference between the loss in (3)and(6)is that trajectories 1:K i;come from the current task, Ti, and are used to construct a policy, , that is good for the upcoming task, Ti+1. Note that even though the policy parameters, i, are sequentially dependent (Fig. 1b), in (6)we always start from the initial parameters, 3. Hence, optimizing LTi;Ti+1()is equivalent to truncated backpropagation through time with a unit lag in the chain of tasks. To construct parameters of the policy for task Ti+1, we start from and do multiple4meta-gradient steps with adaptive step sizes as follows (assuming the number of steps is M): 0 i:=;1:K PTi(j); m i:=m 1 i mrm 1 iLTi 1:K i;m 1 i ; m = 1;:::;M 1; i+1:=M 1 i MrM 1 iLTi 1:K i;M 1 i(7) wherefmgM m=1is a set of meta-gradient step sizes that are optimized jointly with . The computation graph for the meta-update is given in Fig. 1c. The expression for the policy gradient is the same as in (4) but with the expectation is now taken w.r.t. to both TiandTi+1: r;LTi;Ti+1(;) = E1:K i;PTi(j) i+1;PTi+1(j)" LTi+1(i+1;)" r;log(i+1;) +rKX k=1log(k i;)## (8) More details and the analog of the policy gradient theorem for our setting are given in Appendix A. Note that computing adaptation updates requires interacting with the environment under while computing the meta-loss, LTi;Ti+1, requires using , and hence, interacting with each task in the sequence twice. This is often impossible at execution time, and hence we use slightly different algorithms at training and execution times. Meta-learning at training time. Once we have access to a distribution over pairs of consecutive tasks5,P(Ti 1;Ti), we can meta-learn the adaptation updates by optimizing andjointly with a gradient method, as given in Algorithm 1. We use to collect trajectories from Tiandwhen interacting with Ti+1. Intuitively, the algorithm is searching for andsuch that the adaptation update (7)computed on the trajectories from Tibrings us to a policy, , that is good for solving Ti+1. The main assumption here is that the trajectories from Ticontain some information about Ti+1. Note that we treat adaptation steps as part of the computation graph (Fig. 1c) and optimize and via backpropagation through the entire graph, which requires computing second order derivatives. Adaptation at execution time. Note that to compute unbiased adaptation gradients at training time, we have to collect experience in Tiusing. At test time, due to environment nonstationarity, we usually do not have the luxury to access to the same task multiple times. Thus, we keep acting according to and re-use past experience to compute updates of for each new incoming task (see Algorithm 2). To adjust for the fact that the past experience was collected under a policy different from, we use importance weight correction. In case of single step meta-update, we have: i:= 1 KKX k=1(k) i 1(k) rLTi 1(k);1:KPTi 1(ji 1); (9) 3This is due to stability considerations. We ﬁnd empirically that optimization over sequential updates from itoi+1is unstable, often tends to diverge, while starting from the same initialization leads to better behavior. 4Empirically, it turns out that constructing via multiple meta-gradient steps (between 2 and 5) with adaptive step sizes tends yield better results in practice. 5Given a sequences of tasks generated by a nonstationary environment, T1;T2;T3;:::;TL, we use the set of all pairs of consecutive tasks, f(Ti 1;Ti)gL i=1, as the training distribution. 5Published as a conference paper at ICLR 2018 (a)  (b)  (c) Fig. 2: (a) The three types of agents used in experiments. The robots differ in the anatomy: the number of legs, their positions, and constraints on the thigh and knee joints. (b) The nonstationary locomotion environment. The torques applied to red-colored legs are scaled by a dynamically changing factor. (c) RoboSumo environment. wherei 1andiare used to rollout from Ti 1andTi, respectively. Extending importance weight correction to multi-step updates is straightforward and requires simply adding importance weights to each of the intermediate steps in (7). 4 E NVIRONMENTS We have designed a set of environments for testing different aspects of continuous adaptation methods in two scenarios: (i) simple environments that change from episode to episode according to some underlying dynamics, and (ii) a competitive multi-agent environment, RoboSumo , that allows different agents to play sequences of games against each other and keep adapting to incremental changes in each other’s policies. All our environments are based on MuJoCo physics simulator (Todorov et al., 2012), and all agents are simple multi-leg robots, as shown in Fig. 2a. 4.1 D YNAMIC First, we consider the problem of robotic locomotion in a changing environment. We use a six-leg agent (Fig. 2b) that observes the absolute position and velocity of its body, the angles and velocities of its legs, and it acts by applying torques to its joints. The agent is rewarded proportionally to its moving speed in a ﬁxed direction. To induce nonstationarity, we select a pair of legs of the agent and scale down the torques applied to the corresponding joints by a factor that linearly changes from 1 to 0 over the course of 7 episodes. In other words, during the ﬁrst episode all legs are fully functional, while during the last episode the agent has two legs fully paralyzed (even though the policy can generate torques, they are multiplied by 0 before being passed to the environment). The goal of the agent is to learn to adapt from episode to episode by changing its gait so that it is able to move with a maximal speed in a given direction despite the changes in the environment (cf. Cully et al., 2015). Also, there are 15 ways to select a pair of legs of a six-leg creature which gives us 15 different nonstationary environments. This allows us to use a subset of these environments for training and a separate held out set for testing. The training and testing procedures are described in the next section. 4.2 C OMPETITIVE Our multi-agent environment, RoboSumo , allows agents to compete in the 1-vs-1 regime following the standard sumo rules6. We introduce three types of agents, Ant,Bug, andSpider , with different anatomies (Fig. 2a). During the game, each agent observes positions of itself and the opponent, its own joint angles, the corresponding velocities, and the forces exerted on its own body (i.e., equivalent of tactile senses). The action spaces are continuous. Iterated adaptation games. To test adaptation, we deﬁne the iterated adaptation game (Fig. 3)—a game between a pair of agents that consists of Krounds each of which consists of one or more ﬁxed length episodes (500 time steps each). The outcome of each round is either win, loss, or draw. The agent that wins the majority of rounds (with at least 5% margin) is declared the winner of the game. There are two distinguishing aspects of our setup: First, the agents are trained either via pure self-play or versus opponents from a ﬁxed training collection. At test time, they face a new opponent from a testing collection. Second, the agents are allowed to learn (or adapt) at test time. In particular, an 6To win, the agent has to push the opponent out of the ring or make the opponent’s body touch the ground. 6Published as a conference paper at ICLR 2018 Round 1 Round 2 Round 3 Round K Opponent: version 1 version 2 version 3 version K Agent: Episodes: Fig. 3: An agent competes with an opponent in an iterated adaptation games that consist of multi-episode rounds. The agent wins a round if it wins the majority of episodes (wins and losses illustrated with color). Both the agent and its opponent may update their policies from round to round (denoted by the version number). agent should exploit the fact that it plays against the same opponent multiple consecutive rounds and try to adjust its behavior accordingly. Since the opponent may also be adapting, the setup allows to test different continuous adaptation strategies, one versus the other. Reward shaping. InRoboSumo , rewards are naturally sparse: the winner gets +2000, the loser is penalized for -2000, and in case of a draw both opponents receive -1000 points. To encourage fast learning at the early stages of training, we shape the rewards given to agents in the following way: the agent (i) gets reward for staying closer to the center of the ring, for moving towards the opponent, and for exerting forces on the opponent’s body, and (ii) gets penalty inversely proportional to the opponent’s distance to the center of the ring. At test time, the agents continue having access to the shaped reward as well and may use it to update their policies. Throughout our experiments, we use discounted rewards with the discount factor,  = 0:995. More details are in Appendix D.2. Calibration. To study adaptation, we need a well-calibrated environment in which none of the agents has an initial advantage. To ensure the balance, we increased the mass of the weaker agents ( Ant andSpider ) such that the win rates in games between one agent type versus the other type in the non-adaptation regime became almost equal (for details on calibration see Appendix D.3). 5 E XPERIMENTS Our goal is to test different adaptation strategies in the proposed nonstationary RL settings. However, it is known that the test-time behavior of an agent may highly depend on a variety of factors besides the chosen adaptation method, including training curriculum, training algorithm, policy class, etc. Hence, we ﬁrst describe the precise setup that we use in our experiments to eliminate irrelevant factors and focus on the effects of adaptation. Most of the low-level details are deferred to appendices. Video highlights of our experiments are available at https://goo.gl/tboqaN . 5.1 T HE SETUP Policies. We consider 3 types of policy networks: (i) a 2-layer MLP, (ii) embedding (i.e., 1 fully-connected layer replicated across the time dimension) followed by a 1-layer LSTM, and (iii) RL2(Duan et al., 2016) of the same architecture as (ii) which additionally takes previous reward and done signals as inputs at each step, keeps the recurrent state throughout the entire interaction with a given environment (or an opponent), and resets the state once the latter changes. For advantage functions, we use networks of the same structure as for the corresponding policies and have no parameter sharing between the two. Our meta-learning agents use the same policy and advantage function structures as the baselines and learn a 3-step meta-update with adaptive step sizes as given in (7). Illustrations and details on the architectures are given in Appendix B. Meta-learning. We compute meta-updates via gradients of the negative discounted rewards received during a number of previous interactions with the environment. At training time, meta-learners interact with the environment twice, ﬁrst using the initial policy, , and then the meta-updated policy,. At test time, the agents are limited to interacting with the environment only once, and hence always act according to and compute meta-updates using importance-weight correction (see Sec. 3.2 and Algorithm 2). Additionally, to reduce the variance of the meta-updates at test time, the agents store the experience collected during the interaction with the test environment (and the corresponding importance weights) into the experience buffer and keep re-using that experience 7Published as a conference paper at ICLR 2018 1 3 5 705001000Total episodic reward Back two legs 1 3 5 7 Consecutive episodes05001000 Middle two legs 1 3 5 705001000 Front two legs Policy + adaptation method MLP MLP + PPO-tracking MLP + meta-updates LSTM LSTM + PPO-tracking LSTM + meta-updates RL2 Fig. 4: Episodic rewards for 7 consecutive episodes in 3 held out nonstationary locomotion environments. To evaluate adaptation strategies, we ran each of them in each environment for 7 episodes followed by a full reset of the environment, policy, and meta-updates (repeated 50 times). Shaded regions are 95% conﬁdence intervals. Best viewed in color. to updateas in (7). The size of the experience buffer is ﬁxed to 3 episodes for nonstationary locomotion and 75 episodes for RoboSumo . More details are given in Appendix C.1. Adaptation baselines. We consider the following three baseline strategies: (i) naive (or no adaptation), (ii) implicit adaptation via RL2, and (iii) adaptation via tracking (Sutton et al., 2007) that keeps doing PPO updates at execution time. Training in nonstationary locomotion. We train all methods on the same collection of nonstationary locomotion environments constructed by choosing all possible pairs of legs whose joint torques are scaled except 3 pairs that are held out for testing (i.e., 12 training and 3 testing environments for the six-leg creature). The agents are trained on the environments concurrently, i.e., to compute a policy update, we rollout from all environments in parallel and then compute, aggregate, and average the gradients (for details, see Appendix C.2). LSTM policies retain their state over the course of 7 episodes in each environment. Meta-learning agents compute meta-updates for each nonstationary environment separately. Training in RoboSumo .To ensure consistency of the training curriculum for all agents, we ﬁrst pre-train a number of policies of each type for every agent type via pure self-play with the PPO algorithm (Schulman et al., 2017; Bansal et al., 2018). We snapshot and save versions of the pretrained policies at each iteration. This lets us train other agents to play against versions of the pre-trained opponents at various stages of mastery. Next, we train the baselines and the meta-learning agents against the pool of pre-trained opponents7concurrently. At each iteration kwe (a) randomly select an opponent from the training pool, (b) sample a version of the opponent’s policy to be in [1;k] (this ensures that even when the opponent is strong, sometimes an undertrained version is selected which allows the agent learn to win at early stages), and (c) rollout against that opponent. All baseline policies are trained with PPO; meta-learners also used PPO as the outer loop for optimizing and parameters. We retain the states of the LSTM policies over the course of interaction with the same version of the same opponent and reset it each time the opponent version is updated. Similarly to the locomotion setup, meta-learners compute meta-updates for each opponent in the training pool separately. A more detailed description of the distributed training is given in Appendix C.2. Experimental design. We design our experiments to answer the following questions: When the interaction with the environment before it changes is strictly limited to one or very few episodes, what is the behavior of different adaptation methods in nonstationary locomotion and competitive multi-agent environments? What is the sample complexity of different methods, i.e., how many episodes is required for a method to successfully adapt to the changes? We test this by controlling the amount of experience the agent is allowed to get form the same environment before it changes. 7In competitive multi-agent environments, besides self-play, there are plenty of ways to train agents, e.g., train them in pairs against each other concurrently, or randomly match and switch opponents each few iterations. We found that concurrent training often leads to an unbalanced population of agents that have been trained under vastly different curricula and introduces spurious effects that interfere with our analysis of adaptation. Hence, we leave the study of adaptation in naturally emerging curricula in multi-agent settings to the future work. 8Published as a conference paper at ICLR 2018 0 25 50 75 1000.20.40.6Win rate 0 25 50 75 100 Consecutive rounds0.20.40.6 0 25 50 75 1000.20.40.6Opponent: Ant Opponent: Bug Opponent: SpiderAgent: Spider RL2LSTM + PPO-tracking LSTM + meta-updates 0 25 50 75 1000.20.40.60.8Win rate 0 25 50 75 100 Consecutive rounds0.20.40.60.8 0 25 50 75 1000.20.40.60.8Opponent: Ant Opponent: Bug Opponent: SpiderAgent: Bug RL2LSTM + PPO-tracking LSTM + meta-updates 0 25 50 75 1000.20.40.6Win rate 0 25 50 75 100 Consecutive rounds0.00.20.40.6 0 25 50 75 1000.20.40.6Opponent: Ant Opponent: Bug Opponent: SpiderAgent: Ant RL2LSTM + PPO-tracking LSTM + meta-updates Fig. 5: Win rates for different adaptation strategies in iterated games versus 3 different pre-trained opponents. At test time, both agents and opponents started from versions 700. Opponents’ versions were increasing with each consecutive round as if they were learning via self-play, while agents were allowed to adapt only from the limited experience with a given opponent. Each round consisted of 3 episodes. Each iterated game was repeated 100 times; shaded regions denote bootstrapped 95% conﬁdence intervals; no smoothing. Best viewed in color. Additionally, we ask the following questions speciﬁc to the competitive multi-agent setting: Given a diverse population of agents that have been trained under the same curriculum, how do different adaptation methods rank in a competition versus each other? When the population of agents is evolved for several generations—such that the agents interact with each other via iterated adaptation games, and those that lose disappear while the winners get duplicated—what happens with the proportions of different agents in the population? 5.2 A DAPTATION IN THE FEW -SHOT REGIME AND SAMPLE COMPLEXITY Few-shot adaptation in nonstationary locomotion environments. Having trained baselines and meta-learning policies as described in Sec. 5.1, we selected 3 testing environments that corresponded to disabling 3 different pairs of legs of the six-leg agent: back, middle, and front legs. The results are presented on Fig. 4. Three observations: First, during the very ﬁrst episode, the meta-learned initial policy,?, turns out to be suboptimal for the task (it underperforms compared to other policies). However, after 1-2 episodes (and environment changes), it starts performing on par with other policies. Second, by the 6th and 7th episodes, meta-updated policies perform much better than the rest. Note that we use 3 gradient meta-updates for the adaptation of the meta-learners; the meta-updates are computed based on experience collected during the previous 2 episodes. Finally, tracking is not able to improve upon the baseline without adaptation and sometimes leads to even worse results. Adaptation in RoboSumo under the few-shot constraint. To evaluate different adaptation methods in the competitive multi-agent setting consistently, we consider a variation of the iterated adaptation game, where changes in the opponent’s policies at test time are pre-determined but unknown to the 9Published as a conference paper at ICLR 2018 0 25 50 750.20.30.40.50.6Win rate Ant vs Ant 0 25 50 75 Episodes per round0.20.30.40.50.6 Bug vs Bug 0 25 50 750.20.30.40.50.6 Spider vs Spider RL2 LSTM + PPO-tracking LSTM + meta-updates No adaptation Fig. 6: The effect of increased number of episodes per round in the iterated games versus a learning opponent. agents. In particular, we pre-train 3 opponents (1 of each type, Fig. 2a) with LSTM policies with PPO via self-play (the same way as we pre-train the training pool of opponents, see Sec. 5.1) and snapshot their policies at each iteration. Next, we run iterated games between our trained agents that use different adaptation algorithms versus policy snapshots of the pre-trained opponents. Crucially, the policy version of the opponent keeps increasing from round to round as if it was training via self-play8. The agents have to keep adapting to increasingly more competent versions of the opponent (see Fig. 3). This setup allows us to test different adaptation strategies consistently against the same learning opponents. The results are given on Fig. 5. We note that meta-learned adaptation strategies, in most cases, are able to adapt and improve their win-rates within about 100 episodes of interaction with constantly improving opponents. On the other hand, performance of the baselines often deteriorates during the rounds of iterated games. Note that the pre-trained opponents were observing 90 episodes of self-play per iteration, while the agents have access to only 3 episodes per round. Sample complexity of adaptation in RoboSumo .Meta-learning helps to ﬁnd an update suitable for fast or few-shot adaptation. However, how do different adaptation methods behave when more experience is available? To answer this question, we employ the same setup as previously and vary the number of episodes per round in the iterated game from 3 to 90. Each iterated game is repeated 20 times, and we measure the win-rates during the last 25 rounds of the game. The results are presented on Fig. 6. When the number of episodes per round goes above 50, adaptation via tracking technically turns into “learning at test time,” and it is able to learn to compete against the self-trained opponents that it has never seen at training time. The meta-learned adaptation strategy performed near constantly the same in both few-shot and standard regimes. This suggests that the meta-learned strategy acquires a particular bias at training time that allows it to perform better from limited experience but also limits its capacity of utilizing more data. Note that, by design, the meta-updates are ﬁxed to only 3 gradient steps from ?with step-sizes ?(learned at training), while tracking keeps updating the policy with PPO throughout the iterated game. Allowing for meta-updates that become more ﬂexible with the availability of data can help to overcome this limitation. We leave this to future work. 5.3 E VALUATION ON THE POPULATION -LEVEL Combining different adaptation strategies with different policies and agents of different morphologies puts us in a situation where we have a diverse population of agents which we would like to rank according to the level of their mastery in adaptation (or ﬁnd the “ﬁttest”). To do so, we employ TrueSkill (Herbrich et al., 2007)—a metric similar to the ELO rating, but more popular in 1-vs-1 competitive video-games. In this experiment, we consider a population of 105 trained agents: 3 agent types, 7 different policy and adaptation combinations, and 5 different stages of training (from 500 to 2000 training iterations). First, we assume that the initial distribution of any agent’s skill is N(25;25=3)and the default distance that guarantees about 76% of winning, = 4:1667 . Next, we randomly generate 1000 matches between pairs of opponents and let them adapt while competing with each other in 100-round iterated adaptation games (states of the agents are reset before each game). After each game, we 8At the beginning of the iterated game, both agents and their opponent start from version 700, i.e., from the policy obtained after 700 iterations (PPO epochs) of learning to ensure that the initial policy is reasonable. 10Published as a conference paper at ICLR 2018 record the outcome and updated our belief about the skill of the corresponding agents using the TrueSkill algorithm9. The distributions of the skill for the agents of each type after 1000 iterated adaptation games between randomly selected players from the pool are visualized in Fig. 7. MLP LSTM203040TrueSkillSub-population: Ants MLP LSTM203040Sub-population: Bugs MLP LSTM203040Sub-population: Spiders no adaptation PPO-tracking meta-updates RL2 Fig. 7: TrueSkill for the top-performing MLPand LSTM-based agents. TrueSkill was computed based on outcomes (win, loss, or draw) in 1000 iterated adaptation games (100 consecutive rounds per game, 3 episodes per round) between randomly selected pairs of opponents from a population of 105 pre-trained agents. There are a few observations we can make: First, recurrent policies were dominant. Second, adaptation via RL2tended to perform equally or a little worse than plain LSTM with or without tracking in this setup. Finally, agents that meta-learned adaptation rules at training time, consistently demonstrated higher skill scores in each of the categories corresponding to different policies and agent types. Finally, we enlarge the population from 105 to 1050 agents by duplicating each of them 10 times and evolve it (in the “natural selection” sense) for several generations as follows. Initially, we start with a balanced population of different creatures. Next, we randomly match 1000 pairs of agents, make them play iterated adaptation games, remove the agents that lost from the population and duplicate the winners. The same process is repeated 10 times. The result is presented in Fig 8. We see that many agents quickly disappear form initially uniform population and the meta-learners end up dominating. 0 1 2 3 4 5 6 7 8 9 10 Creature generation (#)0255075100Proportion (%)AntsBugsSpidersPolicy + adaptation MLP MLP + PPO-tracking MLP + meta-updates LSTM LSTM + PPO-tracking LSTM + meta-updates RL2 Fig. 8: Evolution of a population of 1050 agents for 10 generations. Best viewed in color. 6 C ONCLUSION AND FUTURE DIRECTIONS In this work, we proposed a simple gradient-based meta-learning approach suitable for continuous adaptation in nonstationary environments. The key idea of the method is to regard nonstationarity as a sequence of stationary tasks and train agents to exploit the dependencies between consecutive tasks such that they can handle similar nonstationarities at execution time. We applied our method to nonstationary locomotion and within a competitive multi-agent setting. For the latter, we designed theRoboSumo environment and deﬁned iterated adaptation games that allowed us to test various aspects of adaptation strategies. In both cases, meta-learned adaptation rules were more efﬁcient than the baselines in the few-shot regime. Additionally, agents that meta-learned to adapt demonstrated the highest level of skill when competing in iterated games against each other. The problem of continuous adaptation in nonstationary and competitive environments is far from being solved, and this work is the ﬁrst attempt to use meta-learning in such setup. Indeed, our meta-learning algorithm has a few limiting assumptions and design choices that we have made mainly due to computational considerations. First, our meta-learning rule is to one-step-ahead update of the 9We used an implementation from http://trueskill.org/ . 11Published as a conference paper at ICLR 2018 policy and is computationally similar to backpropagation through time with a unit time lag. This could potentially be extended to fully recurrent meta-updates that take into account the full history of interaction with the changing environment. Additionally, our meta-updates were based on the gradients of a surrogate loss function. While such updates explicitly optimized the loss, they required computing second order derivatives at training time, slowing down the training process by an order of magnitude compared to baselines. Utilizing information provided by the loss but avoiding explicit backpropagation through the gradients would be more appealing and scalable. Finally, our approach is unlikely to work with sparse rewards as the meta-updates use policy gradients and heavily rely on the reward signal. Introducing auxiliary dense rewards designed to enable meta-learning is a potential way to overcome this issue that we would like to explore in the future work. ACKNOWLEDGEMENTS We would like to thank Harri Edwards, Jakob Foerster, Aditya Grover, Aravind Rajeswaran, Vikash Kumar, Yuhuai Wu and many others at OpenAI for helpful comments and fruitful discussions. 