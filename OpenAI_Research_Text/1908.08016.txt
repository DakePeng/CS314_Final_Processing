TESTING ROBUSTNESS AGAINST UNFORESEEN ADVERSARIES Maximilian Kaufmann∗ UK Frontier AI TaskforceDaniel Kang∗ UC BerkeleyYi Sun∗ University of ChicagoSteven Basart Center for AI Safety Xuwang Yin Center for AI SafetyMantas Mazeika Center for AI SafetyAkul Arora UC BerkeleyAdam Dziedzic Toronto University Franziska Boenisch Vector InstituteJacob Steinhardt UC BerkeleyDan Hendrycks Center for AI Safety ABSTRACT Adversarial robustness research primarily focuses on Lpperturbations, and most defenses are developed with identical training-time and test-time adversaries. However, in realworld applications developers are unlikely to have access to the full range of attacks or corruptions their system will face. Furthermore, worst-case inputs are likely to be diverse and need not be constrained to the Lpball. To narrow in on this discrepancy between research and reality we introduce ImageNet -UA, a framework for evaluating model robustness against a range of unforeseen adversaries , including eighteen new nonLpattacks. To perform well on ImageNet -UA, defenses must overcome a generalization gap and be robust to a diverse attacks not encountered during training. In extensive experiments, we find that existing robustness measures do not capture unforeseen robustness, that standard robustness techniques are beat by alternative training strategies, and that novel methods can improve unforeseen robustness. We present ImageNet -UAas a useful tool for the community for improving the worst-case behavior of machine learning systems. 1 I NTRODUCTION Neural networks perform well on a variety of tasks, yet can be consistently fooled by minor adversarial distortions (Szegedy et al., 2013; Goodfellow et al., 2014). This has led to an extensive and active area of research, mainly focused on the threat model of an “ Lp-bounded adversary” that adds imperceptible distortions to model inputs to cause misclassification. However, this classic threat model may fail to fully capture many real-world concerns regarding worst-case robustness (Gilmer et al., 2018). Firstly, real-world worst-case distributions are likely to be varied, and are unlikely to be constrained to the Lpball. Secondly, developers will not have access to the worst-case inputs to which their systems will be exposed to. For example, online advertisers use perturbed pixels in ads to defeat ad blockers trained only on the previous generation of ads in an ever-escalating arms race (Tram `er et al., 2018). Furthermore, although research has shown that adversarial training can lead to overfitting, wherein robustness against one particular adversary does not generalize (Dai et al., 2022; Yu et al., 2021; Stutz et al., 2020; Tramer & Boneh, 2019), the existing literature is still focuses on defenses that train against the test-time attacks. This robustness to a train-test distribution shift has been studied when considering average-case corruptions (Hendrycks & Dietterich, 2018), but we take this to the worst-case setting. We address the limitations of current adversarial robustness evaluations by providing a repository of nineteen gradient-based attacks, which are used to create ImageNet -UA1—a benchmark for evaluating the unforeseen robustness of models on the popular ImageNet dataset (Deng et al., 2009). Defenses achieving high Unforeseen Adversarial Accuracy ( UA2) onImageNet -UAdemonstrate the ability to generalize to a diverse set of adversaries not seen at train time, demonstrating a much more realistic threat model than the Lpadversaries which are a focus of the literature. ∗Equal contribution. 1Code available at github.com/centerforaisafety/adversarial-corruptions 1arXiv:1908.08016v4  [cs.LG]  30 Oct 2023Wood  Snow Original  Glitch Kaleidoscope Elastic JPEG Gabor Pixel HSV Whirlpool Fog FBM Polkadot Blur Klotski TextureEdge Prison Mix Figure 1: The full suite of attacks. We present nineteen differentiable nonLpattacks as part of our codebase (eighteen of which are novel). For the purpose of visualization, higher distortion levels than are used in our benchmark have been chosen. See Appendix E for adversarial examples generated with the distortion levels used within our benchmark, and Appendix I for a human study on semantic preservation. Our results show that unforeseen robustness is distinct from existing robustness metrics, highlighting the need for a new measure which better captures the generalization of defense methods. We use ImageNet -UA reveal that models with high L∞attack robustness (the most ubiquitous measure of robustness in the literature) do not generalize well to new attacks, recommending L2as a stronger baseline. We further find that Lptraining can be improved on by alternative training processes, and suggest that the community focuses on methods with better generalization behavior. Interestingly, unlike in the Lpcase, we find that progress on CV benchmarks has at least partially tracked unforeseen robustness. We are hopeful that ImageNet -UA can provide an improved progress measure for defenses aiming to achieve real-world worst-case robustness. To summarize, we make the following contributions: • We design eighteen novel nonLpattacks, constituting a large increase in the set of dataset-agnostic non-Lpattacks available in the literature. • We make use of these attacks to form a new benchmark ( ImageNet -UA), standardizing and greatly expanding the scope of unforeseen robustness evaluation. • We show that it UA2 is distinct from existing robustness metrics in the literature, and demonstrates that classical Lp-training focused defense strategies can be improved on. We also measure the unforeseen robustness of a wide variety of techniques, finding promising research directions for generalizing adversarial robustness. 20 5 10 15 20 25 Number of Steps0.00.20.40.60.8P (Otter) Figure 2: Progression of an attack. As we optimize our differentiable corruptions, model performance decreases, while leaving the image semantics unchanged. Unoptimized versions of our attacks have a moderate impact on classifier performance, similar to common corruptions (Hendrycks & Dietterich, 2019), while optimized versions cause large drops in accuracy. 2 R ELATED WORK Evaluating Adversarial Robustness. Adversarial robustness is notoriously difficult to evaluate correctly (Papernot et al., 2017; Athalye et al., 2018). To this end, Carlini et al. (2019) provide extensive guidance for sound adversarial robustness evaluation. Our ImageNet -UAbenchmark incorporates several of their recommendations, such as measuring attack success rates across several magnitudes of distortion and using a broader threat model with diverse differentiable attacks. Existing measures of adversarial robustness (Croce & Hein, 2020; Moosavi-Dezfooli et al., 2015; Weng et al., 2018) almost exclusively, apply only to attacks optimizing over an Lp-ball, limiting their applicability for modeling robustness to new deploymenttime adversaries. Non-LpAttacks. Many attacks either use generative models (Song et al., 2018; Qiu et al., 2019) that are often hard to bound and are susceptible to instabilities, or make us of expensive brute-force search techniques Engstrom et al. (2017). We focus on attacks which are fast by virtue of differentiability, applicable to variety of datasets and independent of auxiliary generative models. Previous works presenting suitable attacks include Laidlaw & Feizi (2019); Shamsabadi et al. (2021); Zhao et al. (2019), who all transform the underlying color space of an image and Xiao et al. (2018) who differentiably warp images, and which we adapt to create our own Elastic attack. The literature does not have a sufficiently diverse set of suitable adversaries to effectively test the generalization properties of defenses, causing us to develop out suite of attacks. Unforeseen and Multi-attack Robustness. There exist defense methods which seek to generalize across an adversarial train-test gap (Dai et al., 2022; Laidlaw et al., 2020; Lin et al., 2020). Yet, comparison between these methods is challenging due to the lack of a standardized benchmark and an insufficient range of adversaries to test against. We fill this gap by implementing a unified benchmark for testing unforeseen robustness. The more developed field of multi-attack robustness (Tramer & Boneh, 2019) aims to create models which are robust to a range of attacks, but works generally focus on a union of Lpadversaries (Maini et al., 2020; Madaan et al., 2021a; Croce & Hein, 2022) and do not enforce that test time adversaries have to differ from those used during training. Common corruptions Several of our attacks (Pixel, Snow, JPEG and Fog) were inspired by existing common corruptions (Hendrycks & Dietterich, 2018). We fundamentally change the generation methods to make these corruptions differentiable, allowing us to focus on worst-case robustness instead of the averagecase robustness (see Section 5.1 for empirical an empirical comparison). 3<latexit sha1_base64="l98jcEUigtbiZXRBP1nzLPPWuOY=">AAACTHicbVBNSyNBEO2J37N+xPXopTFZiCBhJrCrCAtiLh5dMSpksqGnU5O09nQP3TVCGOYHetnD3vZX7GUPKyJsJwbxY+v0+r2q6lcvzqSwGAS/vMrc/MLi0vKK/2F1bX2juvnxwurccOhwLbW5ipkFKRR0UKCEq8wAS2MJl/FNe6Jf3oKxQqtzHGfQS9lQiURwho7qV3nEQSEYoYZ+vU0joWiUMhzFcXFWfi/cC0UKlj6DVlmnh7SttRkIxRDoOSirDW20tbLIFO7u+W5TvxDX5deG2LverfertaAZTIu+B+EM1MisTvvVn9FA8zx1zrhk1nbDIMNewQwKLqH0o9xCxvgNG0LXQcWcsV4xDaOknxwzoImzlGiFdMq+nChYau04jV3n5FD7VpuQ/9O6OSYHvUKoLEdQ/OmjJJcUNZ0kSwfCAEc5doBxI5xXykfMMO7itb4LIXx78ntw0WqGX5qfv7VqR8ezOJbJNtkhDRKSfXJETsgp6RBO7shv8pfcez+8P96D9/jUWvFmM1vkVVUW/wFQVbED</latexit>C2Rn⇥n⇥2: Coordinate Tensor (Constant),Cij=(i, j) <latexit sha1_base64="WYVKs/Yj5qDzN1tNYSI/yeNqdu0=">AAAB6nicbVDLSgNBEOz1GeMr6tHLYCIIQtgN+DgGvXiMaB6QLGF20psMmZ1dZmaFEPIJXjwo4tUv8ubfOEn2oIkFDUVVN91dQSK4Nq777aysrq1vbOa28ts7u3v7hYPDho5TxbDOYhGrVkA1Ci6xbrgR2EoU0igQ2AyGt1O/+YRK81g+mlGCfkT7koecUWOlh9J5qVsoumV3BrJMvIwUIUOtW/jq9GKWRigNE1Trtucmxh9TZTgTOMl3Uo0JZUPax7alkkao/fHs1Ak5tUqPhLGyJQ2Zqb8nxjTSehQFtjOiZqAXvan4n9dOTXjtj7lMUoOSzReFqSAmJtO/SY8rZEaMLKFMcXsrYQOqKDM2nbwNwVt8eZk0KmXvsnxxXylWb7I4cnAMJ3AGHlxBFe6gBnVg0IdneIU3RzgvzrvzMW9dcbKZI/gD5/MHK5GNEw==</latexit>+<latexit sha1_base64="3W9h75k4DMjP55jyMAfmzx1LDIg=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBhPBU9gN+DgGBfEY0ZhAsoTZyWwyZHZ2mekVQsgnePGgiFe/yJt/4yTZgyYWNBRV3XR3BYkUBl3328mtrK6tb+Q3C1vbO7t7xf2DRxOnmvEGi2WsWwE1XArFGyhQ8laiOY0CyZvB8HrqN5+4NiJWDzhKuB/RvhKhYBStdF++KXeLJbfizkCWiZeREmSod4tfnV7M0ogrZJIa0/bcBP0x1SiY5JNCJzU8oWxI+7xtqaIRN/54duqEnFilR8JY21JIZurviTGNjBlFge2MKA7MojcV//PaKYaX/lioJEWu2HxRmEqCMZn+TXpCc4ZyZAllWthbCRtQTRnadAo2BG/x5WXyWK1455Wzu2qpdpXFkYcjOIZT8OACanALdWgAgz48wyu8OdJ5cd6dj3lrzslmDuEPnM8fVJiNLg==</latexit>F<latexit sha1_base64="kHUz65HwYxJEhllYsFnCPXLxBy4=">AAAB9XicbVDLSsNAFL3xWeOr6tJNsBVclaTgY1l047KCfUAby2R60w6dTMLMRCmh/+HGhSJu/Rd3/o3TNgttPXDhcM69M/eeIOFMadf9tlZW19Y3Ngtb9vbO7t5+8eCwqeJUUmzQmMeyHRCFnAlsaKY5thOJJAo4toLRzdRvPaJULBb3epygH5GBYCGjRBvpoUtRaJRMDOxyWO4VS27FncFZJl5OSpCj3it+dfsxTSPzCOVEqY7nJtrPiNSMcpzY3VRhQuiIDLBjqCARKj+bbT1xTo3Sd8JYmhLamam/JzISKTWOAtMZET1Ui95U/M/rpDq88jMmklSjoPOPwpQ7OnamETh9JpFqPjaEUMnMrg4dEkmoSULZJgRv8eRl0qxWvIvK+V21VLvO4yjAMZzAGXhwCTW4hTo0gIKEZ3iFN+vJerHerY9564qVzxzBH1ifP4bNkeE=</latexit>f <latexit sha1_base64="b6gyBeUOz0V/up3/+LCqNLjVn7M=">AAACFXicbVC7TsMwFHXKq5RXgJHFokViQFVSiYeYKljoVhB9SE2oHNehVh0nsh1EFeUnWPgVFgYQYkVi429w2gxQuJKl43PvuY/jRYxKZVlfRmFufmFxqbhcWlldW98wN7faMowFJi0cslB0PSQJo5y0FFWMdCNBUOAx0vFG51m+c0eEpCG/VuOIuAG65dSnGClN9c2Dyj10KIdOgNTQ85Kr9CbRP0UDIiFPTyuwwaNYwYbWkb5ZtqrWJOBfYOegDPJo9s1PZxDiOCBcYYak7NlWpNwECUUxI2nJiSWJEB7p3j0NOdJT3WRyVQr3NDOAfij04wpO2J+KBAVSjgNPV2bLy9lcRv6X68XKP3ETmt1FOJ4O8mMGVQgzi+CACoIVG2uAsKB6V4iHSCCstJElbYI9e/Jf0K5V7aPq4WWtXD/L7SiCHbAL9oENjkEdXIAmaAEMHsATeAGvxqPxbLwZ79PSgpFrtsGvMD6+Ae0Mng8=</latexit>x2Rn⇥n: Input Image<latexit sha1_base64="TBCYa/WKUqbNK6kHjPI01iTYLq8=">AAAB9XicbVDLTsJAFJ3iC/GFunQzEUxckZbEx5LoxiUm8kigkul0ChOm02bmFiEN/+HGhca49V/c+TcO0IWCJ7nJyTn35t57vFhwDbb9beXW1jc2t/LbhZ3dvf2D4uFRU0eJoqxBIxGptkc0E1yyBnAQrB0rRkJPsJY3vJ35rRFTmkfyASYxc0PSlzzglICRHsvjXhfYGFLij6blXrFkV+w58CpxMlJCGeq94lfXj2gSMglUEK07jh2DmxIFnAo2LXQTzWJCh6TPOoZKEjLtpvOrp/jMKD4OImVKAp6rvydSEmo9CT3TGRIY6GVvJv7ndRIIrt2UyzgBJuliUZAIDBGeRYB9rhgFMTGEUMXNrZgOiCIUTFAFE4Kz/PIqaVYrzmXl4r5aqt1kceTRCTpF58hBV6iG7lAdNRBFCj2jV/RmPVkv1rv1sWjNWdnMMfoD6/MHeOiSgQ==</latexit>xadv <latexit sha1_base64="jgiOkofU9sM8mlBjA/Hu9tu7fRQ=">AAACFnicbVDLSsNAFJ3UV42vqks3wVZoQUtS8LEsunHhooJ9QFPKZHrTDp1MwsyktIR+hRt/xY0LRdyKO//G6WOhrQcuHM65l3vv8SJGpbLtbyO1srq2vpHeNLe2d3b3MvsHNRnGgkCVhCwUDQ9LYJRDVVHFoBEJwIHHoO71byZ+fQBC0pA/qFEErQB3OfUpwUpL7cyZS4ArEJR3TdPMuQFWPYJZcjfO+/lh21UwVAnuDMaF01Eh185k7aI9hbVMnDnJojkq7cyX2wlJHOgdhGEpm44dqVaChaKEwdh0YwkRJn3chaamHAcgW8n0rbF1opWO5YdCF1fWVP09keBAylHg6c7J2XLRm4j/ec1Y+VethPIoVsDJbJEfM0uF1iQjq0MFEMVGmmAiqL7VIj0sMNFBSVOH4Cy+vExqpaJzUTy/L2XL1/M40ugIHaM8ctAlKqNbVEFVRNAjekav6M14Ml6Md+Nj1poy5jOH6A+Mzx9o6p5I</latexit>L(f(xadv),y) <latexit sha1_base64="bJBB+rMWDqrmVwgbmwbvirDu2ZM=">AAAB9XicbVDLSsNAFL3xWeOr6tJNsAiuSlLwsSwq6LKCfUAby2R60w6dTMLMRCmh/+HGhSJu/Rd3/o3TNgttPXDhcM69M/eeIOFMadf9tpaWV1bX1gsb9ubW9s5ucW+/oeJUUqzTmMeyFRCFnAmsa6Y5thKJJAo4NoPh1cRvPqJULBb3epSgH5G+YCGjRBvpoUNRaJRM9O3azXW3WHLL7hTOIvFyUoIctW7xq9OLaRqZRygnSrU9N9F+RqRmlOPY7qQKE0KHpI9tQwWJUPnZdOuxc2yUnhPG0pTQzlT9PZGRSKlRFJjOiOiBmvcm4n9eO9XhhZ8xkaQaBZ19FKbc0bEzicDpMYlU85EhhEpmdnXogEhCTRLKNiF48ycvkkal7J2VT+8qpeplHkcBDuEITsCDc6jCLdSgDhQkPMMrvFlP1ov1bn3MWpesfOYA/sD6/AHLOpIO</latexit>PGD<latexit sha1_base64="MRDNonhhlTXrcAu7QblhbSZ4xo4=">AAAB73icbVDLTgJBEOzFF+IL9ehlIph4IrskPo5ELx4xkUcCGzI7zMKE2dl1pteEEH7CiweN8ervePNvHGAPClbSSaWqO91dQSKFQdf9dnJr6xubW/ntws7u3v5B8fCoaeJUM95gsYx1O6CGS6F4AwVK3k40p1EgeSsY3c781hPXRsTqAccJ9yM6UCIUjKKV2uUuioibcq9YcivuHGSVeBkpQYZ6r/jV7ccsjbhCJqkxHc9N0J9QjYJJPi10U8MTykZ0wDuWKmq3+JP5vVNyZpU+CWNtSyGZq78nJjQyZhwFtjOiODTL3kz8z+ukGF77E6GSFLlii0VhKgnGZPY86QvNGcqxJZRpYW8lbEg1ZWgjKtgQvOWXV0mzWvEuKxf31VLtJosjDydwCufgwRXU4A7q0AAGEp7hFd6cR+fFeXc+Fq05J5s5hj9wPn8Ac1KPmA==</latexit>⇥ <latexit sha1_base64="wV0fAR/ZL+C1zjww2vAdpQ2esxE=">AAACHXicbVBNSyNBEO3x2/gV9eilMQoRJMyIH4sns172qGCMkIRQ01OJTXq6h+4a2RD8I172r+zFg7J48CL7b+zEHPwqaHi8V6+q68WZko7C8H8wMTk1PTM7N19YWFxaXimurl06k1uBNWGUsVcxOFRSY40kKbzKLEIaK6zHvdOhXr9B66TRF9TPsJVCV8uOFECeahf3mwI1oZW6W+Bb1fLv3WaCimDneItXk6ETrATFq0QgerxcNybZaRdLYSUcFf8KojEosXGdtYvPzcSIPPWrhALnGlGYUWsAlqRQeFto5g4zPx+62PBQQ4quNRhdd8u3PZPwjrH+aeIj9r1jAKlz/TT2nSnQtfusDcnvtEZOnR+tgdRZTqjF26JOrjgZPoyKJ9KiINX3AISV/q9cXIMF4fNyBR9C9Pnkr+ByrxIdVg7O90onP8dxzLENtsnKLGJH7IT9YmesxgS7Y3/ZA3sM/gT3wb/g6a11Ihh71tmHCl5eAUU3oC4=</latexit>A(x, ) : Adversarial Attack (Wood) <latexit sha1_base64="iS3XBJU4i/rFvXkjTwwI8NeAj2E=">AAACN3icbZDLSgMxFIYz3q23qks3wSq4KjMFL7jysnEhomKr0KklkzmtwUxmSM6IZZi3cuNruNONC0Xc+gamF8TbgcDHf3Jy8v9BIoVB1310hoZHRsfGJyYLU9Mzs3PF+YWaiVPNocpjGeuLgBmQQkEVBUq4SDSwKJBwHlzvd/vnN6CNiNUZdhJoRKytREtwhlZqFo98DgpBC9UurPghSGTUF4r6EcOrIMhO88ssoj6KCAz9gkq+bRFuMaO74U2ZHjK0r5h8pVksuWW3V/QveAMokUEdN4sPfhjzNLLjXDJj6p6bYCNjGgWXkBf81EDC+DVrQ92iYnZ9I+v5zumqVULairU9CmlP/T6RsciYThTYm1075nevK/7Xq6fY2mpkQiWp9cX7i1qppBjTbog0FBo4yo4FxrWwf6X8imnGbZKmYEPwflv+C7VK2dsor59USjt7gzgmyBJZJmvEI5tkhxyQY1IlnNyRJ/JCXp1759l5c977V4ecwcwi+VHOxyexD6vO</latexit> 2Rm⇥m⇥2: Adv. LatentsFigure 3: An illustrative example of one of our attacks. All of our attacks function by performing PGD optimization on a set of latent variables. In the case of the Wood attack, these latent variables are inputs to concentric sine waves ( F(x, y) = sin(p x2+y2)) which are overlaid on the image. We design effective attacks which are fast, easy to optimize, precisely bound, preserve image semantics, are portable across datasets and have variable intensity through the εparameter. 3 T HEUNFORESEEN ROBUSTNESS THREAT MODEL Action Space of Adversaries. The allowed action space of an adversary is defined using a perturbation set Sxof potential adversarial examples for each input x. Given such a set, and a classifier fwhich correctly classifies a point xwith its ground truth label y, anadversarial example xadvis defined to be a member the perturbation set Sxwhich causes the classifier to give an incorrect prediction: xadv∈Sx:f(xadv)̸=f(x) (1) Then, under some distribution Dof interest, the task of adversarial defenses is typically to achieve high accuracy in the face of an adversary which is allowed to optimse within the peturbation set. Theunforeseen robustness of a classifier is the classifier’s accuracy under some distribution of adversaries: E(x,y),A∼D,A min xadv∈SAx{1f(xadv)=y} . (2) This is similar to the usual adversarial accuracy (Madry et al., 2017a) , but instead of including a single Lp adversary, we define a diverse distribution of adversaries A(where each adversary A∈Dom (A)defines a different perturbation set SA xfor each input x). Information Available to the Adversaries. To ensure that our adversaries are as strong as possible (Carlini et al., 2019), and to avoid the usage of expensive black-box optimization techniques, we allow full whitebox access to the victim models. Constraints on the Defender. We enforce that defenders allow adversaries to compute gradients, in line which previous work demonstrating that defenses relying on masking of gradients are ineffective (Athalye et al., 2018). We also enforce that defenses do not make use of access to adversaries which are part of the test-time distribution A. This assumption of unforeseen adversaries is contrary to most of the literature where the most powerful defenses involve explicitly training against the test time adversaries (Madry et al., 2017b), and allows us to model more realistic real-world situations where it is unlikely that defenders will have full knowledge of the adversaries at deployment time. 4Model L∞(ε= 4/255) UA2 Dinov2 Vit-large 27.7 27.2 Convnext-V2-large IN-1k+22K 0.0 19.2 Swin-Large ImageNet1K 0.0 16.2 ConvNext-Base L∞, (ε= 8/255) 58.0 22.3 Resnet-50, L∞(ε= 8/255) 38.9 10 Resnet-50 L2, (ε= 5) 34.1 13.9 Table 1: Lprobustness is not necessary for unforeseen robustness. We highlight some of the models which achieve high UA2, while still being susceptible to Lpattacks. These models demonstrate that unforeseen robustness is distinct from achieving Lprobustness. 4 M EASURING UNFORESEEN ROBUSTNESS To evaluate the unforeseen robustness of models, we introduce a new evaluation framework consisting of a benchmark ImageNet -UAand metric UA2 (Unforeseen Adversarial Accuracy). We also further release our nineteen (eighteen of which novel) approaches for generating nonLpadversarial examples. We performed extensive sweeps to find the most effective hyperparameters for all of our attacks (see Appendix A). 4.1 G ENERATING ADVERSARIAL EXAMPLES Our attacks use a unified generation strategy: Each of our adversaries is defined by a differentiable function A, generating an adversarial input xadvfrom an input image xand some latent variables δ: xadv=A(x, δ). (3) To control the strength of our adversary, we introduce an Lpconstraint to the variables δ(using p=∞or p= 2). We define our perturbation sets in terms of these allowed ranges of optimization variables, i.e., for attack Awith epsilon constraint ε: SA,ε x={A(x, δ)| ∥δ∥p≤ε}. As is typical in the literature (Madry et al., 2017b), we re-frame the finding of adversarial examples in our perturbation set Section 4.1 as a continuous optimization problem, seeking δadvwhich solves: δadv= argmin δ:∥δ∥p≤ε{L(f(A(x, δ)), y)}, (4) and we then use the popular method of Projected Gradient Descent (PGD) (Madry et al., 2017b) to find an approximate solution to Equation (4). This formulation helps us ensure that our attacks are independent of auxiliary generative models, add minimal overhead compared to the popular PGD adversary (see Appendix C), are usable in a dataset-agnostic “plug-and-play” manner, can be used with existing optimization algorithms (see Figure 4a for behavior of attacks under optimization), come with a natural way of varying intensity through adjusting εparameter (see Figure 4b for behavior under varying ε), and have precisely defined perturbation sets which are not dependent on the solutions found to a relaxed constrained optimization problem. As discussed in Section 2, this is not the case for most existing attacks in the literature, prompting us to design our new attacks. 4.2 C ORE ATTACKS To provide fast evaluation, we select eight core attacks to form the focus of our evaluation for unforeseen robustness. We select the core set for diversity and effectiveness across model scale, leaving the other eleven attacks within our repository for the tuning of defense hyperperparameters and for a more complete evaluation of new techniques. The eight core attacks are: Wood. The wood attack is described in Figure 3. 50 20 40 60 80 100 Number of Steps01020304050607080Adv. AccuracyUA2 jpeg elastic wood glitch kaleidescope pixel snow gabor(a) Performance with increased optimization. low medium high Distortion05101520253035Adv. AccuracyUA2 jpeg elastic wood glitch kaleidescope pixel snow gabor (b) Performance as distortion size is varied Figure 4: Attack effectiveness increases with optimization pressure and distortion budget. We average performance against our core attacks across all our benchmarked models, demonstrating that our attacks respond to increased optimization pressure (Figure 4a). We further demonstrate the importance of the gradient-based nature by comparing random grid search to our gradient-based method in Appendix K. Furthermore, we demonstrate the ability for our attack stength to be customisable by showing that increasing distortion budget reduces model performance (Figure 4b). Glitch. Glitch simulates a common behavior in corrupted images of colored fuzziness. Glitch greys out the image, splitting it into horizontal bars, before independently shifting color channels within each of these bars. JPEG. TheJPEG compression algorithm functions by encoding small image patches using the discrete cosine transform, and then quantizing the results. The attack functions by optimizing L∞-constrained perturbations within the JPEG -encoded space of compressed images and then reverse-transforming to obtain the image in pixel space, using ideas from Shin & Song (2017) to make this differentiable. Gabor. Gabor spatially occludes the image with visually diverse Gabor noise (Lagae et al., 2009), optimizing the underlying sparse tensor which the Gabor kernels are applied to. Kaleidoscope. Kaleidoscope overlays randomly colored polygons onto the image, and then optimizes both the homogeneous color of the inside of the shape, and the darkness/lightness of the individual pixels on the shape’s border, up to an L∞constraint. Pixel. Pixel modifies an image so it appears to be of lower quality, by first splitting the image into m×m “pixels” and then and averaging the image color within each block. The optimization variables δthen control the level of pixelation, on a per-block bases. Elastic. Our only non-novel attack. Elastic is adapted from (Xiao et al., 2018), functioning by which warping the image by distortions x′= Flow( x, V), where V:{1, . . . , 224}2→ R2is a vector field on pixel space, and Flow sets the value of pixel (i, j)to the bilinearly interpolated original value at (i, j) + V(i, j). To make the attack suitable for high-resolution images, we modify the original attack by passing a gaussian kernel over V. Snow. Snow functions by optimising the intensity of individually snowflakes within an image, which are created by passing a convolutional filter over a sparsely populated tensor, and then optimising the non-zero entries in this tensor. 4.3 ImageNet -UA:A NEW BENCHMARK FOR UNFORESEEN ROBUSTNESS We introduce ImageNet -UA, a benchmark for evaluating the unforeseen robustness of image classifiers on the popular ImageNet dataset (Deng et al., 2009). We also develop CIFAR -10equivalent CIFAR -10-UAfor computationally efficient evaluation of defense strategies and attack methods. 6Model Clean Acc. L∞UA2 JPEG Elastic Wood Glitch Kal. Pixel Snow Gabor DINOv2 ViT-large Patch14 86.1 15.3 27.7 14.3 42.6 39.7 17.7 46.2 17.2 14.2 29.9 ConvNeXt-V2-large IN-1K+22K 87.3 0.0 19.2 0.0 39.1 34.4 21.4 16.1 15.5 4.0 23.1 ConvNeXt-V2-huge IN-1K 86.3 0.0 17.7 0.0 42.5 21.2 23.8 24.3 6.6 0.7 22.2 ConvNeXt-base, L∞(4/255) 76.1 58.0 22.3 39.0 23.8 47.9 12.9 2.5 9.7 30.2 12.8 ViT-base Patch16, L∞(4/255) 76.8 57.1 25.8 52.6 26.3 47.2 13.8 8.1 11.9 27.1 19.5 Swin-base IN-1K 85.3 0.0 15.2 0.0 31.4 24.6 16.2 6.0 6.9 4.3 32.0 ResNet-50 76.1 0.0 1.6 0.0 4.4 6.3 0.4 0.0 0.3 0.1 0.9 ResNet-50 + CutMix 78.6 0.5 6.1 0.2 17.9 15.5 2.5 0.1 6.7 3.0 2.7 ResNet-50, L∞(8/255) 54.5 38.9 10.0 6.9 11.8 23.9 14.4 0.7 5.2 15.6 1.2 ResNet-50, L2(5) 56.1 34.1 13.9 39.7 11.9 19.4 12.2 0.3 9.7 15.4 2.5 Table 2: ImageNet -UAbaselines We plot a range of models on the Pareto frontier on ImageNet -UA, as well as several baseline ResNet-50 models to compare between. We see a variety of techniques achieving high levels of robustness, demonstrating a rich space of possible interventions. The L∞column tracks robustness against a PGD L∞adversary with ε= 4/255. Numbers denote percentages. Training Train εClean Acc. UA2 Standard 76.1 1.6 L21 69.1 6.4 3 62.8 12.2 5 56.1 13.9 L∞2/255 69.1 6.4 4/255 63.9 7.9 8/255 54.5 10.0 Table 3: Lptraining. We train a range of ResNet-50 models against Lpadversaries onImageNet -UADataset Training Clean Acc. UA2 CIFAR -10L2, ε= 1 82.3 45.8 L∞, ε= 8/255 86.1 41.5 CIFAR -10-50ML2, ε= 0.5 95.2 51.2 L∞, ε= 4/255 92.4 51.5 Table 4: Lptraining on generated data. We see the effect of training when training WRN-28-10 networks on CIFAR -10-50M , a 1000x larger diffusion-model generated version of CIFAR -10(Wang et al., 2023) The unforeseen robustness achieved by a defense is quantified using a new metric, Unforeseen Adversarial Accuracy ( UA2), which measures the robustness of a given classifier facross a diverse range of unforeseen attacks. In line with Equation (2) we model the deployment-time population of adversaries Aas a categorical distribution over some finite set A, with a distortion level ϵAfor each adversary A∈A. Equation (2) then reduces to: UA2 :=1 |A|X A∈AAcc(A, ϵA, f) where Acc(A, εa, f)denotes the adversarial accuracy of classifier fagainst attack Aat distortion level εA. We select the population of adversaries to be the eight core adversaries from Section 4.2, setting A= {JPEG, Elastic, Wood, Glitch, Kaleidoscope, Pixel, Snow, Gabor }. We further divide our benchmark by picking three different distortion levels for each attack, leading to three different measures of unforeseen robustness: UA2 low,UA2 medandUA2 high(see Appendix A for specific ε values used within this work), and we focus on focus on UA2 medfor all of our reports, referring to this distortion level as simply UA2. As distortion levels increase, model performance decreases (Figure 4b). We perform a human study (Appendix I) to ensure UA2 medpreserves image semantics. 5 B ENCHMARKING FOR UNFORESEEN ADVERSARIAL ROBUSTNESS In this section, we evaluate a range of models on our standardized benchmarks ImageNet -UAand CIFAR -10-UA. We aim to present a set of directions for future work, by comparing a wide range of methods. We also hope to explore how the problem of unforeseen robustness different from existing robustness metrics. 7Training Strategy Train εClean Acc. UA2 PixMix 95.1 15.00 L∞ 4/255 89.3 37.3 L∞+ PixMix 4/255 91.4 45.1 L∞ 8/255 84.3 41.4 L∞+ PixMix 8/255 87.1 47.4 Table 5: PixMix and Lptraining. We compare UA2 performance on CIFAR-10 of models trained with PixMix and adversarial training. Combining PixMix with adversarial training results in large improvements inUA2, demonstrating the potential for novel methods to improve UA2. All numbers denote percentages, andL∞training was performed with the TRADES algorithm. 5.1 H OW DO EXISTING ROBUSTNESS MEASURES RELATE TO UNFORESEEN ROBUSTNESS ? We find the difference between existing popular metrics and UA2, highlighting the differential progress made possible by UA2: Worst-case and average-case robustness behave differently. We compare UA2 to the average-case robustness metric given by ImageNet-C (Hendrycks & Dietterich, 2019). As shown in Appendix G, we find that performance on this benchmark correlates with non-optimized versions of our attacks. However, the optimised versions of our attacks have model robustness profiles more similar to Lpadversaries. We see believe this is as after optimisation UA2 becomes a measure of worst case robustness, similar to Lprobust accuracy— contrasting with the average-case robustness considered in ImageNet-C. Lprobustness is correlated, but distinct, from unforeseen robustness. As shown in Appendix J, unforeseen robustness is correlated with Lprobustness. Our attacks also show similar properties to Lpcounterparts, such as the ability for black-box transfer (Appendix L). However, many models show susceptibility toLpadversaries while still performing well on UA2 (Section 4.1), and a range of strategies beat Lptraining baselines Section 5.2 . We conclude that UA2 is distinct from Lprobustness, and present UA2 as an improved progress measure when working towards real-world worst-case robustness. L2-based adversarial training outperforms L∞We see that Lpadversarial training increases the unforeseen robustness of tested models, with L2adversarial training providing the largest increase in UA2 over standard training (1.6% →13.9%), beating models which are trained against L∞adversaries (1.6% →10.0%). We present L2trained models as a strong baseline for unforeseen robustness, noting that the discrepancy between L∞andL2training is particularly relevant as L∞is the most ubiquitous measure of adversarial robustness in the literature. 5.2 H OW CAN WE IMPROVE UNFORESEEN ROBUSTNESS ? We find several promising directions that improve over Lptraining, and suggest that the community should focus more on techniques which we demonstrate to have better generalization properties: Combining image augmentations and L∞training. We combine PixMix and L∞training, finding that this greatly improves unforeseen robustness over either approach alone ( 37.3→45.1, see Section 5.1). This is a novel training strategy which beats strong baselines by combining two distinct robustness techniques ( Lpadversarial training and data augmentation). The surprising effectiveness of this simple method suggests that unforeseen robustness may foster the development of new methods. Multi-attack robustness. To evaluate how existing work on robustness to a union of Lpballs may improve unforeseen robustness, we use CIFAR -10-UAto evaluate a strong multi-attack robustness baseline by (Madaan et al., 2021b), which trains a Meta Noise Generator (MNG) that learns the optimal training perturbations to achieve robustness to a union of Lpadversaries. For WRN-28-10 models on CIFAR -10-UA, we see a large increase in unforeseen robustness compared to the best Lpbaseline ( 21.4%→51.1%, full results in Appendix H ), leaving scaling of such methods to full ImageNet -UAfor future work. Bounding perturbations with perceptual distance. We evaluate the UA2 of models trained with Perceptual Adversarial Training (PAT) (Laidlaw et al., 2020). PAT functions by training a model against an adversary bounded by an estimate of the human perceptual distance, computing the estimate by using the hidden states of an image classifier. For computational reasons we train and evaluate ResNet-50s on a 100-image 8Training Clean Acc. UA2 Standard 76.1 1.0 Moex 79.1 6.0 CutMix 78.6 6.0 Deepaugment + Augmix 75.8 1.8 Table 6: Effects of data augmentation on UA2.We evaluate the UA2 of a range of dataaugmented ResNet50 models.Model Clean Acc. UA2 ConvNeXt-V2-28.6M 83.0 9.8 ConvNeXt-V1-28M 82.1 5.1 ConvNeXt-V2-89M 84.9 14.9 ConvNeXt-V1-89M 83.8 9.7 ConvNeXt-V2-198M 85.8 19.1 ConvNeXt-V1-198M 84.3 10.6 Table 7: Effects of pretraining and regularization on UA2.We compare the unforeseen robustness of ConvNext-V1 and ConvNext-V2 models of equivalent sizes, finding ConvNextV2 models improve over their V1 versions. subset of ImageNet -UA, where this technique outperforms the best Lptrained baselines ( 22.6→26.2, full results in Appendix H). Regularizing high-level features. We evaluate Variational Regularization (VR) (Dai et al., 2022), which adds a penalty term to the loss function for variance in higher level features. We find that the largest gains in unforeseen robustness come from combining VR with PAT, improving over standard PAT ( 26.2→29.5, on a 100 class subset of ImageNet -UA, full results in Appendix H). 5.3 H OW HAS PROGRESS ON CV BENCHMARKS TRACKED UNFORESEEN ROBUSTNESS ? Computer vision progress has partially tracked unforeseen robustness. Comparing the UA2 of ResNet50 to ConvNeXt-V2-huge ( 1%→19.1%UA2) demonstrates the effects of almost a decade of CV advances, including self-supervised pretraining, hardware improvements, data augmentations, architectural changes and new regularization techniques. More generally, we find a range of modern architectures and training strategies doing well (see Section 4.3, full results in Figure 7). This is gives a positive view of how progress on standard CV benchmarks has tracked underlying robustness metrics, contrasting with classical Lpadversarial robustness where standard training techniques have little effect (Madry et al., 2017a). Scale, data augmentation and pretraining improve robustness. We do a more careful analysis of how three of the most effective CV techniques have improved robustness. As shown in Section 5.2, we find that data augmentation improves on unforeseen robustness, even in cases where they reduce standard accuracy. We also compare the performance of ConvNeXt-V1 and ConvNeXt-V2 models, which differ through the introduction of self-supervised pretraining and a new normalization layer. When controlling for model capacity these methods demonstrate large increase unforeseen robustness (see Section 5.2). We also note that DINOv2, the best performing model on our benchmark, is the product of self-supervised pretraining at a large scale. 6 C ONCLUSION In this paper, we introduced a new benchmark for unforeseen adversaries (ImageNet -UA) laying groundwork for future research in improving real world adversarial robustness. We provide nineteen (eighteen novel) nonLpattacks as part of our repository, using these to construct a new metric UA2 (Unforeseen Adversarial Accuracy). We then make use use this standardized benchmark to evaluate classical Lptraining techniques, showing that the common practice of L∞training and evaluation may be misleading, as L2 shows higher unforeseen robustness. We additionally demonstrate that a variety of interventions outside ofLpadversarial training can improve unforeseen robustness, both through existing techniques in the CV literature and through specialised training strategies. We hope that the ImageNet -UArobustness framework will help guide adversarial robustness research, such that we continue making meaningful progress towards making machine learning safer for use in real-world systems. 