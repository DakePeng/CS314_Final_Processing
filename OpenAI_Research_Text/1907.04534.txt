arXiv:1907.04534v1  [cs.CY]  10 Jul 2019The Role of Cooperation in Responsible AI Development Amanda Askell∗ OpenAI amanda@openai.comMiles Brundage OpenAI miles@openai.comGillian Hadﬁeld OpenAI gillian@openai.com Abstract In this paper, we argue that competitive pressures could inc entivize AI companies to underinvest in ensuring their systems are safe, secure, a nd have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to impro ve the prospects for industry cooperation on the responsible development of AI. Introduction Machine learning (ML) is used to develop increasingly capab le systems targeted at tasks like voice recognition, fraud detection, and the automation of vehicl es. These systems are sometimes referred to as narrow artiﬁcial intelligence (AI) systems. Some companie s are also using machine learning techniques to try to develop more general systems that can learn effecti vely across a variety of domains rather than in a single target domain. Although there is a great deal of un certainty about the development path of future AI systems—whether they will remain specialized or g row increasingly general, for example— many agree that if the current rate of progress in these domai ns continues then it is likely that advanced artiﬁcial intelligence systems will have an increasingly l arge impact on society. This paper focuses on the private development of AI systems t hat could have signiﬁcant expected social or economic impact, and the incentives AI companies hav e to develop these systems responsibly. Responsible development involves ensuring that AI systems are safe, secure, and socially beneﬁcial. In most industries, private companies have incentives to in vest in developing their products responsibly. These include market incentives, liability laws, and regul ation. We argue that AI companies have the same incentives to develop AI systems responsibly, althoug h they appear to be weaker than they are in other industries. Competition between AI companies could d ecrease the incentives of each company to develop responsibly by increasing their incentives to deve lop faster. As a result, if AI companies would prefer to develop AI systems with risk levels that are closer to what is socially optimal—as we believe many do—responsible AI development can be seen as a collecti ve action problem.1 We identify ﬁve key factors that make it more likely that comp anies will be able to overcome this collective action problem and cooperate—develop AI responsibly w ith the understanding that others will do likewise. These factors are: high trust between developers (High Trust ), high shared gains from mutual cooperation ( Shared Upside ), limited exposure to potential losses in the event of unrec iprocated cooperation ( Low Exposure ), limited gains from not reciprocating the cooperation of o thers ( Low Advantage ), and high shared losses from mutual defection ( Shared Downside ). ∗Primary/corresponding author. 1AI research companies increasingly have teams dedicated to the safe and ethical development of technology and many large technology companies participate in volunta ry efforts to articulate and establish principles and guidelines, and in some cases call for government regulatio n, to address AI-related risks. Preprint. Work in progress.Using these ﬁve factors, we identify four strategies that AI companies and other relevant parties could use to increase the prospects for cooperation around respon sible AI development. These include correcting harmful misconceptions about AI development, collabor ating on shared research and engineering challenges, opening up more aspects of AI development to app ropriate oversight, and incentivizing greater adherence to ethical and safety standards. This lis t is not intended to be exhaustive, but to show that it is possible to take useful steps towards more respons ible AI development. The paper is composed of three sections. In section 1, we outl ine responsible AI development and its associated costs and beneﬁts. In section 2, we show that comp etitive pressures can generate incentives for AI companies to invest less in responsible development t han they would in the absence of competition, and outline the ﬁve factors that can help solve such c ollective action problems. In section 3, we outline the strategies that can help companies realize the g ains from cooperation. We close with some questions for further research. 1 The beneﬁts and costs of responsible AI development AI systems have the ability to harm or create value for the com panies that develop them, the people that use them, and members of the public who are affected by their u se. In order to have high expected value for users and society, AI systems must be safe—they must reli ably work as intended—and secure— they must have limited potential for misuse or subversion. A I systems should also not introduce what Zwetsloot and Dafoe (2019) call “structural risks”, which i nvolve shaping the broader environment in subtle but harmful ways.2The greater the harm that can result from safety failures, mi suse, or structural risks, the more important it is that the system is safe and ben eﬁcial in a wide range of possible conditions (Dunn, 2003). This requires the responsible development of AI. 1.1 What is responsible AI development? AI systems are increasingly used to accomplish a wide range o f tasks, some of which are critical to users’ health and wellbeing. As the range of such tasks grows, the po tential for accidents and misuse also grows, raising serious safety and security concerns (Amode i, Olah, et al., 2016; Brundage, Avin, et al., 2018). Harmful scenarios associated with insufﬁciently ca utious AI development have already surfaced with, for example, biases learned from large datasets disto rting decisions in credit markets and the criminal justice system, facial recognition technologies disrupting established expectations of privacy and autonomy, and auto-pilot functions in some automobiles causing new types of driving risk (while reducing others). Longer term, larger scale scenarios incl ude dangers such as inadvertent escalation of military conﬂict involving autonomous weapon systems or wi despread job displacement. Responsible AI development involves taking steps to ensure that AI systems have an acceptably low risk of harming their users or society and, ideally, to increase t heir likelihood of being socially beneﬁcial. This involves testing the safety and security of systems dur ing development, evaluating the potential social impact of the systems prior to release, being willing to abandon research projects that fail to meet a high bar of safety, and being willing to delay the release of a system until it has been established that it does not pose a risk to consumers or the public. Responsibl e AI development comes in degrees but it will be useful to treat it as a binary concept for the purpos es of this paper. We will say that an AI system has been developed responsibly if the risks of it caus ing harms are at levels most people would consider tolerable, taking into account their severity, an d that the amount of evidence grounding these risk estimates would also be considered acceptable.3 Responsible AI development involves work on safety, securi ty, and the structural risks associated with AI systems. Work on the safety of AI aims to mitigate accident risks (Amodei, Olah, et al., 2016) and ensure that AI systems function as intended (Ortega, Maini, et al., 2018) and behave in ways that people want (Irving et al., 2018). Work on the security of AI aims to p revent AI systems from being attacked, 2Zwetsloot and Dafoe (2019) argue that the “the accident-mis use dichotomy obscures how technologies, including AI, often create risk by shaping the environment and incentives”. We restrict accident risks to technical accidents and misuse risks to direct misapplications of a sy stem. ‘Structural risks’, as we use the term, are intended to capture the broader impact of AI systems on society and soc ial institutions. 3This will generally mean if an AI system is developed respons ibly, the risk of irreversible catastrophic harm from that system—whether through accident, misuse, or nega tive social impact—must be very low. This is consistent with what Sunstein (2005) calls the ‘Irreversible Harm Precautionary Principle’. 2co-opted, or misused by bad actors (Brundage, Avin, et al., 2 018).4Work evaluating the structural impact of AI aims to identify and mitigate both the immediate an d long term structural risks that AI systems pose to society: risks that don’t quite ﬁt under narrow deﬁni tions of accident and misuse. These include joblesness, military conﬂict, and threats to political and social institutions.5 1.2 The cost of responsible AI development It is likely that responsible development will come at some c ost to companies, and this cost may not be recouped in the long-term via increased sales or the avoidan ce of litigation. In order to build AI systems responsibly, companies will likely need to invest resource s into data collection and curation, system testing, research into the possible social impacts of their system, and, in some cases, technical research to guarantee that the system is reliably safe. In general, th e safer that a company wants a product to be, the more constraints there are on the kind of product the comp any can build and the more resources it will need to invest in research and testing during and after i ts development. If the additional resources invested in ensuring that an AI s ystem is safe and beneﬁcial could have been put towards developing an AI system with fewer constraints m ore quickly, we should expect responsible AI development to require more time and money than incautiou s AI development. This means that responsible development is particularly costly to compani es if the value of being the ﬁrst to develop and deploy a given type of AI system is high (even if the ﬁrst sy stem developed and deployed is not demonstrably safe and beneﬁcial). There are generally several advantages that are conferred o n the ﬁrst company to develop a given technology (Lieberman and Montgomery, 1988). If innovations ca n be patented or kept secret, the company can gain a larger share of the market by continuing to produce a superior product and by creating switching costs for users. Being a ﬁrst-mover also allows the compa ny to acquire scarce resources ahead of competitors. If hardware, data, or research talent become s carce, for example, then gaining access to them early confers an advantage.6And if late movers are not able to catch up quickly then ﬁrst-m over advantages will be greater. In the context of AI development , having a lead in the development of a certain class of AI systems could confer a ﬁrst mover advant age. This effect would be especially pronounced in the case of discontinuous changes in AI capabi lities7, but such a discontinuity is not necessary in order for a ﬁrst mover advantage to occur. Responsible development may therefore be costly both in ter ms of immediate resources required, and in the potential loss of a ﬁrst-mover advantage. Other poten tial costs of responsible AI development include performance costs and a loss of revenue from not buil ding certain lucrative AI systems on the grounds of safety, security, or impact evaluation. An examp le of a performance cost is imposing a limit on the speed that self-driving vehicles can travel in order t o make them safer. An example of revenue loss is refusing to build a certain kind of facial recognitio n system because it may undermine basic civil liberties (Smith, 2018b). AI companies may not strongly value being the ﬁrst to develop a particular AI system because ﬁrstmover advantages do not always exist. Indeed, there are ofte n advantages to entering a market after the front-runner. These include being able to free-ride on t he R&D of the front-runner, to act on more information about the relevant market, to act under more reg ulatory certainty, and having more ﬂexible assets and structures that let a company respond more ef fectively to changes in the environment (Gilbert and Birnbaum-More, 1996). These can outweigh the a dvantages of being the ﬁrst to enter that same market. And it has been argued that late mover advantage s often do outweigh ﬁrst mover advantages (Markides and Geroski, 2004; Querbes and Frenken, 201 7). We therefore acknowledge that the assumption that there will be a ﬁrst-mover advantage in AI de velopment may not be true. If a ﬁrst-mover advantage in AI is weak or non-existent then companies are le ss likely to engage in a race to the bottom on safety since speed is of lower value. Instead of offering p redictions, this paper should be thought of as an analysis of more pessimistic scenarios that involve at least a moderate ﬁrst mover advantage. 4Mitigating misuse risks is sometimes included under AI safe ty, broadly construed (Christiano, 2016) 5The literature on the societal impact of AI is vast. See Cummi ngs (2017) on AI and warfare, for example. For a broader overview see Dafoe (2018). 6As Klepper (1996) notes, larger ﬁrst-movers can also spread their prior investment in R&D over a larger number of applications. 7The possibility of discontinuous progress in AI is discusse d by Good (1966), Chalmers (2009), Yudkowsky (2013), Shanahan (2015) and Bostrom (2017b). AI Impacts (20 18) provide a critical overview of the arguments for discontinuity and Christiano (2018) presents arguments ag ainst the claim. 3Much of the discussion of AI development races assumes that t hey have a deﬁnitive endpoint. Although some have hypothesized that if AI progress is discontinuous or sufﬁciently rapid then it could essentially have a deﬁnitive endpoint, the case for this remains s peculative.8It is therefore important to note that AI development may take the form of a perpetual R&D r ace: a race to stay technologically ahead of competitors rather than a race to reach some particu lar technological endpoint (Aoki, 1991; Breitmoser et al., 2010). If this is the case then AI companie s would still have an incentive to speed up development in order to stay ahead of others, especially i f the gap between companies was small. The present analysis is applicable to perpetual races in whi ch there is at least a moderate ﬁrst mover advantage, several companies are competing to stay ahead, a nd leadership is not yet entrenched.9 1.3 The beneﬁts of responsible AI development In the law and economics literature on product safety, it is g enerally accepted that market forces create incentives for companies to invest in making their products safe (Oi et al., 1973). Suppose that companies have accurate information about how safe the products t hey are developing are and that consumers have access to accurate information about how safe a company ’s product is, either prior to release or by observing the harms caused by a product after it is relea sed (Ben-Shahar, 1998; Chen and Hua, 2017).10If consumers have a preference for safer products and respon d rationally to this preference, they will not buy products that are insufﬁciently safe, or wi ll pay less for them than for safer alternatives (Polinsky and Shavell, 2009). Releasing unsafe products wi ll also result in a costly loss of reputation for companies (Daughety and Reinganum, 1995).11Finally, releasing unsafe products could result in burdensome regulation of the industry or in litigation cost s. Therefore companies that are concerned about a sufﬁciently long time-horizon involving repeated i nteraction with customers, regulators, and other stakeholders that incentivize safety should interna lize the value of responsible development. Market forces alone may not always incentivize companies to invest the appropriate amount into ensuring their products are safe. If consumers cannot get acce ss to information about the safety of a product—how likely safety failures are or how costly they ar e—then companies have an incentive to under-invest in safety. And if companies have inaccurate in formation about the safety of the products they are developing, they will not invest in safety to the deg ree demanded by consumers. Finally, poor corporate governance can result in suboptimal decisions ab out risk (Cai et al., 2010). Product liability law and safety regulation are intended to correct such marke t failures by providing consumers with information about products, incentivizing companies to invest m ore in safety, and compensating consumers that are harmed by product safety failures (Hylton, 2012; La ndes and Posner, 1985).12 We may expect companies to under-invest in safety if the cost s to consumers don’t result in commensurate costs for the company; either via a reduction in reven ue, reputation loss, ﬁnes from regulators, or successful litigation by consumers. Safety failures can also affect those who do not consume the product, however. Consider a 2018 recall of over 8,000 V olks wagen vehicles potentially affected by a brake caliper issue that could result in increased stopp ing distances or loss of vehicle control (Consumer Reports, 2018). A safety failure resulting from t his could harm not only the vehicle’s occupants but also pedestrians and other drivers.13Harms that safety failures inﬂict on non-consumers are negative externalities, and beneﬁts that safer products pr oduce for non-consumers are positive externalities. We should anticipate companies under-investing in r educing negative externalities and increasing 8If AI development is extremely rapid then gaps between each c ompany would likely increase over time. This means that a company that is ahead of others may at some point b e ahead of them by a great deal in strategically important areas, and could use this to undermine their compe titors (Bostrom, 2017b, pp. 91-104) 9Breitmoser et al. (2010) note that perpetual R&D races tend t o collapse into leadership monopolies. The larger the gap between the front-runner and the company in second pl ace in a perpetual race, the less of an incentive the front-runner has to trade safety for speed. 10See Daughety et al. (2013), who make similar assumptions in t heir idealized model of markets. 11Rhee and Haunschild (2006) provide evidence that the relati onship between safety failures and reputation loss may be more complex than this, however. 12See Stiglitz (2009) on government regulation as a response t o market failures or inefﬁciencies, but note that actual motivations for government regulation are typically m ore complicated (Henson and Caswell, 1999). Calabresi (1970) provides comprehensive overview of of the role of law in the minimization of costs from safety failures. The relationship between product liability law and safety regu lations—in particular, whether it is efﬁcient to use them jointly—is a matter of some debate (Shavell, 1984; Kolstad e t al., 1990). 13There is currently uncertainty about who should be held liab le for the harms that the safety failures of autonomous systems inﬂict on the public (Schellekens, 2015; T he Atlantic, 2018). 4positive externalities relative to their social value, sin ce the costs and beneﬁts this produces for society don’t result in commensurate costs and beneﬁts for the compa ny (Dahlman, 1979). To give a concrete example, consider facial recognition tec hnology. Microsoft have argued that this technology could be used in ways that many would consider har mful: to violate individuals’ privacy or suppress their political speech, for example (Smith, 201 8a,b). Even if companies would prefer to build facial recognition systems that cannot be misused, ei ther to avoid causing harm or to avoid the reputation costs of this harm, the cost of developing safegu ards may not outweigh their beneﬁts if companies cannot be held liable for these harms and there is n o regulation preventing misuse. For this reason, Microsoft has called for regulation that would requ ire that companies invest in measures that reduce the risks from facial recognition technology, and th at could also mitigate potential misuse of the technology by commercial entities or by governments (Smith , 2018a). The discussion thus far treats companies as though they were motivated only by proﬁt, i.e. they only care about things like reputation and product safety insofa r as they are a means to make more proﬁt or avoid losses. This view is common in the literature on corpor ate social responsibility (Campbell, 2007; Devinney, 2009) but it is clearly an abstraction. Companies are run by, invested in, and composed of humans that care about the impact their products will have on the world and on other people. Employees at technology companies have already shown that they care a g reat deal about the social implications of the systems they are building (Minsberg, 2019). The things that motivate AI companies other than proﬁts, suc h as beneﬁting people rather than harming them, will generally push even more in favor of responsible d evelopment: they will rarely push against it. Assuming that companies are motivated solely by proﬁt there fore lets us analyze a kind of ‘worst case scenario’ for responsible development. We will therefore o ften treat companies as though they were driven solely by proﬁt, even though we do not ﬁnd this plausib le. It is important that the reader bear this in mind, since treating companies as proﬁt-driven enti ties can be self-fulﬁlling, and can therefore contribute to the very problems we are attempting to solve. 1.4 Are existing incentives for responsible AI development enough? If markets are functioning well and companies and consumers have perfect information about the expected harm of a product, companies should invest the social ly optimal amount into product safety (Daughety et al., 2018). In real-world scenarios in which ma rkets may not function perfectly and information asymmetries exist, incentives for companies to i nvest sufﬁciently in product safety typically come from three sources: market forces, liability law, and i ndustry or government regulation.14These three sources of incentives may not provide strong enough in centives for AI companies to engage in responsible AI development, however. We will brieﬂy survey some reasons for this. 1.4.1 Limited consumer information Consumers of AI systems include individuals, private compa nies, and public institutions. Although different consumers will have access to different levels of information about AI systems, information about the expected harm of AI systems is likely to be quite lim ited on average. As cutting-edge AI systems become more complex, it will be difﬁcult for consume rs not involved in the development of those systems to get accurate information about how safe the systems are. Consumers cannot directly evaluate the safety of aviation software, for example, and w ill face similar difﬁculties when it comes to directly evaluating the safety of complex machine learni ng models. This is compounded by the fact that it is notoriously difﬁcult to explain the decisions mad e by neural networks (Doshi-Velez and Kim, 2017; Olah et al., 2018). If consumers cannot assess how risk y a given AI system is, they cannot adjust their willingness to pay for it accordingly. They are also le ss able to identify and exert pressure on AI companies that are investing too little in safety (Anton et a l., 2004). Consumers could get information about how safe an AI system i s by tracking safety failures after its release, but such a ‘wait and see’ strategy could leave both c onsumers and the public vulnerable to harmful safety failures. This is of particular concern if th ose safety failures could be irreversible or catastrophic. And the probability of irreversible or catas trophic safety failures is likely to increase as AI 14Other mechanisms include no fault liability systems like ma ndatory insurance and increasing the information available to consumers (Cornell et al., 1976). 5systems become more capable and general, since more advance d systems are more likely to be relied upon across a wider range of domains and in domains where fail ures are more harmful.15 1.4.2 Limited company and regulator information Measuring the safety, security, and social impact of AI syst ems may turn out to be extremely difﬁcult even for those who understand the technical details of the sy stem. Neural networks are difﬁcult to interpret and as such, failures may be difﬁcult to predict. I f AI companies are over-conﬁdent that their system is not risky, they may under-invest in important risk -reducing measures during development or release a system that causes unintended harm. If regulators cannot assess how risky a given AI system is, th ey may be overly stringent or overly liberal when using regulatory controls (Shavell, 1984). The abilit y to get accurate information about AI systems therefore seems to be crucial for ex ante safety measures.16 Our current capacities to identify and measure the expected harms of particular AI systems are extremely limited. We still do not fully understand the decisions made by complex machine learning models (Olah et al., 2018; Hohman et al., 2018) and the high-dimensi onality of the inputs to AI systems makes it such that exhaustive enumeration of all possible inputs a nd outputs is typically infeasible. There may therefore be little consensus about whether a particula r system is likely to be unsafe, unsecure, or socially harmful at present. Given this, it is likely that ad ditional capacity will need to be invested by companies or regulators or both in order to decrease these in formation asymmetries. 1.4.3 Negative externalities from AI Harms caused by AI systems are likely to affect third parties . Biases in algorithmic pre-trial risk assessment are more likely to harm those accused of crimes than thos e that purchase the tools,17those that beneﬁt from AI automation may be quite distinct from the peop le who are displaced by automation,18 and a major AI disaster—such as an AI system with a faulty rewa rd function19being integrated into a critical system—could affect a large portion of society th at is distinct from the AI company and its consumers. AI also has the potential to be a general purpose t echnology—a technology that radically affects many sectors of the economy—and if this is the case we should expect its impact to be systemic (Brynjolfsson et al., 2018; Cockburn et al., 2018). The harms from AI systems may also be difﬁcult to internalize . For example, the social harms that result from an increased use of AI systems—such as reduced tr ust in online sources—could be complex and diffuse, and it may be difﬁcult to hold any one company str ictly liable for them. If the harm is sufﬁciently large, it may also be too large for a company or in surer to cover all losses (see note 15). Finally, AI systems could create negative externalities fo r future generations that are not in a position to penalize companies or prevent them from occurring (Lazear, 1983). We should expect AI companies to under-invest in measures that could prevent these kinds of n egative externalities.20 1.4.4 The difﬁculty of constructing effective AI regulatio n There is currently little in the way of AI-targeted regulati on, including government regulation, industry self-regulation, international standards, and clarity on how existing laws will be applied to AI (see note 15Such safety failures could occur if AI systems have some crit ical function like controlling national power grids or nuclear weapons systems, or if they can be used to undermin e these systems. The more consequential a given technology is, the higher the potential cost of releasing an insufﬁciently safe version of that technology to both the company and to society. But is worth noting that if the expect ed cost of catastrophic safety failures is capped by a company’s ability to pay then we might expect companies to un der-weight these tail-end risks. For a taxonomy of AI risks, see Yampolskiy (2015). 16Leike et al. (2017) introduce simple environments for evalu ating the safety of AI agents, and note that future versions of these environments could be used to benchmark th e safety performance of AI agents. 17See Tsukayama and Williams (2018) on how bias in ML systems co uld harm those in the California criminal justice system. Pleiss et al. (2017) demonstrates the uniqu e difﬁculties of designing bias-free ML systems. 18Segal (2018) notes that the jobs that have declined as a resul t of automation so far are intermediate-skill jobs like farming. Although it displaces some workers, auto mation has positive effects like increased productivity (Acemoglu and Restrepo, 2018). The overall effect that AI au tomation will have on the labor force is unclear. 19See Krakovna (2018) and Clark and Amodei (2016) for examples of faulty reward functions in ML systems. 20Safety failures that affect a large portion of the populatio n will not be treated as externalities by companies if they harm the company or its consumers, though they could sti ll be given insufﬁcient weight. 613). Well-designed regulatory mechanisms can incentivize companies to invest appropriate resources in safety, security, and impact evaluation when market failur es or coordination failures have weakened the other incentives to do so. Poorly-designed regulation can b e harmful rather than helpful, however. Such regulation can discourage innovation (Heyes, 2009) and eve n increase risks to the public (Latin, 1988). AI regulation seems particularly tricky to get right, as it w ould require a detailed understanding of the technology on the part of regulators.21The fact that private AI companies can generally relocate ea sily also means that any attempt to regulate AI nationally could r esult in international regulatory competition rather than an increase in responsible development.22Regulation that is reactive and slow may also be insufﬁcient to deal with the challenges raised by AI systems . AI systems can operate much faster than humans, which can lead to what Johnson et al. (2013) call ‘ult rafast extreme events’ (UEEs) such as ﬂash crashes caused by algorithmic trading.23 1.4.5 The potential for rapid AI development Some have hypothesized that progress in AI development will be discontinuous (see note 7). On this view, there are some types of AI systems—typically advanced ‘general’ AI systems that are capable of learning effectively across a wide variety of domains—th at, if developed, would represent a sudden shift from everything that came before them, and could produ ce the equivalent of many years of prior progress on some relevant metric.24If AI progress is discontinuous then developing an AI system that constitutes a sudden leap forward could give a company a larg e advantage over others, since the next best system would be years behind it in terms of prior progres s in the ﬁeld.25Consider the advantage that a company today would gain if they managed to develop som ething over a decade ahead of current systems used for cyber offense and defense, for example. If progress in AI development is discontinuous then market f orces and liability law may do little to encourage safe development.26The value of developing a system that gives a company a huge ad vantage— that could be used to undermine competition or seize resourc es, for example—would be largely divorced from the process of getting market feedback. And a company ca n only be held liable for accidents if these accidents are not catastrophic and the existing legal framework can both keep up with the rapidity of technological progress and enforce judgments against co mpanies. Therefore if AI progress is discontinuous, ex ante safety measures like industry self-regula tion or international oversight may be more effective than ex post safety measures like market response and liability. 1.5 Summary Incentives to develop safe products generally come from the market, liability laws, and regulation (Rubin, 2011), as well as factors that motivate AI companies beside proﬁts, such as a general desire to avoid doing harm. For AI companies, the proﬁt motive to dev elop AI responsibly is likely to come from the additional revenue generated by AI systems that are more valuable to consumers, the avoidance 21Hadﬁeld (2017) and Hadﬁeld and Clark (2019) outline a regula tory framework for AI that could overcome barriers like information asymmetries and slow response ti mes: key problems for the regulation of new technology. 22Esty and Geradin (2001) offer an overview of different persp ectives on regulatory competition, while Genschel and Plumper (1997) note that regulatory competiti on and international co-operation can actually increase levels of regulation. Erdélyi and Goldsmith (2018) argue th at an international AI regulatory agency should be established, but on the grounds that AI has externalities th at cross national boundaries. 23For more on this problem, see Muehlhauser and Hibbard (2014) . Anticipating, preventing, and responding to catastrophic AI-caused UEEs may present a key challenge in A I safety and policy. 24See Ehrnberg (1995) on deﬁnitions of technological discont inuities. The AI discontinuity hypothesis should not be confused with the claim that there will be rapid AI deve lopment in the future—progress in AI development could be continuous but extremely rapid, e.g. hyperbolic (C hristiano, 2017)—but that there will be a system that represents a sudden leap forward in AI capabilities. It may be possible to achieve a decisive advantage over competitors if progress is rapid but not discontinuous. 25Bostrom (2017b) claims that such an AI could give a company a ‘ decisive strategic advantage’: ‘a level of technological and other advantages sufﬁcient to enable it t o achieve complete world domination’ (p.96, ibid.). But the concerns we raise here apply even if the advantage is extr eme but not decisive in this sense. 26This may be true even if progress in AI development is continu ous but rapid. Even if no single company has a profound advantage over others, mechanisms like regulatio n and liability could be too slow to catch up with the rate of technological progress. It is worth noting that if AI prog ress takes this shape then responsible AI development may be more like a one-shot game than an iterated game, which r educes developers’ incentives to cooperate on responsible development for reasons that we discuss in the n ext section. 7of reputational harm from safety failures, the avoidance of widespread harms caused by AI systems (see note 20), and the avoidance of tort litigation or regulatory penalties. A key factor that can inﬂuence the cost-beneﬁt ratio of respo nsible AI development that we have not discussed, however, is the competitive environment in whic h the AI systems in question are being developed. In the next section we will explore the impact tha t competition between AI companies can have on the incentives that each company has to invest or fail to invest in responsible development. 2 The need for collective action on responsible AI developme nt We have argued that safer, more secure, and more socially val uable AI systems will tend to have a higher market value, be less likely to cause costly accidents that t he company is held liable for, and so on. This means that if a company is guaranteed to be the ﬁrst to develop a system of this type, we can expect that they will invest resources to ensure that their system i s safe, secure, and socially beneﬁcial to the extent that this is incentivized by regulators, liability l aw, and market forces. This means the more that positive and negative externalities of AI systems have been internalized via these mechanisms, the more that companies can expect to invest in responsible developm ent.27 In this section we will argue that, even with these incentive s in place, competitive pressures can cause AI companies to invest less in responsible development than they otherwise would. Responsible AI development can therefore take the form of a collective acti on problem. We then identify and discuss ﬁve key factors that improve the prospects for cooperation betw een AI companies that could ﬁnd themselves in a collective action problem over responsible developmen t. 2.1 How competitive pressures can lead to collective action problems To see how the competitive environment could affect investm ent in responsible development, suppose that several AI companies are working on a similar type of sys tem. If there is a large degree of substitutability between the inputs of different aspects of devel opment, we should not expect AI companies to invest in responsible development beyond the point at whi ch the expected marginal return is lower than the expected marginal return from investing in other ar eas of development. Suppose each company places less value on coming second than on coming ﬁrst, less v alue in coming third than in coming second, and so on. These companies will likely engage in a techno logical race: a competition to develop a technology in which the largest reward goes to the ﬁrst compa ny (Grossman and Shapiro, 1985).28The resulting dynamics may be similar to those we would expect to see in patent races between ﬁrms.29 There are various strategies companies could use in a “winne r takes more” race: they could try to develop and maintain a strong technical lead or they could tr y to to maintain a close position behind the technical leader, for example.30For now, we will assume that the best strategy involves tryin g to develop and maintain a strong technical lead throughout the race. Since speed is more valuable when racing against others, we s hould expect investment into responsible development to be lower when companies are racing against ea ch other.31Armstrong et al. (2016) point out that in an AI development race, responsible develo pment could be prey to a “race to the bottom” dynamic. Consider what happens if one company decid es to increase their development speed by decreasing their investment in safety, security, and imp act evaluation. This increases their expected ranking in the race and decreases the expected ranking of oth ers in the race. A decrease in expected 27If the ﬁrst company could prevent future competitors from en tering the market (i.e. the ﬁrst company could expect to be the only company), it is likely this would reduce but not eliminate market incentives to invest in responsible development (Sheshinski, 1976). 28As we noted in the previous section, this is a non-trivial ass umption that will not hold in all cases. 29Patent races have positive effects on innovation, though at the cost of duplicating efforts (Judd et al., 2012). 30The best strategy may depend on the competitive environment . Dasgupta and Stiglitz (1980) argue that monopolist companies will attempt to outspend their rivals on R&D to prevent a duopoly, while Doraszelski (2003) shows that there are conditions in which companies that are b ehind will invest to catch up. 31How much lower will depend on various features of the race, su ch as how close it is and the value placed on each position. Note that this argument assumes that investm ents with even worse expected marginal returns have already been cut. It also assumes that investments in respon sible development contribute less to development speed than other available investments: not that they contribute nothing to development speed. 8ranking gives competing AI companies an incentive to decrea se their own investment in these areas in order to maintain or increase their expected ranking in the r ace.32 We might ask why racing to the bottom on product safety is not u biquitous in other industries in which decreasing time-to-market is valuable, such as in the pharm aceutical industry.33The most plausible explanation of this difference is that the cost of safety fai lures has been internalized to a greater extent in more established industries via external regulation, self -regulation, liability, and market forces. These mechanisms can jointly raise the “bottom” on product safety to a level that is generally considered acceptable by regulators and consumers.34 In a race to the bottom on safety, competing AI companies coul d reduce their investment in responsible development to the point that winning the technology race—s uccessfully developing the system they are racing to develop before others—is barely of net positiv e value for the winner even after all the ﬁrst-mover advantages, including positive reputational e ffects, the ability to capture resources like data, hardware and talent, and creating switching costs for consu mers, have been taken into account.35 2.2 When competition has negative rather than positive effe cts The race to the bottom on safety described above is a collecti ve action problem: a situation in which all agents would be better off if they could all cooperate wit h one another, but each agent believes it is in their interest to defect rather than cooperate.36As Heckathorn (1989, p. 78) states, “the inclinations 