arXiv:1907.04534v1  [cs.CY]  10 Jul 2019The Role of Cooperation in
Responsible AI Development
Amanda Askell∗
OpenAI
amanda@openai.comMiles Brundage
OpenAI
miles@openai.comGillian Hadﬁeld
OpenAI
gillian@openai.com
Abstract
In this paper, we argue that competitive pressures could inc entivize AI companies
to underinvest in ensuring their systems are safe, secure, a nd have a positive social
impact. Ensuring that AI systems are developed responsibly may therefore require
preventing and solving collective action problems between companies. We note that
there are several key factors that improve the prospects for cooperation in collective
action problems. We use this to identify strategies to impro ve the prospects for indus-
try cooperation on the responsible development of AI.
Introduction
Machine learning (ML) is used to develop increasingly capab le systems targeted at tasks like voice
recognition, fraud detection, and the automation of vehicl es. These systems are sometimes referred to as
narrow artiﬁcial intelligence (AI) systems. Some companie s are also using machine learning techniques
to try to develop more general systems that can learn effecti vely across a variety of domains rather than
in a single target domain. Although there is a great deal of un certainty about the development path of
future AI systems—whether they will remain specialized or g row increasingly general, for example—
many agree that if the current rate of progress in these domai ns continues then it is likely that advanced
artiﬁcial intelligence systems will have an increasingly l arge impact on society.
This paper focuses on the private development of AI systems t hat could have signiﬁcant expected so-
cial or economic impact, and the incentives AI companies hav e to develop these systems responsibly.
Responsible development involves ensuring that AI systems are safe, secure, and socially beneﬁcial.
In most industries, private companies have incentives to in vest in developing their products responsibly.
These include market incentives, liability laws, and regul ation. We argue that AI companies have the
same incentives to develop AI systems responsibly, althoug h they appear to be weaker than they are in
other industries. Competition between AI companies could d ecrease the incentives of each company to
develop responsibly by increasing their incentives to deve lop faster. As a result, if AI companies would
prefer to develop AI systems with risk levels that are closer to what is socially optimal—as we believe
many do—responsible AI development can be seen as a collecti ve action problem.1
We identify ﬁve key factors that make it more likely that comp anies will be able to overcome this collec-
tive action problem and cooperate—develop AI responsibly w ith the understanding that others will do
likewise. These factors are: high trust between developers (High Trust ), high shared gains from mutual
cooperation ( Shared Upside ), limited exposure to potential losses in the event of unrec iprocated cooper-
ation ( Low Exposure ), limited gains from not reciprocating the cooperation of o thers ( Low Advantage ),
and high shared losses from mutual defection ( Shared Downside ).
∗Primary/corresponding author.
1AI research companies increasingly have teams dedicated to the safe and ethical development of technology
and many large technology companies participate in volunta ry efforts to articulate and establish principles and
guidelines, and in some cases call for government regulatio n, to address AI-related risks.
Preprint. Work in progress.Using these ﬁve factors, we identify four strategies that AI companies and other relevant parties could
use to increase the prospects for cooperation around respon sible AI development. These include correct-
ing harmful misconceptions about AI development, collabor ating on shared research and engineering
challenges, opening up more aspects of AI development to app ropriate oversight, and incentivizing
greater adherence to ethical and safety standards. This lis t is not intended to be exhaustive, but to show
that it is possible to take useful steps towards more respons ible AI development.
The paper is composed of three sections. In section 1, we outl ine responsible AI development and its
associated costs and beneﬁts. In section 2, we show that comp etitive pressures can generate incentives
for AI companies to invest less in responsible development t han they would in the absence of compe-
tition, and outline the ﬁve factors that can help solve such c ollective action problems. In section 3, we
outline the strategies that can help companies realize the g ains from cooperation. We close with some
questions for further research.
1 The beneﬁts and costs of responsible AI development
AI systems have the ability to harm or create value for the com panies that develop them, the people that
use them, and members of the public who are affected by their u se. In order to have high expected value
for users and society, AI systems must be safe—they must reli ably work as intended—and secure—
they must have limited potential for misuse or subversion. A I systems should also not introduce what
Zwetsloot and Dafoe (2019) call “structural risks”, which i nvolve shaping the broader environment in
subtle but harmful ways.2The greater the harm that can result from safety failures, mi suse, or structural
risks, the more important it is that the system is safe and ben eﬁcial in a wide range of possible conditions
(Dunn, 2003). This requires the responsible development of AI.
1.1 What is responsible AI development?
AI systems are increasingly used to accomplish a wide range o f tasks, some of which are critical to users’
health and wellbeing. As the range of such tasks grows, the po tential for accidents and misuse also
grows, raising serious safety and security concerns (Amode i, Olah, et al., 2016; Brundage, Avin, et al.,
2018). Harmful scenarios associated with insufﬁciently ca utious AI development have already surfaced
with, for example, biases learned from large datasets disto rting decisions in credit markets and the
criminal justice system, facial recognition technologies disrupting established expectations of privacy
and autonomy, and auto-pilot functions in some automobiles causing new types of driving risk (while
reducing others). Longer term, larger scale scenarios incl ude dangers such as inadvertent escalation of
military conﬂict involving autonomous weapon systems or wi despread job displacement.
Responsible AI development involves taking steps to ensure that AI systems have an acceptably low risk
of harming their users or society and, ideally, to increase t heir likelihood of being socially beneﬁcial.
This involves testing the safety and security of systems dur ing development, evaluating the potential
social impact of the systems prior to release, being willing to abandon research projects that fail to meet
a high bar of safety, and being willing to delay the release of a system until it has been established that
it does not pose a risk to consumers or the public. Responsibl e AI development comes in degrees but
it will be useful to treat it as a binary concept for the purpos es of this paper. We will say that an AI
system has been developed responsibly if the risks of it caus ing harms are at levels most people would
consider tolerable, taking into account their severity, an d that the amount of evidence grounding these
risk estimates would also be considered acceptable.3
Responsible AI development involves work on safety, securi ty, and the structural risks associated with
AI systems. Work on the safety of AI aims to mitigate accident risks (Amodei, Olah, et al., 2016) and
ensure that AI systems function as intended (Ortega, Maini, et al., 2018) and behave in ways that people
want (Irving et al., 2018). Work on the security of AI aims to p revent AI systems from being attacked,
2Zwetsloot and Dafoe (2019) argue that the “the accident-mis use dichotomy obscures how technologies, in-
cluding AI, often create risk by shaping the environment and incentives”. We restrict accident risks to technical
accidents and misuse risks to direct misapplications of a sy stem. ‘Structural risks’, as we use the term, are intended
to capture the broader impact of AI systems on society and soc ial institutions.
3This will generally mean if an AI system is developed respons ibly, the risk of irreversible catastrophic harm
from that system—whether through accident, misuse, or nega tive social impact—must be very low. This is consis-
tent with what Sunstein (2005) calls the ‘Irreversible Harm Precautionary Principle’.
2co-opted, or misused by bad actors (Brundage, Avin, et al., 2 018).4Work evaluating the structural im-
pact of AI aims to identify and mitigate both the immediate an d long term structural risks that AI systems
pose to society: risks that don’t quite ﬁt under narrow deﬁni tions of accident and misuse. These include
joblesness, military conﬂict, and threats to political and social institutions.5
1.2 The cost of responsible AI development
It is likely that responsible development will come at some c ost to companies, and this cost may not be
recouped in the long-term via increased sales or the avoidan ce of litigation. In order to build AI systems
responsibly, companies will likely need to invest resource s into data collection and curation, system
testing, research into the possible social impacts of their system, and, in some cases, technical research
to guarantee that the system is reliably safe. In general, th e safer that a company wants a product to be,
the more constraints there are on the kind of product the comp any can build and the more resources it
will need to invest in research and testing during and after i ts development.
If the additional resources invested in ensuring that an AI s ystem is safe and beneﬁcial could have been
put towards developing an AI system with fewer constraints m ore quickly, we should expect responsible
AI development to require more time and money than incautiou s AI development. This means that
responsible development is particularly costly to compani es if the value of being the ﬁrst to develop
and deploy a given type of AI system is high (even if the ﬁrst sy stem developed and deployed is not
demonstrably safe and beneﬁcial).
There are generally several advantages that are conferred o n the ﬁrst company to develop a given tech-
nology (Lieberman and Montgomery, 1988). If innovations ca n be patented or kept secret, the company
can gain a larger share of the market by continuing to produce a superior product and by creating switch-
ing costs for users. Being a ﬁrst-mover also allows the compa ny to acquire scarce resources ahead of
competitors. If hardware, data, or research talent become s carce, for example, then gaining access to
them early confers an advantage.6And if late movers are not able to catch up quickly then ﬁrst-m over
advantages will be greater. In the context of AI development , having a lead in the development of
a certain class of AI systems could confer a ﬁrst mover advant age. This effect would be especially
pronounced in the case of discontinuous changes in AI capabi lities7, but such a discontinuity is not
necessary in order for a ﬁrst mover advantage to occur.
Responsible development may therefore be costly both in ter ms of immediate resources required, and
in the potential loss of a ﬁrst-mover advantage. Other poten tial costs of responsible AI development
include performance costs and a loss of revenue from not buil ding certain lucrative AI systems on the
grounds of safety, security, or impact evaluation. An examp le of a performance cost is imposing a limit
on the speed that self-driving vehicles can travel in order t o make them safer. An example of revenue
loss is refusing to build a certain kind of facial recognitio n system because it may undermine basic civil
liberties (Smith, 2018b).
AI companies may not strongly value being the ﬁrst to develop a particular AI system because ﬁrst-
mover advantages do not always exist. Indeed, there are ofte n advantages to entering a market after
the front-runner. These include being able to free-ride on t he R&D of the front-runner, to act on more
information about the relevant market, to act under more reg ulatory certainty, and having more ﬂex-
ible assets and structures that let a company respond more ef fectively to changes in the environment
(Gilbert and Birnbaum-More, 1996). These can outweigh the a dvantages of being the ﬁrst to enter that
same market. And it has been argued that late mover advantage s often do outweigh ﬁrst mover advan-
tages (Markides and Geroski, 2004; Querbes and Frenken, 201 7). We therefore acknowledge that the
assumption that there will be a ﬁrst-mover advantage in AI de velopment may not be true. If a ﬁrst-mover
advantage in AI is weak or non-existent then companies are le ss likely to engage in a race to the bottom
on safety since speed is of lower value. Instead of offering p redictions, this paper should be thought of
as an analysis of more pessimistic scenarios that involve at least a moderate ﬁrst mover advantage.
4Mitigating misuse risks is sometimes included under AI safe ty, broadly construed (Christiano, 2016)
5The literature on the societal impact of AI is vast. See Cummi ngs (2017) on AI and warfare, for example. For
a broader overview see Dafoe (2018).
6As Klepper (1996) notes, larger ﬁrst-movers can also spread their prior investment in R&D over a larger
number of applications.
7The possibility of discontinuous progress in AI is discusse d by Good (1966), Chalmers (2009), Yudkowsky
(2013), Shanahan (2015) and Bostrom (2017b). AI Impacts (20 18) provide a critical overview of the arguments for
discontinuity and Christiano (2018) presents arguments ag ainst the claim.
3Much of the discussion of AI development races assumes that t hey have a deﬁnitive endpoint. Although
some have hypothesized that if AI progress is discontinuous or sufﬁciently rapid then it could essen-
tially have a deﬁnitive endpoint, the case for this remains s peculative.8It is therefore important to
note that AI development may take the form of a perpetual R&D r ace: a race to stay technologically
ahead of competitors rather than a race to reach some particu lar technological endpoint (Aoki, 1991;
Breitmoser et al., 2010). If this is the case then AI companie s would still have an incentive to speed
up development in order to stay ahead of others, especially i f the gap between companies was small.
The present analysis is applicable to perpetual races in whi ch there is at least a moderate ﬁrst mover
advantage, several companies are competing to stay ahead, a nd leadership is not yet entrenched.9
1.3 The beneﬁts of responsible AI development
In the law and economics literature on product safety, it is g enerally accepted that market forces create
incentives for companies to invest in making their products safe (Oi et al., 1973). Suppose that compa-
nies have accurate information about how safe the products t hey are developing are and that consumers
have access to accurate information about how safe a company ’s product is, either prior to release
or by observing the harms caused by a product after it is relea sed (Ben-Shahar, 1998; Chen and Hua,
2017).10If consumers have a preference for safer products and respon d rationally to this preference,
they will not buy products that are insufﬁciently safe, or wi ll pay less for them than for safer alternatives
(Polinsky and Shavell, 2009). Releasing unsafe products wi ll also result in a costly loss of reputation
for companies (Daughety and Reinganum, 1995).11Finally, releasing unsafe products could result in
burdensome regulation of the industry or in litigation cost s. Therefore companies that are concerned
about a sufﬁciently long time-horizon involving repeated i nteraction with customers, regulators, and
other stakeholders that incentivize safety should interna lize the value of responsible development.
Market forces alone may not always incentivize companies to invest the appropriate amount into en-
suring their products are safe. If consumers cannot get acce ss to information about the safety of a
product—how likely safety failures are or how costly they ar e—then companies have an incentive to
under-invest in safety. And if companies have inaccurate in formation about the safety of the products
they are developing, they will not invest in safety to the deg ree demanded by consumers. Finally, poor
corporate governance can result in suboptimal decisions ab out risk (Cai et al., 2010). Product liability
law and safety regulation are intended to correct such marke t failures by providing consumers with infor-
mation about products, incentivizing companies to invest m ore in safety, and compensating consumers
that are harmed by product safety failures (Hylton, 2012; La ndes and Posner, 1985).12
We may expect companies to under-invest in safety if the cost s to consumers don’t result in commen-
surate costs for the company; either via a reduction in reven ue, reputation loss, ﬁnes from regulators,
or successful litigation by consumers. Safety failures can also affect those who do not consume the
product, however. Consider a 2018 recall of over 8,000 V olks wagen vehicles potentially affected
by a brake caliper issue that could result in increased stopp ing distances or loss of vehicle control
(Consumer Reports, 2018). A safety failure resulting from t his could harm not only the vehicle’s occu-
pants but also pedestrians and other drivers.13Harms that safety failures inﬂict on non-consumers are
negative externalities, and beneﬁts that safer products pr oduce for non-consumers are positive external-
ities. We should anticipate companies under-investing in r educing negative externalities and increasing
8If AI development is extremely rapid then gaps between each c ompany would likely increase over time. This
means that a company that is ahead of others may at some point b e ahead of them by a great deal in strategically
important areas, and could use this to undermine their compe titors (Bostrom, 2017b, pp. 91-104)
9Breitmoser et al. (2010) note that perpetual R&D races tend t o collapse into leadership monopolies. The larger
the gap between the front-runner and the company in second pl ace in a perpetual race, the less of an incentive the
front-runner has to trade safety for speed.
10See Daughety et al. (2013), who make similar assumptions in t heir idealized model of markets.
11Rhee and Haunschild (2006) provide evidence that the relati onship between safety failures and reputation loss
may be more complex than this, however.
12See Stiglitz (2009) on government regulation as a response t o market failures or inefﬁciencies, but note that ac-
tual motivations for government regulation are typically m ore complicated (Henson and Caswell, 1999). Calabresi
(1970) provides comprehensive overview of of the role of law in the minimization of costs from safety failures. The
relationship between product liability law and safety regu lations—in particular, whether it is efﬁcient to use them
jointly—is a matter of some debate (Shavell, 1984; Kolstad e t al., 1990).
13There is currently uncertainty about who should be held liab le for the harms that the safety failures of au-
tonomous systems inﬂict on the public (Schellekens, 2015; T he Atlantic, 2018).
4positive externalities relative to their social value, sin ce the costs and beneﬁts this produces for society
don’t result in commensurate costs and beneﬁts for the compa ny (Dahlman, 1979).
To give a concrete example, consider facial recognition tec hnology. Microsoft have argued that this
technology could be used in ways that many would consider har mful: to violate individuals’ privacy
or suppress their political speech, for example (Smith, 201 8a,b). Even if companies would prefer to
build facial recognition systems that cannot be misused, ei ther to avoid causing harm or to avoid the
reputation costs of this harm, the cost of developing safegu ards may not outweigh their beneﬁts if
companies cannot be held liable for these harms and there is n o regulation preventing misuse. For this
reason, Microsoft has called for regulation that would requ ire that companies invest in measures that
reduce the risks from facial recognition technology, and th at could also mitigate potential misuse of the
technology by commercial entities or by governments (Smith , 2018a).
The discussion thus far treats companies as though they were motivated only by proﬁt, i.e. they only
care about things like reputation and product safety insofa r as they are a means to make more proﬁt or
avoid losses. This view is common in the literature on corpor ate social responsibility (Campbell, 2007;
Devinney, 2009) but it is clearly an abstraction. Companies are run by, invested in, and composed of
humans that care about the impact their products will have on the world and on other people. Employees
at technology companies have already shown that they care a g reat deal about the social implications of
the systems they are building (Minsberg, 2019).
The things that motivate AI companies other than proﬁts, suc h as beneﬁting people rather than harming
them, will generally push even more in favor of responsible d evelopment: they will rarely push against it.
Assuming that companies are motivated solely by proﬁt there fore lets us analyze a kind of ‘worst case
scenario’ for responsible development. We will therefore o ften treat companies as though they were
driven solely by proﬁt, even though we do not ﬁnd this plausib le. It is important that the reader bear
this in mind, since treating companies as proﬁt-driven enti ties can be self-fulﬁlling, and can therefore
contribute to the very problems we are attempting to solve.
1.4 Are existing incentives for responsible AI development enough?
If markets are functioning well and companies and consumers have perfect information about the ex-
pected harm of a product, companies should invest the social ly optimal amount into product safety
(Daughety et al., 2018). In real-world scenarios in which ma rkets may not function perfectly and in-
formation asymmetries exist, incentives for companies to i nvest sufﬁciently in product safety typically
come from three sources: market forces, liability law, and i ndustry or government regulation.14These
three sources of incentives may not provide strong enough in centives for AI companies to engage in
responsible AI development, however. We will brieﬂy survey some reasons for this.
1.4.1 Limited consumer information
Consumers of AI systems include individuals, private compa nies, and public institutions. Although
different consumers will have access to different levels of information about AI systems, information
about the expected harm of AI systems is likely to be quite lim ited on average. As cutting-edge AI
systems become more complex, it will be difﬁcult for consume rs not involved in the development of
those systems to get accurate information about how safe the systems are. Consumers cannot directly
evaluate the safety of aviation software, for example, and w ill face similar difﬁculties when it comes
to directly evaluating the safety of complex machine learni ng models. This is compounded by the fact
that it is notoriously difﬁcult to explain the decisions mad e by neural networks (Doshi-Velez and Kim,
2017; Olah et al., 2018). If consumers cannot assess how risk y a given AI system is, they cannot adjust
their willingness to pay for it accordingly. They are also le ss able to identify and exert pressure on AI
companies that are investing too little in safety (Anton et a l., 2004).
Consumers could get information about how safe an AI system i s by tracking safety failures after its
release, but such a ‘wait and see’ strategy could leave both c onsumers and the public vulnerable to
harmful safety failures. This is of particular concern if th ose safety failures could be irreversible or
catastrophic. And the probability of irreversible or catas trophic safety failures is likely to increase as AI
14Other mechanisms include no fault liability systems like ma ndatory insurance and increasing the information
available to consumers (Cornell et al., 1976).
5systems become more capable and general, since more advance d systems are more likely to be relied
upon across a wider range of domains and in domains where fail ures are more harmful.15
1.4.2 Limited company and regulator information
Measuring the safety, security, and social impact of AI syst ems may turn out to be extremely difﬁcult
even for those who understand the technical details of the sy stem. Neural networks are difﬁcult to
interpret and as such, failures may be difﬁcult to predict. I f AI companies are over-conﬁdent that their
system is not risky, they may under-invest in important risk -reducing measures during development or
release a system that causes unintended harm.
If regulators cannot assess how risky a given AI system is, th ey may be overly stringent or overly liberal
when using regulatory controls (Shavell, 1984). The abilit y to get accurate information about AI systems
therefore seems to be crucial for ex ante safety measures.16
Our current capacities to identify and measure the expected harms of particular AI systems are extremely
limited. We still do not fully understand the decisions made by complex machine learning models
(Olah et al., 2018; Hohman et al., 2018) and the high-dimensi onality of the inputs to AI systems makes
it such that exhaustive enumeration of all possible inputs a nd outputs is typically infeasible. There
may therefore be little consensus about whether a particula r system is likely to be unsafe, unsecure, or
socially harmful at present. Given this, it is likely that ad ditional capacity will need to be invested by
companies or regulators or both in order to decrease these in formation asymmetries.
1.4.3 Negative externalities from AI
Harms caused by AI systems are likely to affect third parties . Biases in algorithmic pre-trial risk assess-
ment are more likely to harm those accused of crimes than thos e that purchase the tools,17those that
beneﬁt from AI automation may be quite distinct from the peop le who are displaced by automation,18
and a major AI disaster—such as an AI system with a faulty rewa rd function19being integrated into
a critical system—could affect a large portion of society th at is distinct from the AI company and its
consumers. AI also has the potential to be a general purpose t echnology—a technology that radically
affects many sectors of the economy—and if this is the case we should expect its impact to be systemic
(Brynjolfsson et al., 2018; Cockburn et al., 2018).
The harms from AI systems may also be difﬁcult to internalize . For example, the social harms that
result from an increased use of AI systems—such as reduced tr ust in online sources—could be complex
and diffuse, and it may be difﬁcult to hold any one company str ictly liable for them. If the harm is
sufﬁciently large, it may also be too large for a company or in surer to cover all losses (see note 15).
Finally, AI systems could create negative externalities fo r future generations that are not in a position to
penalize companies or prevent them from occurring (Lazear, 1983). We should expect AI companies to
under-invest in measures that could prevent these kinds of n egative externalities.20
1.4.4 The difﬁculty of constructing effective AI regulatio n
There is currently little in the way of AI-targeted regulati on, including government regulation, industry
self-regulation, international standards, and clarity on how existing laws will be applied to AI (see note
15Such safety failures could occur if AI systems have some crit ical function like controlling national power grids
or nuclear weapons systems, or if they can be used to undermin e these systems. The more consequential a given
technology is, the higher the potential cost of releasing an insufﬁciently safe version of that technology to both the
company and to society. But is worth noting that if the expect ed cost of catastrophic safety failures is capped by a
company’s ability to pay then we might expect companies to un der-weight these tail-end risks. For a taxonomy of
AI risks, see Yampolskiy (2015).
16Leike et al. (2017) introduce simple environments for evalu ating the safety of AI agents, and note that future
versions of these environments could be used to benchmark th e safety performance of AI agents.
17See Tsukayama and Williams (2018) on how bias in ML systems co uld harm those in the California criminal
justice system. Pleiss et al. (2017) demonstrates the uniqu e difﬁculties of designing bias-free ML systems.
18Segal (2018) notes that the jobs that have declined as a resul t of automation so far are intermediate-skill
jobs like farming. Although it displaces some workers, auto mation has positive effects like increased productivity
(Acemoglu and Restrepo, 2018). The overall effect that AI au tomation will have on the labor force is unclear.
19See Krakovna (2018) and Clark and Amodei (2016) for examples of faulty reward functions in ML systems.
20Safety failures that affect a large portion of the populatio n will not be treated as externalities by companies if
they harm the company or its consumers, though they could sti ll be given insufﬁcient weight.
613). Well-designed regulatory mechanisms can incentivize companies to invest appropriate resources in
safety, security, and impact evaluation when market failur es or coordination failures have weakened the
other incentives to do so. Poorly-designed regulation can b e harmful rather than helpful, however. Such
regulation can discourage innovation (Heyes, 2009) and eve n increase risks to the public (Latin, 1988).
AI regulation seems particularly tricky to get right, as it w ould require a detailed understanding of the
technology on the part of regulators.21The fact that private AI companies can generally relocate ea sily
also means that any attempt to regulate AI nationally could r esult in international regulatory competition
rather than an increase in responsible development.22Regulation that is reactive and slow may also be
insufﬁcient to deal with the challenges raised by AI systems . AI systems can operate much faster than
humans, which can lead to what Johnson et al. (2013) call ‘ult rafast extreme events’ (UEEs) such as
ﬂash crashes caused by algorithmic trading.23
1.4.5 The potential for rapid AI development
Some have hypothesized that progress in AI development will be discontinuous (see note 7). On this
view, there are some types of AI systems—typically advanced ‘general’ AI systems that are capable
of learning effectively across a wide variety of domains—th at, if developed, would represent a sudden
shift from everything that came before them, and could produ ce the equivalent of many years of prior
progress on some relevant metric.24If AI progress is discontinuous then developing an AI system that
constitutes a sudden leap forward could give a company a larg e advantage over others, since the next
best system would be years behind it in terms of prior progres s in the ﬁeld.25Consider the advantage
that a company today would gain if they managed to develop som ething over a decade ahead of current
systems used for cyber offense and defense, for example.
If progress in AI development is discontinuous then market f orces and liability law may do little to en-
courage safe development.26The value of developing a system that gives a company a huge ad vantage—
that could be used to undermine competition or seize resourc es, for example—would be largely divorced
from the process of getting market feedback. And a company ca n only be held liable for accidents if
these accidents are not catastrophic and the existing legal framework can both keep up with the rapidity
of technological progress and enforce judgments against co mpanies. Therefore if AI progress is discon-
tinuous, ex ante safety measures like industry self-regula tion or international oversight may be more
effective than ex post safety measures like market response and liability.
1.5 Summary
Incentives to develop safe products generally come from the market, liability laws, and regulation
(Rubin, 2011), as well as factors that motivate AI companies beside proﬁts, such as a general desire
to avoid doing harm. For AI companies, the proﬁt motive to dev elop AI responsibly is likely to come
from the additional revenue generated by AI systems that are more valuable to consumers, the avoidance
21Hadﬁeld (2017) and Hadﬁeld and Clark (2019) outline a regula tory framework for AI that could overcome
barriers like information asymmetries and slow response ti mes: key problems for the regulation of new technology.
22Esty and Geradin (2001) offer an overview of different persp ectives on regulatory competition, while
Genschel and Plumper (1997) note that regulatory competiti on and international co-operation can actually increase
levels of regulation. Erdélyi and Goldsmith (2018) argue th at an international AI regulatory agency should be
established, but on the grounds that AI has externalities th at cross national boundaries.
23For more on this problem, see Muehlhauser and Hibbard (2014) . Anticipating, preventing, and responding to
catastrophic AI-caused UEEs may present a key challenge in A I safety and policy.
24See Ehrnberg (1995) on deﬁnitions of technological discont inuities. The AI discontinuity hypothesis should
not be confused with the claim that there will be rapid AI deve lopment in the future—progress in AI development
could be continuous but extremely rapid, e.g. hyperbolic (C hristiano, 2017)—but that there will be a system
that represents a sudden leap forward in AI capabilities. It may be possible to achieve a decisive advantage over
competitors if progress is rapid but not discontinuous.
25Bostrom (2017b) claims that such an AI could give a company a ‘ decisive strategic advantage’: ‘a level of
technological and other advantages sufﬁcient to enable it t o achieve complete world domination’ (p.96, ibid.). But
the concerns we raise here apply even if the advantage is extr eme but not decisive in this sense.
26This may be true even if progress in AI development is continu ous but rapid. Even if no single company has a
profound advantage over others, mechanisms like regulatio n and liability could be too slow to catch up with the rate
of technological progress. It is worth noting that if AI prog ress takes this shape then responsible AI development
may be more like a one-shot game than an iterated game, which r educes developers’ incentives to cooperate on
responsible development for reasons that we discuss in the n ext section.
7of reputational harm from safety failures, the avoidance of widespread harms caused by AI systems (see
note 20), and the avoidance of tort litigation or regulatory penalties.
A key factor that can inﬂuence the cost-beneﬁt ratio of respo nsible AI development that we have not
discussed, however, is the competitive environment in whic h the AI systems in question are being
developed. In the next section we will explore the impact tha t competition between AI companies can
have on the incentives that each company has to invest or fail to invest in responsible development.
2 The need for collective action on responsible AI developme nt
We have argued that safer, more secure, and more socially val uable AI systems will tend to have a higher
market value, be less likely to cause costly accidents that t he company is held liable for, and so on. This
means that if a company is guaranteed to be the ﬁrst to develop a system of this type, we can expect
that they will invest resources to ensure that their system i s safe, secure, and socially beneﬁcial to the
extent that this is incentivized by regulators, liability l aw, and market forces. This means the more that
positive and negative externalities of AI systems have been internalized via these mechanisms, the more
that companies can expect to invest in responsible developm ent.27
In this section we will argue that, even with these incentive s in place, competitive pressures can cause
AI companies to invest less in responsible development than they otherwise would. Responsible AI
development can therefore take the form of a collective acti on problem. We then identify and discuss ﬁve
key factors that improve the prospects for cooperation betw een AI companies that could ﬁnd themselves
in a collective action problem over responsible developmen t.
2.1 How competitive pressures can lead to collective action problems
To see how the competitive environment could affect investm ent in responsible development, suppose
that several AI companies are working on a similar type of sys tem. If there is a large degree of substi-
tutability between the inputs of different aspects of devel opment, we should not expect AI companies
to invest in responsible development beyond the point at whi ch the expected marginal return is lower
than the expected marginal return from investing in other ar eas of development. Suppose each company
places less value on coming second than on coming ﬁrst, less v alue in coming third than in coming sec-
ond, and so on. These companies will likely engage in a techno logical race: a competition to develop a
technology in which the largest reward goes to the ﬁrst compa ny (Grossman and Shapiro, 1985).28The
resulting dynamics may be similar to those we would expect to see in patent races between ﬁrms.29
There are various strategies companies could use in a “winne r takes more” race: they could try to
develop and maintain a strong technical lead or they could tr y to to maintain a close position behind
the technical leader, for example.30For now, we will assume that the best strategy involves tryin g to
develop and maintain a strong technical lead throughout the race.
Since speed is more valuable when racing against others, we s hould expect investment into responsible
development to be lower when companies are racing against ea ch other.31Armstrong et al. (2016)
point out that in an AI development race, responsible develo pment could be prey to a “race to the
bottom” dynamic. Consider what happens if one company decid es to increase their development speed
by decreasing their investment in safety, security, and imp act evaluation. This increases their expected
ranking in the race and decreases the expected ranking of oth ers in the race. A decrease in expected
27If the ﬁrst company could prevent future competitors from en tering the market (i.e. the ﬁrst company could
expect to be the only company), it is likely this would reduce but not eliminate market incentives to invest in
responsible development (Sheshinski, 1976).
28As we noted in the previous section, this is a non-trivial ass umption that will not hold in all cases.
29Patent races have positive effects on innovation, though at the cost of duplicating efforts (Judd et al., 2012).
30The best strategy may depend on the competitive environment . Dasgupta and Stiglitz (1980) argue that mo-
nopolist companies will attempt to outspend their rivals on R&D to prevent a duopoly, while Doraszelski (2003)
shows that there are conditions in which companies that are b ehind will invest to catch up.
31How much lower will depend on various features of the race, su ch as how close it is and the value placed on
each position. Note that this argument assumes that investm ents with even worse expected marginal returns have
already been cut. It also assumes that investments in respon sible development contribute less to development speed
than other available investments: not that they contribute nothing to development speed.
8ranking gives competing AI companies an incentive to decrea se their own investment in these areas in
order to maintain or increase their expected ranking in the r ace.32
We might ask why racing to the bottom on product safety is not u biquitous in other industries in which
decreasing time-to-market is valuable, such as in the pharm aceutical industry.33The most plausible
explanation of this difference is that the cost of safety fai lures has been internalized to a greater extent in
more established industries via external regulation, self -regulation, liability, and market forces. These
mechanisms can jointly raise the “bottom” on product safety to a level that is generally considered
acceptable by regulators and consumers.34
In a race to the bottom on safety, competing AI companies coul d reduce their investment in responsible
development to the point that winning the technology race—s uccessfully developing the system they
are racing to develop before others—is barely of net positiv e value for the winner even after all the
ﬁrst-mover advantages, including positive reputational e ffects, the ability to capture resources like data,
hardware and talent, and creating switching costs for consu mers, have been taken into account.35
2.2 When competition has negative rather than positive effe cts
The race to the bottom on safety described above is a collecti ve action problem: a situation in which
all agents would be better off if they could all cooperate wit h one another, but each agent believes it is
in their interest to defect rather than cooperate.36As Heckathorn (1989, p. 78) states, “the inclinations
of individuals (that is, each actor’s preferences regardin g his or her own behavior) are in conﬂict with
regulatory interests (that is, each actor’s preferences re garding the behavior of others). The collective
action problem arises when a group possesses a common intere st, or faces a common fate.”
In a race to the bottom on safety, it is in each company’s inter est to reduce their investment in responsible
development in order to increase development speed. If all c ompanies do this, however, there is a single
equilibrium: one in which much or all of the value that could h ave been be gained with coordination
is destroyed. If each company defects, they will have a simil ar position in the race to the one that they
would have had if they had all successfully coordinated, but they will be developing systems that are
more risky than the ones they would have developed if they had all managed to successfully coordinate.
In other words, the situation in which they ﬁnd themselves is strictly worse than the situation in which
coordination was successful.
Collective action problems between companies can have posi tive effects on consumers and the public.
A price war is a collective action problem between companies with mostly positive effect on consumers,
for example, as it results in lower prices.37Antitrust law exists to maintain competition between com-
panies that has a positive effect on consumers and to prevent collusion between companies that has a
negative effect on consumers (e.g. price ﬁxing).
32In this scenario, companies have full information about the investments made by other companies are their
likelihood of winning. But this assumption is not necessary , since companies can invest in accordance with their
expectation about the investments and win probabilities of other companies. Armstrong et al. (2016) explore sce-
narios in which AI companies have different levels of inform ation about their own and others’ capabilities.
33It is worth noting that similar concerns about the desire to d evelop quickly conﬂicting with risk management
have been expressed in other industries that involve novel t echnology, such as the use of nanoparticles and nan-
otechnology in the food industry (Morgan, 2005; Cushen et al ., 2012).
34It could also be the that the best strategies in technologica l races do not involve trying to develop a strong
technological lead, or that there are unidentiﬁed factors t hat make racing to the bottom on product safety undesirable:
factors that may apply equally to the development of AI syste ms.
35The company with the winning system could even consider thei r own system to be worse than developing
nothing at all absent competition, though this would only ha ppen if they considered the release of the alternative
winning system to be even worse for them than the release of th eir own worse-than-nothing system.
36This is a weakening of the deﬁnition that Jon Elster derives f rom Schelling (2006 [1978]), which states ‘First,
each individual derives greater beneﬁts under conditions o f universal cooperation than he does under conditions of
universal noncooperation. Second, each derives more beneﬁ ts if he abstains from cooperation, regardless of what
others do.’ (Elster, 1985, p.139). We simply replace ‘regar dless of what others do’ with ‘given what we expect
others will do.’ See Holzinger (2003) for a broader deﬁnitio n and taxonomy.
37When companies engage in price wars, prices often end up clos e to their marginal cost of production
(Bresnahan, 1987). Prices can even be temporarily set below the marginal cost of production in order to push
competitors out of the market (Guiltinan and Gundlach, 1996 ), sometimes in violation of antitrust.
9When there are negative effects from production that are not captured by the incentives facing producers
(i.e. negative externalities), however, competition does not lead to the socially optimal outcome. If this
outcome is also bad for the producers, it is a collective acti on problem for producers.
A race to the bottom on safety falls into this category if it re sults in AI systems with safety levels below
what is socially optimal and below what AI companies would pr efer. Pollution by companies is another
example of a collective action problem between companies th at has a negative effect on the public
(Lévêque, 1999).
Before discussing strategies for cooperation such as self- regulation in more depth, however, it will
be useful to understand the incentives that AI companies hav e to abide by norms that involve mutual
investment in responsible AI development. This will be the f ocus of the remainder of this section.
2.3 Incentives to cooperate in collective action problems
In an AI development race, companies “cooperate” if they mai ntain some acceptable level of investment
in responsible development and they “defect” if they fail to maintain this level of investment, thereby
acting in their own interest (hypothetically) and against t he collective interest. Encouraging companies
to cooperate should therefore not be confused with encourag ing them to stop competing. Companies
agreeing not to compete across the investment in safety dime nsion does not imply that they will cease
to compete across the R&D dimension. Competitive dynamics t hat contain cooperative elements are
sometimes referred to as a “coopetition”.38
If companies have incentives to prevent or mitigate collect ive action problems that have negative effects
on consumers or the public then we should expect the companie s themselves (and not just third parties
like government regulators) to take steps to solve them. And companies often do attempt to cooperate
to prevent or solve collective action problems of this sort. One example of a mechanism used to this end
is industry self-regulation. (Gunningham and Rees, 1997).39Examples of self-regulation include Re-
sponsible Care: a self-regulation program in the US chemica ls industry (Gamper-Rabindran and Finger,
2013), and the Institute of Nuclear Power Operations (INPO) : an industry organization that conducts in-
spections and facilitates the sharing of best practices in t he nuclear power industry (Davis and Wolfram,
2012; Hausman, 2014).40
In order to identify features that affect the degree to which it is in a company’s interest to cooperate on
responsible development, it will be helpful to highlight fe atures that increase incentives to cooperate in
collective action problems generally. To do this, consider the payoff matrix of a cooperate-defect game
in which two agents (AI companies) can cooperate (develop re sponsibly) or defect (fail to develop
responsibly). Here the ﬁrst letter in each pair represent th e expected payoff for Agent 1, and the second
letter in each pair represents the payoff for Agent 2.41
Agent 2
Cooperate Defect
Agent 1Cooperate a1,a2b1,b2
Defect c1,c2d1,d2
Table 1: A Normal Form Cooperate-Defect Game
38See Bengtsson and Kock (2000) and Tsai (2002).
39Industry self-regulation can also be incentivized by gover nment regulators via meta-regulation (Parker, 2007;
Coglianese and Mendelson, 2010).
40Other examples of self-regulation can be found in a variety o f industries, as self-regulation is sometimes used
to preempt government regulation (Lenox, 2007). How succes sful such self-regulation is at reducing the negative
effects of collective action problems varies a great deal by industry. The INPO is generally considered to be a
more successful self-regulatory scheme than Responsible C are, for example (Cohen and Sundararajan, 2015, pp.
126-7). This may be because the INPO, unlike Responsible Car e, has an agreement with a government regulator,
the Nuclear Regulatory Commission, which can monitor the pr ogram and provide meaningful sanctions, which
may be required for successful self-regulation (King and Le nox, 2000). O’Keefe (forthcoming 2019) explores one
possible form of antitrust-compliant self-regulation in t he AI industry.
41We assume that these expected utilities have already factor ed in agents’ attitudes towards risk and discuss
some of the simpliﬁcations of this framework below.
10Letpbe the probability that Agent 1 assigns to Agent 2 cooperatin g and let qbe the probability that
Agent 2 assigns to Agent 1 cooperating. We assume it is ration al for Agent 1 to cooperate if the expected
value of cooperation (the likelihood Agent 2 will cooperate timesa1plus the likelihood Agent 2 will
defect times b1) is greater than the expected value of defection (the likeli hood Agent 2 will cooperate
timesc1plus the likelihood Agent 2 will defect times d1). We assume the same is true of Agent 2.42
This lets us identify ﬁve highly interrelated factors that i ncrease an agent’s incentive to cooperate. These
factors are as follows, where expected values are relative t o the agent’s beliefs:43
(1) High Trust: being more conﬁdent that others will cooperate ( p,q)44
(2) Shared Upside: assigning a higher expected value to mutual cooperation ( a1,a2)
(3) Low Exposure: assigning a lower expected cost to unreciprocated cooperat ion (b1,c2)
(4) Low Advantage: assigning a lower expected value to not reciprocating coope ration (c1,b2)
(5) Shared Downside assigning a lower expected value to mutual defection ( d1,d2)
The last four factors each refer to the expected value of an ac tion conditional on the behavior of the other
agent, such as cooperating and having your cooperation reci procated. Note that the expected value of an
action depends on how good the agent perceives the outcome to be and how likely the agent perceives
it to be. This means that an agent could be in a ‘low exposure’ s cenario if she considers unreciprocated
cooperation to be not very valuable or not very likely or both . We can provide agents with evidence
about the likelihood and value of each outcome by changing th e world in some perceptible way, e.g.
by offering a reward for responsible development, or by givi ng them evidence about the way the world
already is, e.g. by correcting false beliefs.
It is useful to separate the degree of trust (factor 1) from in centives (factors 2-5) in order to discuss its
role in cooperation, but trust is not independent of incenti ves or vice versa. If one agent comes to trust
an agent more, this increases the expected value of the outco mes that involve cooperation.45The same
is true in reverse: if the expected value of the outcomes that involve cooperation increase, it is more
likely that the other agent will cooperate. In other words, i ncreasing trust can increase incentives to
cooperate, and increasing incentives to cooperate can incr ease trust between agents.46
This means that if a company can provide information about it self that increases the probability the
other assigns to it cooperating, this will increase the degr ees of trust between the companies and make
it more likely each company’s trust threshold will be met. Tw o important facts follow from this. First,
information that companies provide about their intentions and actions—how transparent they are—can
play an important role in whether other companies will coope rate with them. Second, trust is prey
to virtuous and vicious cycles. If one company demonstrably increases its trust in another, the other
company should increase its trust in return. But if one compa ny demonstrably decreases its trust in
another, the other company should decrease its trust in retu rn.47.
A real world race to the bottom on safety would unfold over man y interactions. The factors identiﬁed
here also increase the prospect of cooperation in sequentia l games, however.48And iterated collective
42In other words, it is rational for Agent 1 to cooperate if p×a1+(1−p)×b1> p×c1+(1−p)×d1and
it is rational for Agent 2 to cooperate if q×a2+(1−q)×c2> q×b2+(1−q)×d2. These two agents are in a
collective action problem if it is irrational for both agent s to cooperate, but a1> d1anda2> d2. If both sides of
these equations are equal then defecting and cooperating ar e both rationally permissible for the agent. Note that if
a1+a2> d1+d2buta1≯d1ora2≯d2(i.e. defecting is rational for at least one agent but mutual cooperation
creates more total value for both agents than mutual defecti on does) then the likelihood of cooperation increases if
redistribution is possible, i.e. if the agents can bargain t owards a solution.
43If Agent 1 and Agent 2 are not in an anti-coordination game the n Agent 1’s incentives to cooperate increase
as (1)pincreases, (2) the expected value of a1increases, (3) the expected value of c1increases, (4) the expected
value ofb1decreases, and (5) the expected value of d1decreases. Naturally, the inverse of each of these factors w ill
decrease the agent’s incentive to cooperate.
44This is not the only deﬁnition of ‘trust’, but it is the one tha t is most relevant to the current analysis.
45We say ‘in the situations we consider here’ because if the age nts are in an anti-coordination game then increas-
ing Agent 1’s trust in Agent 2 will decrease Agent 1’s incenti ves to cooperate.
46Again, this will not be true in certain anti-coordination ga mes.
47This is one reason why a degree of ‘forgiveness’ can be strate gically valuable: it can prevent errors, misinter-
pretations, or aberrant behavior from plunging both player s into a vicious cycle of distrust prematurely, and can
pull players out of such a cycle (Axelrod, 1980)
48The main adjustment we need to make to the factors above in ext ensive form games will be to the ﬁrst factor:
high trust. If we let Cimean that agent icooperates and assume that agents can only either cooperate or defect, in
11action problems are generally easier to solve than one-shot collective action problems because, in iter-
ated collective action problems, players have an incentive (and opportunity) to cooperate early in the
game in order to establish trust and avoid retaliation.49Using one-shot games to illustrate our points is
therefore more likely to skew us towards undue pessimism abo ut our ability to solve races to the bottom
rather than undue optimism.
One shortcoming of our analysis, however, is that it appeals to an overly simpliﬁed conception of co-
operation and defection. For example, we assume that the opt ions available to agents can be divided
into ‘cooperation’ and ‘defection’. In reality, cooperati on will come in varying degrees—companies
can invest different amounts in responsible development, f or example—and it would be better to talk
about the degree of cooperation that we can expect between ag ents.50We also assume that companies
will make an intentional decision to coooperate or defect ov er time. In reality, companies could fail to
foresee the consequences of investing very little into area s like safety, and may therefore defect without
intending to. Third, we assume that both companies perfectl y understand the actions and assertions of
the other. In reality, it may not be clear whether a company is living up to an agreement to develop
AI responsibly. If agreements are not clear then there may no t be a bright line between defection and
non-defection that companies can respond to (Chassang, 201 0; Gibbons and Henderson, 2012). A more
complete analysis of collective action problems in AI devel opment should build a more realistic model
of what cooperating and defecting during AI development wou ld look like.
We have argued that in order to “solve” a collective action pr oblem, we can try to transform it into a
situation in which mutual cooperation is rational. If we can transform it into a situation in which agents
have lower minimum trust thresholds (generally determined by the payoff matrix) and greater trust of
each other—greater conﬁdence that if they cooperate, other s will reciprocate (Kydd, 2007, p. 9)—then
we should expect a higher degree of mutual cooperation.51Given this, we should expect “lower
conﬂict” collective action problems—problems in which age nts have stronger incentives to cooperate—
to be easier to solve than “higher conﬂict” collective actio n problems—problems in which agents have
weaker incentives to cooperate.52
2.4 The cooperative factors in AI development
Whether an AI development race will result in a collective ac tion problem and, if so, how bad it will
be are both open questions.53But there are many features of an AI development race that aff ect both
the likelihood and severity of collective action problems. For example, having close frontrunners would
likely worsen a collective action problem—would reduce the tractability of resolving it—because this
increases the expected value frontrunners will assign to no t reciprocating the cooperation of others
(low advantage) and therefore increases the probability th ey assign to not having their own cooperation
extensive form games our ‘high trust’ factor would say that t he incentives for Agent 1 to cooperate with Agent 2
increase as p(C2|C1)andp(¬C2|¬C1)increase. The other four factors can remain largely unchang ed.
49In an iterated Prisoner’s Dilemma, for example, cooperatio n can be incentivized by things like the threat of
retaliation and the promise of reciprocity (Axelrod, 1984; Nowak, 2006). The promise of reciprocity increases the
expected value of mutual cooperation today (shared upside) and the threat of retaliation decreases the expected
value of betraying the cooperation of others today (low adva ntage). The payoff structure of the iterated Prisoner’s
Dilemma may therefore be more like that of a Stag Hunt (Seabri ght, 1993, p.123). See Mailath et al. (1991) on the
extent to which important features of extensive form games c an be preserved in normal form.
50We are attempting to illustrate the general structure of rea sons to cooperate in AI development rather than
analyzing a particular case in detail.
51Sometimes collective action problems are the result of one o r more agents having mistaken beliefs about the
expected value of cooperating and defecting. When this is th e source of the problem, it can be ‘solved’ by correcting
these misconceptions.
52Scenarios in which agents have stronger incentives to coope rate with one another involve less ‘conﬂict’
(Robinson and Goforth, 2005; Schelling, 1980). How easy it i s to solve collective action problems depends both on
the degree of conﬂict involved and the nature and magnitude o f the resources we have at our disposal. For example,
the Stag Hunt is easier to solve than the Prisoner’s Dilemma. All possible adjustments to the Prisoner’s Dilemma
that result in a solution will, if applied to the Stag Hunt, re sult in a solution to this problem as well. But only some
adjustments to payoffs and probabilities that solve the for mer would also solve the latter.
53It is also worth bearing in mind that scenarios can be superﬁc ially similar to collective action problems even
though it is in everyone’s interest to cooperate.
12reciprocated (high trust).54Similarly, a misaligned perception of the risks associated with different AI
systems could worsen a collective action problem if it cause s less cautious companies to assign a lower
cost to not reciprocating cooperation (low advantage), whi ch could increase the probability that cautious
companies assign to having their cooperation unreciprocat ed by less cautious companies (high trust) and
increase the expected harm that cautious companies expect t o arise from incautious companies getting
ahead this way (low exposure).
Features that affect the likelihood and severity of a collec tive action problem for responsible develop-
ment can be used to decrease its likelihood and severity if th ey are are features that we can control.
For example, fundamental distrust between companies is lik ely to worsen a collective action problem
because companies are less likely to expect that their coope ration will be reciprocated (high trust). Build-
ing trust between AI companies can therefore decrease the se verity of collective action problems. An
AI race development in which the expected value of winning is much greater than the expected value
of losing is also likely to have a worse collective action pro blem (low exposure and low advantage).55
If close frontrunners worsen collective action problems, A I companies may agree to take steps to avoid
engaging in a harmful race to the bottom on safety. For exampl e, citing concerns about race dynamics,
OpenAI (2018) have stated that “if a value-aligned, safety- conscious project comes close to building
AGI before we do, we commit to stop competing with and start as sisting this project.”
The mechanisms to incentivize investment in product safety outlined in the previous section—market
forces, regulation, and liability—all operate to prevent c ollective action problems for product safety.
Consumers often pay less for products that are unsafe (low ad vantage and shared downside) and more
for safe products (shared upside and low exposure). Governm ent regulation either removes the option
to underinvest in safety or increases the cost of underinves ting in safety via sanctions and ﬁnes (low
advantage and shared downside). And the possibility of bein g held liable for harms caused by unsafe
products decreases the expected value of underinvesting in safety to get ahead (low advantage and shared
downside).
Market forces, regulation, and liability are all mechanism s operating outside of the AI industry that
affect the incentives that AI companies have to develop resp onsibly. But if responsible AI development
is a collective action problem then each AI company expects t o beneﬁt from being in a better equilibrium
and therefore has an incentive to ensure that the AI industry itself collectively coordinates to maintain
some acceptable level of responsible development. Compani es should be willing to invest in cooperative
mechanisms to the degree that these mechanisms increase the likelihood that they will be able to capture
the cooperation surplus: the additional expected value tha t cooperation would generate for them.56
This means that industry-led mechanisms like greater self- regulation could also be developed to incen-
tivize responsible AI development.57
2.5 Summary
In this section we argued that responsible AI development ma y take the form of a collective action
problem. We also identiﬁed ﬁve factors that generally incre ase the likelihood of mutual cooperation
and can help solve such collective action problems. In the ne xt section we will translate this into more
concrete suggestions for increasing cooperation on safety between AI companies.
54This is consistent with the conclusion of Armstrong et al. (2 016) that frontrunners will take more risks if they
have a close competitor. The claim that competition is more i ntense among close competitors has also been made
in the literature on R&D races also (Grossman and Shapiro, 19 85; Harris and Vickers, 1987).
55We have focused on cases that involve cooperation, but we can use the more cooperation-neutral factors in note
43 to look at the expected cost to one company if another compa ny wins regardless of the degree of cooperation
between the two companies. To give another example of this, A rmstrong et al. (2016) discuss the level of enmity
between companies. Higher enmity would then be expected to w orsen a collective action problem by increasing
the cost of losing the race to the other company (low exposure ).
56This concept is similar to the Harsanyi dividend, which quan tiﬁes the value created by a coalition (Harsanyi,
1963). The value of trust as a commodity is explored by Dasgup ta (2000). Companies should be willing to pay
more for credible demonstrations of their intention to coop erate.
57King and Lenox (2000) highlights the difﬁculties of self-re gulation by looking at the chemical industry’s Re-
sponsible Care program. A self-regulatory program that is c onsidered more successful, however, is the Institute of
Nuclear Power Operations (INPO). See Coglianese and Mendel son (2010) for an analysis of both.
133 Strategies to improve AI industry cooperation on safety
In the previous section, we argued ﬁve factors make it more li kely that AI companies will cooperate
if they are faced with a collective action problem: (1) being more conﬁdent that others will cooperate,
(2) assigning a higher expected value to mutual cooperation , (3) assigning a lower expected cost to
unreciprocated cooperation, (4) assigning a lower expecte d value to not reciprocating cooperation, (5)
assigning a lower expected value to mutual defection.
These ﬁve factors give high-level direction regarding how t o ensure that the fruits of cooperation in AI
are realized. However, it is not always obvious what these ﬁv e factors mean in the real world, so there
is a need for translating these factors into tangible policy strategies that various actors can implement in
order to improve cooperation prospects.
It is impossible to prescribe such strategies fully in advan ce, because we lack information about the
future which would be needed in order to make informed future decisions, and because a particular
policy proposal could be effective if well-implemented but counterproductive if poorly executed. How-
ever, while detailed, long-term policy prescriptions woul d be premature today, there are several coarse-
grained strategies that seem robustly desirable even if som e of the low-level details require research,
dialogue, and passage of time before they can be clariﬁed.
We believe that the four strategies we identify in this secti on are robustly desirable in the sense that they
all have substantial beneﬁts with respect to at least one of t he factors above, and are unlikely to be very
harmful with respect to the others.
3.1 Promote accurate beliefs about the opportunities for co operation
As noted in prior sections, there are multiple competing con ceptions of AI development. In cases where
people are demonstrably uninformed about key aspects of AI d evelopment, it is likely beneﬁcial to
correct them, and more generally for stakeholders to make nu anced public statements consistent with
the spirit of AI development that involves cooperation on no rms of responsible development.
Some misconceptions that should be corrected in order to imp rove prospects for such cooperation
include incorrect beliefs that safety and security risks ca n be safely ignored (Brundage, Avin, et al.,
2018; Amodei, Olah, et al., 2016; Ortega, Maini, et al., 2018 ), an unwarranted focus on relative gains
and losses instead of absolute gains and losses (shared upsi de, low exposure, low advantage, shared
downside), and mistaken belief in interests being more misa ligned than they are (low exposure and low
advantage), In addition to correcting speciﬁc misconcepti ons, there is also likely value in proactively
informing people about the case for cooperating on responsi ble development generally.
For example, recent years have seen substantial effort by re searchers and activists to highlight the biases
being learned by deployed AI systems in critical societal do mains such as criminal justice and in widely
used technological platforms such as recommender systems. This work has highlighted the risks of
incautious development to a large and growing swathe of the A I community. Similarly, concerns have
been raised about both the bias, efﬁcacy, and other properti es of medical AI systems, as well as self-
driving vehicles and other emerging technologies. Analyzi ng and communicating these sorts of risks
is critical for generating interest in cooperation among a s ufﬁciently wide range of actors, as well as in
identifying appropriate norms around research, publicati on, and deployment given the safety risks and
the ways of mitigating them that have been identiﬁed.
In many cases, common knowledge that multiple parties share a concern or interest can be critical for
the initiation of cooperation, and a misconception that par ties lack such a shared concern or interest
could be damaging to cooperation on issues like safety. Avoi ding such misunderstanding may be partic-
ularly important in the case of international cooperation o n responsible AI development across distinct
countries with different languages and cultural frames of r eference.
Propagating accurate information about existing beliefs c an also be valuable, as it allows multiple par-
ties to stabilize their expectations. For example, the Asil omar AI Principles (Future of Life Institute,
2017) commit the many signatories to arms race avoidance, an d various statements of principles before
and after this have similarly committed many actors to vario us (admittedly still abstract) cooperative
statements and actions. Expanding the breadth and depth of s uch dialogue, especially across cultural
and language boundaries, will be critical in fostering unde rstanding of the large gains from mutual
14responsible development (shared upside) and the large loss es from mutual irresponsible development
(shared downside), and in establishing common knowledge th at such understanding exists (high trust).
It is possible to create positive spirals of trust, in which a n increase in one party’s trust causes the trusted
party to increase their trust in turn. We can also stumble int o negative trust spirals, however, in which a
loss of trust leads to further distrust between parties. It i s therefore also important to avoid feeding into
unnecessarily adversarial rhetoric about AI development, lest it become self-fulﬁlling (Kreps, 2019).
3.2 Collaborate on shared research and engineering challen ges
On a range of possible research challenges—from basic AI res earch to applied AI projects to AI safety
and security research—it can be beneﬁcial for multiple part ies to actively pool resources and ideas, pro-
vided this can be done in a way that is procompetive and compli ant with antitrust laws (FTC/DoJ, 2000),
does not raise security concerns for the companies particip ating, and so on.
Joint research can provide value for cooperation via useful technical insights (such as solutions to safety
problems; low exposure and low advantage), stabilizing exp ectations regarding who is working on what
via public information about joint investments as well as in terpersonal dialogue (versus work being
more shrouded in secrecy; high trust and shared downside), c oncretizing the joint upsides of AI (e.g.
AI for good collaborations; shared upside), and facilitati ng more societally beneﬁcial publication and
deployment decisions by various actors (e.g. via collabora tive analysis of the risks of speciﬁc systems;
shared upside).58Note that we refer speciﬁcally here to active and explicit re search collaboration, of
which some already occurs, alongside a much greater amount o f implicit collaboration on AI research
that already exists due to the high degree of openness in the A I research community.
Active and explicit research collaboration in AI, especial ly across institutional and national borders, is
currently fairly limited in quantity, scale, and scope. Thi s is for a range of reasons. In order to maintain
legitimate academic and industrial competition, research ers or their managers may be averse to publish-
ing certain research outputs early or at all. And research id eas, results, datasets, and code can be hard to
disentangle from proprietary product plans and technical i nfrastructure. Furthermore, safety or security
considerations can in some cases make the joint analysis of a particular system more challenging than
it would otherwise be (Radford, Wu, et al., 2019). There are a lso linguistic and logistical barriers to
collaborating across long distances and across different c ultures and languages.
While we acknowledge that such challenges exist, we advocat e a more thorough mapping of possible
collaborations across organizational and national border s, with particular attention to research and en-
gineering challenges whose solutions might be of wide utili ty. Areas to consider might include joint
research into the formal veriﬁcation of AI systems’ capabil ities and other aspects of AI safety and
security with wide application; various applied “AI for goo d” projects whose results might have wide-
ranging and largely positive applications (e.g. in domains like sustainability and health); coordinating
on the use of particular benchmarks; joint creation and shar ing of datasets that aid in safety research; and
joint development of countermeasures against global AI-re lated threats such as the misuse of synthetic
media generation online.
3.3 Open up more aspects of AI development to appropriate ove rsight and feedback
Openness about one’s beliefs, actions, and plans is critica l to establishing trust generally. In the case
of AI development, those building and deploying AI systems n eed to provide information about their
development process so that users can make informed decisio ns. Likewise, governments need to be able
to appropriately oversee safety-critical AI systems, and ( in the absence of relevant regulation) companies
need to be able to provide information to one another that sho ws they are following appropriate norms.
The general appeal of openness for cooperation-related rea sons does not imply that all aspects of AI
development should always be open, and as AI systems become m ore capable, it will be increasingly
58For example, OpenAI’s approach to the release of the GPT-2 la nguage model family (Radford, Wu, et al.,
2019) involves staged release, in which a model is released i ncrementally due to safety and security concerns, and
partnership-based sharing, in which a model is shared with a small number of research partners to enable research
on that system without necessarily requiring broad-based a ccess. This experiment in responsible publication, and
others like it such as the Allen Institute for Artiﬁcial Inte lligence and the University of Washington’s approach on
their Grover family of language models, may help to “derisk” this particular form of research-level collaboration
discussed in the next subsection.
15important to decide responsibly what should and shouldn’t b e made open (Brundage, Avin, et al., 2018;
Bostrom, 2017a; Krakovna, 2016). Full transparency is prob lematic as an ideal to strive for, in that
it is neither necessary nor sufﬁcient for achieving account ability in all cases (Desai and Kroll, 2017;
Ananny and Crawford, 2018). Further, some information abou t AI development cannot or should not
be shared for reasons of safety, security, ethics, or law. Fo r example, AI developers might legitimately
be wary of releasing code that is intimately tied to propriet ary infrastructure, and should certainly be
wary of releasing private data as well as AI systems that are e asily amenable to misuse.
Given that full openness is rarely called for, but that some o penness is required for building trust, there
is a need for continuing effort to implement existing modes o f trust-building in AI, as well as to dis-
cover new ones. Different mechanisms for achieving opennes s regarding how AI systems are developed
and operated include, e.g., publicizing decision-making p rinciples and processes, explaining publica-
tion/release decisions, sharing accessible information a bout how particular AI systems and broad classes
of AI systems work, allowing external visitors to the lab, an d opening up individual AI systems to de-
tailed scrutiny (e.g. via bug bounties or open sourcing).
Such openness is critical in allowing reputation to play its stabilizing role in cooperation. Indeed, some
actors have explicitly pointed to the challenges of monitor ing the development and use of lethal au-
tonomous weapons as as a reason not to agree to strict rules, s uggesting that the inability to track others’
behavior reliably could be a bottleneck on some forms of mutu ally beneﬁcial cooperation (e.g. joint
restraints on weapons development). In cases such as this, a richer set of tools for opening up actors to
critical scrutiny and feedback (while managing the associa ted risks) would be useful, and we encourage
continued exploration of approaches such as those mentione d above as well as others in order to widen
the range of cooperative actions available to AI developers .
In combination, the appropriate application of transparen cy mechanisms such as these should reduce the
severity of concerns about others behaving irresponsibly ( low exposure), reduce the temptation to defect
in partially competitive situations (low advantage), and i ncrease conﬁdence that others’ statements about
their behavior are accurate (high trust). Openness is a part icularly powerful strategy, and applicable to a
wider range of cooperation problems, if it can be gradually r atcheted up in an iterative fashion between
parties, as opposed to happening all at once. This gradual ap proach can reduce the temptation to defect
at any particular stage (low advantage) and increase conﬁde nce in others cooperating (shared downside).
3.4 Incentivize adherence to high standards of safety
Cooperative actors might want to introduce additional ince ntives (reward and/or punishment) related to
responsible AI development beyond those that exist today, o r would exist by default in the future. E.g.
such actors might strongly value compliance with certain no rms intrinsically, and prefer that those who
comply with appropriate norms be rewarded; or one might want to deliberately bring about an incentive
for oneself to act in a certain way, as a commitment mechanism ; one might also want to use incentives
as a complement to other governance tools such as monitoring of behavior and direct regulation; and
one might want to generally inﬂuence the incentives of many a ctors in a particular direction, and support
policies that bring this about.
There are several categories of incentives that one might wa nt to consider in this context. Creating
incentives for key actors to act cooperatively, if done effe ctively, would help with all ﬁve factors simul-
taneously. Potential incentives include:
•Social incentives (e.g. valorizing or criticizing certain behaviors related to AI development)
can inﬂuence different companies’ perceptions of risks and opportunities
•Economic incentives (induced by governments, philanthrop ists, industry, or consumer behav-
ior) can increase the share of high-value AI systems in parti cular markets or more generally,
and increase attention to particular norms59
•Legal incentives (i.e. proscribing certain forms of AI deve lopment with ﬁnancial or greater
penalties) could sharply reduce temptation by some actors t o defect in certain ways.
•Domain-speciﬁc incentives of particular relevance to AI (e .g. early access to the latest genera-
tion of computing power) could be used to encourage certain f orms of behavior.
59Mutual agreements to distribute the economic gains from win ning the AI development (O’Keefe et al.,
forthcoming 2019) could also decrease the severity of colle ctive action problems.
16As argued in each case above, these strategies are robustly d esirable from the perspective of enabling
cooperation, but our articulation of them leaves many quest ions unanswered. In particular, sharpening
these recommendations and adapting them over time will requ ire technical and social scientiﬁc research,
creative institutional design, and bold policy experiment ation, e.g. via regulatory markets as discussed
in Hadﬁeld and Clark (2019).
4 Conclusion and Future Directions
In this paper we have argued that competition between AI comp anies could create a collective action
problem for responsible AI development. We have identiﬁed ﬁ ve key factors that make it more likely
that companies will cooperate on responsible development: high trust, shared upside, low exposure, low
advantage, and shared downside. We have shown that these ﬁve factors can help us to identify strategies
to help AI companies develop responsibly and thereby realiz e the gains from cooperation. This also has
important positive externalities for consumers and the gen eral public.
If our analysis is on the right track then it is best thought of as the beginning of a program of research,
rather than the last word on the subject. Much work needs to be done to identify whether collective
action problems for responsible AI development will occur i f we vary who is developing AI, how many
entities are developing AI, what systems they are developin g, and so on. More work must also be done
to identify and evaluate strategies that can prevent or miti gate these kinds of collective action problems
across a wide range of possible scenarios.
The possible future research directions on this issue are br oad and we do not aim to provide a compre-
hensive list of them here, but examples of potentially fruit ful research questions include:
1. How might the competitive dynamics of industry developme nt of AI differ from government-
led or government-supported AI development?
2. What is the proper role of legal institutions, government s, and standardization bodies in re-
solving collective action problems between companies, par ticularly if those collective action
problems can arise between companies internationally?
3. What further strategies can be discovered or constructed to help prevent collective action prob-
lems for responsible AI development from forming, and to hel p solve such problems if they do
arise? What lessons can we draw from history or from contempo rary industries?
4. How might competitive dynamics be affected by particular technical developments, or expec-
tations of such developments?
As we noted at the outset, there is substantial uncertainty a bout the nature and pace of developments in
AI. If the impact of AI systems on society is likely to increas e, however, then greater attention must be
paid to ensuring that the systems being developed and releas ed are safe, secure, and socially beneﬁcial.
In this paper we argued that existing incentives to develop A I responsibly may be weaker than is ideal,
and that this may be compounded by competitive pressure betw een companies, leading to a collective
action problem on the responsible development of AI.
That such collective action problems will arise or that they will be maintained if they do arise is far from
a foregone conclusion, however. Finding ways of preventing and solving these problems may require
new ways of building trust in novel technological contexts, and in some cases to assume some risk in
the expectation that others will reciprocate in turn. While intellectually and politically challenging, we
think such efforts are integral to realizing the positive-s um potential of AI.
Acknowledgments
We are grateful to Michael Page, Jack Clark, Larissa Schiavo , Carl Shulman, Luke Muehlhauser, Ge-
offrey Irving, Sarah Kreps, Paul Scharre, Michael Horowitz , Robert Trager, Tamay Besiroglu, Helen
Toner, Cullen O’Keefe, Rebecca Crootof, Ben Garﬁnkel, Adam Gleave, Jasmine Wang, and Toby
Shevlane for valuable feedback on earlier versions of this p aper.
17References
Daron Acemoglu and Pascual Restrepo. Artiﬁcial intelligen ce, automation and work. Technical report,
National Bureau of Economic Research, 2018.
AI Impacts. Likelihood of discontinuous progress around th e development of agi. https://aiimpacts.
org/likelihood-of-discontinuous-progress-around-the -development-of-agi/, Feb 2018. (Accessed on
03/12/2019).
Dario Amodei, Chris Olah, et al. Concrete problems in ai safe ty.arXiv preprint arXiv:1606.06565 ,
2016.
Mike Ananny and Kate Crawford. Seeing without knowing: Limi tations of the transparency ideal and
its application to algorithmic accountability. New Media & Society , 20(3):973–989, 2018.
Wilma Rose Q Anton, George Deltas, and Madhu Khanna. Incenti ves for environmental
self-regulation and implications for environmental perfo rmance. Journal of environmental
economics and management , 48(1):632–654, 2004.
Reiko Aoki. R&d competition for product innovation: An endl ess race. The American Economic
Review , 81(2):252–256, 1991.
Stuart Armstrong, Nick Bostrom, and Carl Shulman. Racing to the precipice: a model of artiﬁcial
intelligence development. AI & Society , 31(2):201–206, 2016.
R. M. Axelrod. The Evolution of Cooperation . Basic books. Basic Books, 1984. ISBN
9780465021215. URL https://books.google.com/books?id= NJZBCGbNs98C.
Robert Axelrod. More effective choice in the prisoner’s dil emma. Journal of Conﬂict Resolution , 24
(3):379–403, 1980.
Omri Ben-Shahar. Should products liability be based on hind sight? Journal of Law, Economics, &
Organization , pages 325–357, 1998.
Maria Bengtsson and Sören Kock. ” coopetition” in business n etworks—to cooperate and compete
simultaneously. Industrial marketing management , 29(5):411–426, 2000.
Nick Bostrom. Strategic implications of openness in ai deve lopment. Global Policy , 8(2):135–148,
2017a.
Nick Bostrom. Superintelligence . Oxford University Press, 2017b.
Yves Breitmoser, Jonathan HW Tan, and Daniel John Zizzo. Und erstanding perpetual r&d races.
Economic Theory , 44(3):445–467, 2010.
Timothy F Bresnahan. Competition and collusion in the ameri can automobile industry: The 1955 price
war. The Journal of Industrial Economics , pages 457–482, 1987.
Miles Brundage, Shahar Avin, et al. The Malicious Use of Arti ﬁcial Intelligence: Forecasting,
Prevention, and Mitigation. arXiv e-prints , art. arXiv:1802.07228, Feb 2018.
Erik Brynjolfsson, Daniel Rock, and Chad Syverson. Artiﬁci al intelligence and the modern
productivity paradox: A clash of expectations and statisti cs. In The economics of artiﬁcial
intelligence: An agenda . University of Chicago Press, 2018.
Jian Cai, Kent Cherny, Todd Milbourn, et al. Compensation an d risk incentives in banking and ﬁnance.
Economic Commentary , (2010-13), 2010.
G. Calabresi. The Costs of Accidents: A Legal and Economic Analysis . Legal and Economic Analysis.
Yale University Press, 1970. ISBN 9780300011142. URL https ://books.google.com/books?
id=PEMLlb7XxyMC.
John L. Campbell. Why would corporations behave in socially responsible ways? an institutional
theory of corporate social responsibility. Academy of Management Review , 32(3):946–967, jul 2007.
doi: 10.5465/amr.2007.25275684. URL https://doi.org/10 .5465/amr.2007.25275684.
18D Chalmers. The singularity: A philosophical analysis. Science ﬁction and philosophy: From time
travel to superintelligence , pages 171–224, 2009.
Sylvain Chassang. Building routines: Learning, cooperati on, and the dynamics of incomplete
relational contracts. American Economic Review , 100(1):448–65, 2010.
Yongmin Chen and Xinyu Hua. Competition, product safety, an d product liability. The Journal of Law,
Economics, and Organization , 33(2):237–267, 2017.
Paul Christiano. AI “safety” vs “control” vs “alignment”. A I Alignment, https://ai-alignment.com/
ai-safety-vs-control-vs-alignment-2a4b42a863cc, Nov 2 016. (Accessed on 03/13/2019).
Paul Christiano. Hyperbolic growth. The sideways view , https://sideways-view.com/2017/10/04/
hyperbolic-growth/, Oct 2017. (Accessed on 03/21/2019).
Paul Christiano. Takeoff speeds. The sideways view , https://sideways-view.com/2018/02/24/
takeoff-speeds/, Feb 2018. (Accessed on 03/21/2019).
Jack Clark and Dario Amodei. Faulty reward functions in the w ild. Open AI Blog, https://openai.com/
blog/faulty-reward-functions/, Dec 2016. (Accessed on 03 /13/2019).
Iain M Cockburn, Rebecca Henderson, and Scott Stern. The imp act of artiﬁcial intelligence on
innovation. Technical report, National Bureau of Economic Research, 2018.
Cary Coglianese and Evan Mendelson. Meta-regulation and se lf-regulation. Regulation , pages 12–11,
2010.
Molly Cohen and Arun Sundararajan. Self-regulation and inn ovation in the peer-to-peer sharing
economy. U. Chi. L. Rev. Dialogue , 82:116, 2015.
Consumer Reports. V olkswagen and Audi Recall for Brake Issu es.
https://www.consumerreports.org/car-recalls-defects /
volkswagen-audi-recall-sedans-suvs-for-brake-issues /, Aug 2018. (Accessed on 03/12/2019).
Nina W Cornell, Roger G Noll, and Barry R Weingast. Safety reg ulation. 1976.
Missy Cummings. Artiﬁcial intelligence and the future of warfare . Chatham House for the Royal
Institute of International Affairs, 2017.
Maeve Cushen, J Kerry, M Morris, Malco Cruz-Romero, and Enda Cummins. Nanotechnologies in the
food industry–recent developments, risks and regulation. Trends in food science & technology , 24
(1):30–46, 2012.
Allan Dafoe. AI Governance: A Research Agenda. Governance of AI Program, Future of Humanity
Institute, University of Oxford: Oxford, UK , 2018.
Carl J Dahlman. The problem of externality. The journal of law and economics , 22(1):141–162, 1979.
Partha Dasgupta. Trust as a commodity. Trust: Making and breaking cooperative relations , 4:49–72,
2000.
Partha Dasgupta and Joseph Stiglitz. Uncertainty, industr ial structure, and the speed of r&d. The Bell
Journal of Economics , pages 1–28, 1980.
Andrew F Daughety and Jennifer F Reinganum. Product safety: liability, R&D, and signaling. The
American Economic Review , pages 1187–1206, 1995.
Andrew F Daughety, Jennifer F Reinganum, et al. Economic ana lysis of products liability: theory.
Research Handbook on the Economics of Torts , pages 69–96, 2013.
Andrew F Daughety, Jennifer F Reinganum, et al. Market struc ture, liability, and product safety.
Handbook of Game Theory and Industrial Organization, Volum e II: Applications , 2:225, 2018.
Lucas W Davis and Catherine Wolfram. Deregulation, consoli dation, and efﬁciency: Evidence from us
nuclear power. American Economic Journal: Applied Economics , 4(4):194–225, 2012.
19Deven R. Desai and Joshua A. Kroll. Trust but verify: A guide t o algorithms and the law. Harvard
Journal of Law & Technology , 31(1):1, 2017.
Timothy M Devinney. Is the socially responsible corporatio n a myth? the good, the bad, and the ugly
of corporate social responsibility, 2009.
Ulrich Doraszelski. An r&d race with knowledge accumulatio n.Rand Journal of economics , pages
20–42, 2003.
Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608 , 2017.
William R Dunn. Designing safety-critical computer system s.Computer , 36(11):40–46, 2003.
Ellinor Ehrnberg. On the deﬁnition and measurement of techn ological discontinuities. Technovation ,
15(7):437–452, sep 1995. doi: 10.1016/0166-4972(95)9659 3-i. URL https://doi.org/10.1016/
0166-4972(95)96593-i.
Jon Elster. Rationality, morality, and collective action. Ethics , 96(1):136–155, 1985.
Olivia J Erdélyi and Judy Goldsmith. Regulating artiﬁcial i ntelligence: Proposal for a global solution.
InProceedings of the 2018 AAAI/ACM Conference on AI, Ethics, a nd Society , pages 95–101. ACM,
2018.
Daniel C Esty and Damien Geradin. Regulatory competition and economic integration: compara tive
perspectives . Oxford University Press, 2001.
Federal Trade Commission and U.S. Department of Justice. An titrust guidelines for collaborations
among competitors. https://www.ftc.gov/sites/default/ ﬁles/attachments/dealings-competitors/
ftcdojguidelines.pdf, April 2000.
Future of Life Institute. AI Principles. https://futureoﬂ ife.org/ai-principles/?cn-reloaded=1, 2017.
(Accessed on 04/01/2019).
Shanti Gamper-Rabindran and Stephen R Finger. Does industr y self-regulation reduce pollution?
responsible care in the chemical industry. Journal of Regulatory Economics , 43(1):1–30, 2013.
Philipp Genschel and Thomas Plumper. Regulatory competiti on and international co-operation.
Journal of European Public Policy , 4(4):626–642, 1997.
Robert Gibbons and Rebecca Henderson. What do managers do?: Exploring persistent performance
differences among seemingly similar enterprises . Harvard Business School, 2012.
Joseph T Gilbert and Philip H Birnbaum-More. Innovation tim ing advantages: From economic theory
to strategic application. Journal of Engineering and Technology Management , 12(4):245–266, 1996.
Irving John Good. Speculations concerning the ﬁrst ultrain telligent machine. In Advances in
computers , volume 6, pages 31–88. Elsevier, 1966.
Gene M Grossman and Carl Shapiro. Dynamic R&D competition, 1 985.
Joseph P Guiltinan and Gregory T Gundlach. Aggressive and pr edatory pricing: A framework for
analysis. Journal of marketing , 60(3):87–102, 1996.
Neil Gunningham and Joseph Rees. Industry self-regulation : an institutional perspective. Law &
Policy , 19(4):363–414, 1997.
Gillian Hadﬁeld and Jack Clark. Regulatory markets for AI sa fety. Safe Machine Learning workshop
at ICLR , 2019.
G.K. Hadﬁeld. Rules for a Flat World: Why Humans Invented Law and how to Rein vent it for a
Complex Global Economy . Titolo collana. Oxford University Press, 2017. ISBN 97801 99916528.
URL https://books.google.com/books?id=TBYBDQAAQBAJ.
Christopher Harris and John Vickers. Racing with uncertain ty.The Review of Economic Studies , 54(1):
1, jan 1987. doi: 10.2307/2297442. URL https://doi.org/10 .2307/2297442.
20John C Harsanyi. A simpliﬁed bargaining model for the n-pers on cooperative game. International
Economic Review , 4(2):194–220, 1963.
Catherine Hausman. Corporate incentives and nuclear safet y.American Economic Journal: Economic
Policy , 6(3):178–206, 2014.
Douglas D. Heckathorn. Collective action and the second-or der free-rider problem. Rationality and
Society , 1(1):78–100, 1989. doi: 10.1177/1043463189001001006. U RL https://doi.org/10.1177/
1043463189001001006.
Spencer Henson and Julie Caswell. Food safety regulation: a n overview of contemporary issues. Food
policy , 24(6):589–603, 1999.
Anthony Heyes. Is environmental regulation bad for competi tion? a survey. Journal of Regulatory
Economics , 36(1):1–28, 2009.
Fred Matthew Hohman, Minsuk Kahng, Robert Pienta, and Duen H orng Chau. Visual analytics in
deep learning: An interrogative survey for the next frontie rs.IEEE transactions on visualization and
computer graphics , 2018.
Katharina Holzinger. The problems of collective action: A n ew approach. MPI Collective Goods
Preprint , (2003/2), 2003.
Keith N Hylton. The law and economics of products liability. Notre Dame L. Rev. , 88:2457, 2012.
Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safe ty via debate. arXiv preprint
arXiv:1805.00899 , 2018.
Neil Johnson, Guannan Zhao, Eric Hunsader, Hong Qi, Nichola s Johnson, Jing Meng, and Brian
Tivnan. Abrupt rise of new machine ecology beyond human resp onse time. Scientiﬁc reports , 3:
2627, 2013.
By Kenneth L Judd, Karl Schmedders, and ¸ Sevin Yeltekin. Opt imal rules for patent races.
International Economic Review , 53(1):23–52, 2012.
Andrew A King and Michael J Lenox. Industry self-regulation without sanctions: The chemical
industry’s responsible care program. Academy of management journal , 43(4):698–716, 2000.
Steven Klepper. Entry, exit, growth, and innovation over th e product life cycle. The American
Economic Review , 86(3):562–583, 1996. ISSN 00028282. URL http://www.jsto r.org/stable/
2118212.
Charles D. Kolstad, Thomas S. Ulen, and Gary V . Johnson. Ex po st liability for harm vs. ex ante safety
regulation: Substitutes or complements? The American Economic Review , 80(4):888–901, 1990.
ISSN 00028282. URL http://www.jstor.org/stable/2006714 .
Victoria Krakovna. Clopen ai: Openness in different aspect s of ai development. https://vkrakovna.
wordpress.com/2016/08/01/clopen-ai-openness-in-diff erent-aspects-of-ai-development/, Aug 2016.
(Accessed on 06/27/2019).
Victoria Krakovna. Speciﬁcation gaming examples in ai. htt ps://vkrakovna.wordpress.com/2018/04/
02/speciﬁcation-gaming-examples-in-ai/, April 2018. (A ccessed on 05/14/2019).
Sarah Kreps. Is this a sputnik moment for artiﬁcial intellig ence? or why the cold war past should not
be prologue (unpublished manuscript). 2019.
Andrew H Kydd. Trust and mistrust in international relations . Princeton University Press, 2007.
William M. Landes and Richard A. Posner. A positive economic analysis of products liability. The
Journal of Legal Studies , 14(3):535–567, 1985. ISSN 00472530, 15375366. URL http:/ /www.jstor.
org/stable/724257.
Howard Latin. Good science, bad regulation, and toxic risk a ssessment. Yale J. on Reg. , 5:89, 1988.
Edward Lazear. Intergenerational externalities. Canadian Journal of Economics , pages 212–228, 1983.
21Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A Ortega , Tom Everitt, Andrew Lefrancq, Laurent
Orseau, and Shane Legg. Ai safety gridworlds. arXiv preprint arXiv:1711.09883 , 2017.
M Lenox. The prospects for industry self-regulation of envi ronmental externalities. Making global
regulation effective: What role for self-regulation , 2007.
François Lévêque. Externalities, collective goods and the requirement of a state’s intervention in
pollution abatement. In Voluntary Approaches in Environmental Policy , pages 17–26. Springer,
1999.
Marvin B Lieberman and David B Montgomery. First-mover adva ntages. Strategic management
journal , 9(S1):41–58, 1988.
George J Mailath, Larry Samuelson, and J Swinkels. Extensiv e form reasoning in normal form games.
Foundations in Microeconomic Theory , page 451, 1991.
Constantinos C Markides and Paul A Geroski. Fast second: How smart companies bypass radical
innovation to enter and dominate new markets , volume 325. John Wiley & Sons, 2004.
Talya Minsberg. The newfound power of tech workers. The New York Times , https://www.nytimes.
com/2019/03/02/business/tech-employees-protests.htm l, Mar 2019. (Accessed on 03/14/2019).
Kara Morgan. Development of a preliminary framework for inf orming the risk analysis and risk
management of nanoparticles. Risk Analysis: An International Journal , 25(6):1621–1635, 2005.
Luke Muehlhauser and Bill Hibbard. Viewpoint exploratory e ngineering in artiﬁcial intelligence.
Communications of the ACM , 57:32–34, 09 2014. doi: 10.1145/2644257.
Martin A Nowak. Five rules for the evolution of cooperation. science , 314(5805):1560–1563, 2006.
Walter Y Oi et al. The economics of product safety. Bell Journal of Economics , 4(1):3–28, 1973.
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and
Alexander Mordvintsev. The building blocks of interpretab ility. Distill , 3(3):e10, 2018.
OpenAI. Openai charter. https://openai.com/charter/, Ap ril 2018. (Accessed on 04/01/2019).
Pedro A. Ortega, Vishal Maini, et al. Building safe artiﬁcia l intelligence: speciﬁcation, robustness, and
assurance. https://medium.com/@deepmindsafetyresearc h/
building-safe-artiﬁcial-intelligence-52f5f75058f1, S ep 2018. (Accessed on 03/13/2019).
Cullen O’Keefe. Antitrust-compliant ai industry self-reg ulation. forthcoming 2019.
Cullen O’Keefe et al. The windfall clause. forthcoming 2019 .
Christine Parker. Meta-regulation: legal accountability for corporate social responsibility. 2007.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and
calibration. In Advances in Neural Information Processing Systems , pages 5680–5689, 2017.
A. Mitchell Polinsky and Steven Shavell. The uneasy case for product liability. Harv. L. Rev. , 123:
1437, 2009.
Adrien Querbes and Koen Frenken. Evolving user needs and lat e-mover advantage. Strategic
organization , 15(1):67–90, 2017.
Alec Radford, Jeffrey Wu, et al. Language models are unsuper vised multitask learners. OpenAI Blog ,
2019.
Mooweon Rhee and Pamela R Haunschild. The liability of good r eputation: A study of product recalls
in the us automobile industry. Organization Science , 17(1):101–117, 2006.
DR Robinson and DJ Goforth. Conﬂict, no conﬂict, common inte rests, and mixed interests in 2 ×2
games. Deparament of Mathematics and Computer Science , 2005.
Paul H Rubin. Markets, tort law, and regulation to achieve sa fety. Cato J. , 31:217, 2011.
22Maurice Schellekens. Self-driving cars and the chilling ef fect of liability law. Computer Law &
Security Review , 31(4):506–517, 2015.
T.C. Schelling. Micromotives and Macrobehavior . W. W. Norton, 2006 [1978]. ISBN 9780393069778.
URL https://books.google.com/books?id=DenWKRgqzWMC.
Thomas C Schelling. The strategy of conﬂict . Harvard university press, 1980.
Paul Seabright. Managing local commons: theoretical issue s in incentive design. Journal of economic
perspectives , 7(4):113–134, 1993.
Michael Segal. How automation is changing work. Nature , 563(7733):S132–S135, Nov 2018. doi:
10.1038/d41586-018-07501-y. URL https://doi.org/10.10 38/d41586-018-07501-y.
Murray Shanahan. The technological singularity . MIT Press, 2015.
Steven Shavell. Liability for harm versus regulation of saf ety.The Journal of Legal Studies , 13(2):
357–374, 1984.
Eytan Sheshinski. Price, quality and quantity regulation i n monopoly situations. Economica , 43(170):
127–137, 1976.
Brad Smith. Facial recognition technology: The need for pub lic regulation and corporate
responsibility. https://blogs.microsoft.com/on-the-i ssues/2018/07/13/
facial-recognition-technology-the-need-for-public-r egulation-and-corporate-responsibility/, Jul
2018a. (Accessed on 03/20/2019).
Brad Smith. Facial recognition: It’s time for action. Microsoft on the Issues , https://blogs.microsoft.
com/on-the-issues/2018/12/06/facial-recognition-its -time-for-action/, Dec 2018b. (Accessed on
03/20/2019).
Joseph Stiglitz. Regulation and failure. New perspectives on regulation , 576, 2009.
Cass R Sunstein. Irreversible and catastrophic. Cornell L. Rev. , 91:841, 2005.
The Atlantic. Who is liable for a death caused by a self-drivi ng car? https://www.theatlantic.com/
technology/archive/2018/03/can-you-sue-a-robocar/55 6007/, Mar 2018. (Accessed on 03/12/2019).
Wenpin Tsai. Social structure of “coopetition” within a mul tiunit organization: Coordination,
competition, and intraorganizational knowledge sharing. Organization science , 13(2):179–190,
2002.
Hayley Tsukayama and Jamie Williams. If a pre-trial risk ass essment tool does not satisfy these
criteria, it needs to stay out of the courtroom | electronic f rontier foundation. EFF, https://www.eff.
org/deeplinks/2018/11/
if-pre-trial-risk-assessment-tool-does-not-satisfy- these-criteria-it-needs-stay, Nov 2018. (Accessed
on 03/13/2019).
Roman V . Yampolskiy. Taxonomy of pathways to dangerous AI. arXiv preprint arXiv:1511.03246 ,
2015.
Eliezer Yudkowsky. Intelligence explosion microeconomic s. https://intelligence.org/ﬁles/IEM.pdf,
2013. (Accessed on 03/21/2019).
Remco Zwetsloot and Allan Dafoe. Thinking About Risks From A I: Accidents, Misuse and Structure,
Feb 2019. Lawfare , https://www.lawfareblog.com/
thinking-about-risks-ai-accidents-misuse-and-struct ure (Accessed 03/07/2019).
23