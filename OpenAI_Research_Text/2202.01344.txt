Formal Mathematics Statement Curriculum Learning Stanislas Polu1Jesse Michael Han1Kunhao Zheng2Mantas Baksys3Igor Babuschkin1Ilya Sutskever1 Abstract We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufﬁciently varied difﬁculty, expert iteration is capable of ﬁnding and solving a curriculum of increasingly difﬁcult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads. 1. Introduction Deep learning has enjoyed spectacular success in many domains, including language (Brown et al., 2020; Devlin et al., 2018; Wu et al., 2016), vision (Radford et al., 2021; Tan & Le, 2019), and image generation (Ramesh et al., 2021; Karras et al., 2019). One domain where deep learning has not yet enjoyed a comparable success is in tasks that require extensive planning andsymbolic reasoning , with the exception of two-player games (Silver et al., 2016; 2017; Berner et al., 2019; Vinyals et al., 2019). In such games, deep learning systems exhibit a considerable degree of reasoning, especially when trained with self-play combined with a search procedure such as Monte Carlo Tree Search (MCTS) (Browne et al., 2012). But the resulting reasoning abilities achieved are limited due to the relatively narrow scope of games. As such, theorem proving in interactive proof assistants, or formal mathematics, appears as an interesting game-like domain to tackle due to its increased scope. Like games, formal mathematics has an automated way of determining 1OpenAI2École Polytechnique3University of Cambridge. Correspondence to: Stanislas Polu <spolu@openai.com>. Preprint. Under review.whether a trajectory ( i.e.a proof) is successful ( i.e.formally correct). But the vast scope of formal mathematics means that any strong reasoning result obtained in it will be more meaningful than comparable results in games ( e.g.ﬁnding proofs to mathematical conjectures), and could even be applicable to important practical problems ( e.g. software veriﬁcation). However, tackling formal mathematics involves two main challenges that we must address in order to continue making progress: Inﬁnite action space Not only does formal mathematics have an extremely large search space (like Go for example), it also has an inﬁnite action space. At each step of proof search, the model must choose not from a well-behaved ﬁnite set of actions, but a complex and inﬁnite set of tactics, potentially involving exogenous mathematical terms that have to be generated (e.g., generating a mathematical statement to be used as a witness, an object used steps such as “there exists an x...”, or a cut, the introduction and the chaining of a lemma in the middle of a proof). No direct self-play setup In formal mathematics, a prover is not playing against an opponent but against a set of statements to prove. When faced with a statement that is just too hard, there is no obvious reframing of the formal mathematics setup that will let the prover generate intermediary easier statements to tackle ﬁrst. This asymmetry prevents naive application of the symmetric self-play algorithms commonly used in 2-player games. These two differences make a naive application of reinforcement learning to formal mathematics unlikely to succeed. Past work proposed to address the inﬁnite action space problem by sampling from a language model (Polu & Sutskever, 2020). This paper focuses on this second problem and our basis for addressing it is the observation that the key role of self-play is to provide an unsupervised curriculum. We propose instead to supply auxiliary sets of problem statements (without requiring proofs) of varying difﬁculty. We empirically show that, when the difﬁculty of these auxiliary problems is varied enough, a simple expert iteration procedure is able to solve a curriculum of increasingly difﬁcult problems, eventually generalizing to our target distribution. We show that this works with both automaticallygenerated and manually-curated auxiliary distributions ofarXiv:2202.01344v1  [cs.LG]  3 Feb 2022Formal Mathematics Statement Curriculum Learning problems and leverage this to achieve state-of-the-art on the miniF2F benchmark. Our results suggest that continuous self-improvement in formal mathematics can potentially be reduced to the problem of generating such sets of formal statements, which we have done in part manually in this work, but could eventually be scaled in the future with more automation (such as more domain-speciﬁc statements generator or even informal to formal machine translation). 1.1.miniF2F benchmark In this work, we target the miniF2F (Zheng et al., 2021) benchmark, which consists of 244 validation and 244 test formalized statements of mathematical problems from various competitions. We believe it to be a better measure of mathematical reasoning compared to a formal libraryderived split. Also, the extreme scarcity in formal libraries of this type of problems makes it an ideal test-bed for the expert iteration methodology studied in this paper. 1.2. Contribution Our contributions are the following: we present lean-gym , a simple REPL interface for interacting with the Lean theorem prover; we propose an expert iteration methodology for GPT-f (Polu & Sutskever, 2020) which uses proofs generated by our models as training data to iteratively improve their performance; we demonstrate that, at ﬁxed compute budget, expert iteration outperforms proof search only; we present a synthetic inequality generator and study how expert iteration ﬁnds and solves a curriculum of increasingly difﬁcult problems from a set of generated statements of various difﬁculty; ﬁnally, we present a manually curated set of formalized problem statements and leverage it to achieve state-of-the-art performance on the miniF2F benchmark. 2. Related Work Our work strongly relies on, and can be seen as a natural continuation of the work presented in the original GPT-f paper (Polu & Sutskever, 2020) which studies the use of language models to generate tactics, the PACT paper (Han et al., 2021) which applies GPT-f to Lean and studies the beneﬁts from co-training on self-supervised objectives, and theminiF2F benchmark (Zheng et al., 2021). We present additional related work in Appendix A. 3. Formal Environment We choose Lean (de Moura et al., 2015; lea) as our formal environment. Unlike Metamath (Megill & Wheeler, 2019), which has been studied in the original GPT-f paper (Polu & Sutskever, 2020), Lean beneﬁts from high-level tactics which were shown to be beneﬁcial in the context oftheminiF2F benchmark (Zheng et al., 2021)–Lean proofs are typically 10x shorter than Metamath’s. Also, Lean has recently received a lot of attention from the mathematical community, thanks to projects such as the Perfectoid Spaces (Buzzard et al., 2019) and the Liquid Tensor experiment (Scholze, 2020), and beneﬁts from a vibrant community of hundreds of contributors to its main mathematical library called mathlib . We refer to the PACT paper’s Background section (Han et al., 2021) for a detailed introduction to Lean in the context of neural theorem proving. 3.1.lean-gym In the PACT paper (Han et al., 2021), proof search is performed by the Lean runtime using the LEANSTEPenvironment, with a generic backend interface to models. While easy to use–one just needs to plug in their model–this approach makes it difﬁcult to alter and iterate on the search procedure because it is programmed in Lean (which is not designed or intended for cluster-wide parallelised I/O intensive tasks), and the coupling of the search procedure with the Lean runtime introduces challenges when scaling to a large number of parallel workers. To solve these issues we implemented lean-gym1– a simple REPL interface over the standard input/output implemented in Lean directly. We present lean-gym ’s API and discuss some of its advantages and limitations in Appendix B. 3.2. Proof datasets extraction We rely on the proof extraction methodology presented in the PACT paper (Han et al., 2021) to extract human tactic proof steps from mathlib (thetactic dataset) as well as the various other proof artifacts ( mix1 andmix2 datasets). We also extract mathlib-{train,valid,test} , the set of statements from mathlib along the split proposed in Han et al. (2021) (the validation andtestsplits of tactic, mix1, mix2 being aligned with mathlib-{valid, test} as the splits are determined by declaration name hashes (across all data sources including proof-term mining) as opposed to individual proof steps or data-points). 4. Expert Iteration Expert iteration was introduced in Silver et al. (2017) and broadly consists in iteratively training models on their previously sampled trajectories, to achieve continuous improvement. In this section we present our expert iteration methodology, including the models and pre-training strategies we rely on. 1https://github.com/openai/lean-gymFormal Mathematics Statement Curriculum Learning 4.1. Model We use decoder-only Transformers similar to GPT-3 (Brown et al., 2020). Throughout this paper we focus on a model with 36 layers and 774 million trainable parameters (referred to as the 700m model in the GPT-f paper (Polu & Sutskever, 2020)). 4.2. Pre-Training We pre-train our models successively on GPT-3’s postprocessed version of CommonCrawl (for 300B tokens) and an updated version of WebMath (Polu & Sutskever, 2020) (for 72B tokens) whose mix is presented in Appendix C. 4.3. Training objectives 4.3.1. Proofstep objective The proofstep objective , introduced in Polu & Sutskever (2020), consists in generating a PROOFSTEP (a Lean tactic) given a GOAL (a Lean tactic state). We also condition this objective on the current DECLARATION (a Lean theorem name), which remains the same throughout a proof search: DECL <DECLARATION> GOAL <GOAL> PROOFSTEP <PROOFSTEP> . The rationale for conditioning on the declaration name is to hint our models on the position of the current declaration in themathlib library. It can be considered as a weak proxy signal for the large amount of information not shown to the model (the full environment consisting of the available imports and currently open declarations such as module names, notations, declared instances, ...). The declaration name lets models at least in principle memorize and then retrieve some of that information, knowing that lean-gym errors if a theorem or deﬁnition that is not available in the environment associated with the current declaration is used by tactics generated by our models. Also note that conversely to Polu & Sutskever (2020) and like Han et al. (2021) <GOAL> is not necessarily a single goal but a Lean tactic state, which possibly comprises multiple goals. 4.3.2. Proofsize objective We depart from Polu & Sutskever (2020) and use a proofsize objective to guide our proof searches, which consists in generating one token that represents a proof size estimate bucket for the current goal (Lean tactic state): DECL <DECLARATION> GOAL <GOAL> PROOFSIZE <PROOFSIZE_BUCKET_TOKEN> For a given goal g, either the goal was proved as part of the proof search and we denote its proof size (the number of tactic applications (compounded Lean tactics counting as one)) asps(g), or the goal was not proved in which case we assign the goal to a bucket that virtually represents "inﬁnite"proof sizes. We use 11 buckets B= 0:::10and compute the proofsize bucketb(g)for a goalgby assigning inﬁnite proof sizes to bucket 0, all proof sizes over 20to bucket 1and linearly projecting proof sizes lower than 20on the remaining buckets 2;:::;10(10being the bucket for the shortest proof sizes). In practice, when training and sampling from the model, we mapBto the tokens ’A’ :::’K’. To value goals as we run proof searches, we sample the proofsize bucket token and record the logits pb(g)for each viable bucket and use them to get a weighted average with the following formula: v(g) =1 #BP b2Bpb(g):b. As an example, if the model assigns p0= 1(hencepb6=0= 0) thenv(g) = 0 . Conversely if the model assigns p10= 1(10being the bucket for the shortest proof sizes) then v(g) = 1 . The rationale for using this proofsize objective instead of theoutcome objective described in Polu & Sutskever (2020) is that (i) it achieves better performance compared to the outcome objective (see table 1), and (ii) it prioritizes goals that potentially lead to shorter proofs during proof search, creating an intrinsic incentive for the system to converge towards shorter proofs. Similarly to Polu & Sutskever (2020) we favor this token-based approach to the introduction of a separate value head to keep the overall architecture simple. This way the proofsize objective can be implemented by simply augmenting the training dataset and without any architectural change. 4.4. Bootstrapping Bootstrapping consists in the steps required to train an initial model on both the proofstep objective and the proofsize objective . Given a pre-trained model on WebMath , we ﬁne-tune it on the tactic dataset extracted from mathlib as well as the proof artifacts dataset mix1 as described in Han et al. (2021). This initial model, which we denote 0is solely trained on the proofstep objective . We use the validation splits of the tactic andm1datasets to early-stop training. Note that this is our only use of mathlib-valid to inﬂuence the training process throughout this paper. To generate data for the proofsize objective , we use0to sample proofs for statements from mathlib-train . For each statement from mathlib-train (25k) we attempt a= 1proof searches using the cumulative logprob priority search described in Polu & Sutskever (2020) (which does not require a trained value function) using d= 512 expansions and e= 8samples per expansion. We denote the set of successful proof searches created in this process as S0. UsingS0we generate dataset D0by concatenating: (i) theFormal Mathematics Statement Curriculum Learning Table 1. Performance of 0and1onmathlib-valid andminiF2Fvalid compared to PACT Lean GPT-f as reported in Han et al. (2021); Zheng et al. (2021). All models have the same architecture. 0is sampled using cumulative logprob priority best-ﬁrst search. 1is sampled using best-ﬁrst search based on the proofsize objective. We report our setup ( d= 512 expansions and e= 8 tactic samples per expansions) as well as the setups used in Han et al. (2021); Zheng et al. (2021) to control for compute. We also report the performance of 1onmathlib-valid when trained using the outcome objective from Polu & Sutskever (2020) as an ablation of our proposed proofsize objective . Model d e pass@1 pass@8 mathlib-valid PACT 512 16 48.4% 0(PACT setup) 512 16 48.5% 57.6% 0 512 8 46.7% 57.5% 1 512 8 56.3% 66.3% 1(outcome objective ) 512 8 55.6% 65.9% miniF2F-valid MiniF2F 128 16 23.9% 29.3% 0(MiniF2F setup) 128 16 27.6% 31.8% 0 512 8 28.4% 33.6% 1 512 8 28.5% 35.5% 1(outcome objective ) 512 8 28.3% 34.7% initial tactic dataset ( proofstep objective ), (ii) a deduplicated set of proofsteps extracted from the proofs in S0 (proofstep objective ) and (iii) a deduplicated set of proofsize tuples (goals and proofsize) extracted from the full proof searches inS0(proofsize objective ). Note that the full proof searches in S0include goals that are visited but eventually remain unproved, which provides useful negative examples for the trained value function (even if these negatives may include provable goals that simply were not prioritized by the search). Also note that S0doesn’t include failed proof searches (which would contain only negative examples and no proofstep objective data). We ﬁne-tune 0onD0for exactly one epoch (no use of validation data for early-stopping) to obtain our initial model 1trained on both the proofstep objective and the proofsize objective .0is used in our expert iteration setup as base model to ﬁne-tune from at each iteration, and 1is our ﬁrst iterated model or mathlib bootstrapped model trained on both objectives. We report in Table 1 the pass rates of 0and1onmathlibvalid andminiF2F-valid and compare with previously reported pass rates for equivalent amounts of compute. As reported in Polu & Sutskever (2020), training a value function to guide search greatly improves the pass rates of 1 onmathlib-valid (see Polu & Sutskever (2020) for an ablation of the value function). Interestingly, the gap between 0and1onminiF2F-valid is not as signiﬁcant, demon-strating that training a value function on proofs sampled from mathlib-train has limited transfer to miniF2F-valid . The main differences with Zheng et al. (2021), potentially explaining the gap on minif2f-valid (27:6%vs23:9%), consists in the new pre-training described in section 4.2 as well as the use of a more recent mathlib checkpoint for the mix1 , mix2 andtactic datasets. 4.5. Iterated sampling and training Our expert iteration process takes as input: (i) a set of formal statements St, (ii) a function a:St  !Nindicating the number of proof search attempts to run per statement at each iteration, (iii) a base model 0to ﬁne-tune from at each iteration, and (iv) a mathlib bootstrapped model 1trained on both objectives. Each iteration kconsists in sampling proof searches for statements in Stusingk, ﬁltering successful proof searches Skto extract a new dataset Dk, and ﬁne-tuning 0on it to obtaink+1, on which we can iterate. To sample proof searches from Stwe use the best-ﬁrst search described in Polu & Sutskever (2020) with the value function described in section 4.3.2. We attempt a(s2St)proof searches for each statement swithd= 512 expansions and e= 8 samples per expansion. We denote the set of successful proof searches for iteration kasSk. UsingSkwe generate datasets Dkby concatenating: (i) the initial tactic dataset ( proofstep objective ), (ii) a deduplicated set of proofsteps extracted from the proofs inS 1ikSk(proofstep objective ), and (iii) a deduplicated set of proofsize tuples (goals and proofsize) extracted from the full proof searches inS 1ikSk(proofsize objective ). Note that we use a global deduplication across iterations for both proofsteps and proofsize tuples which we found to be important to maintain the stability of the expert iteration procedure. This global deduplication is somewhat equivalent for each statement to growing a unique proof tree by aggregating all the proof searches that have been run for it across iterations. This virtual proof tree accumulates a growing number of positive proof paths as well as a growing number of visited goals that remain unproven. We use these goals as negative examples for the proofsize objective , labeling them with an inﬁnite proofsize. Positive goals are deduplicated keeping the minimum proof sizes across proof searches. Finallykis obtained by ﬁne-tuning 0for exactly one epoch onDk. Note that the initial tactic dataset is included in each Dk, despite0being already trained on it (along with mix1 ). We found this repetition to be beneﬁcial overall (as it adds the mathlib extracted proofsteps to our deduplicated per statements virtual proof trees) despite it leading to a slight overﬁt on the tactic dataset in termsFormal Mathematics Statement Curriculum Learning of validation loss. 4.6. Expert iteration on mathlib-train In this section we propose to set Stto the statements in mathlib-train , run our expert iteration process with it and report performance on both mathlib-valid andminiF2F-valid . Performance is reported in terms of pass rate (percentage of successful proof searches) as a function of the number of attempts per statement, noted pass@k wherekis the number of attempts per statement at test time. To reduce noise in these metrics we generally run more than kattempts at test time (generally 32to compute pass @1andpass @8), averaging across attempts as needed to obtain a smoother pass@k value. Given the large number of statements in mathlib-train (25k) we uniformly set a= 1and use0and1as described in section 4.4 and report pass@1 andpass@8 across 8 iterations in ﬁgure 1. The pass@1 onmathlib-valid goes from 56:3%for1to62:6%for9. The performance steadily improves and follows a clear logarithmic scaling law on mathlib-valid . It is also notable that, initially, transfer to outof-distribution minif2f-valid appears limited but eventually kicks in as we reach better performance on mathlib-valid . This demonstrates that the expert iteration process does not just overﬁt to mathlib but also leads to improved performance on out-of-distribution statements. 2 4 6 855606570 2 4 6 8303540pass@1 pass@8mathlib-valid minif2f-valid Figure 1. pass@1 (plain) and pass@8 (dotted) for mathlib-valid andminif2f-valid when running 8 expert iterations with Stset to be the statements in mathlib-train . The x-axis is log-scaled. It corresponds to the indices of the kmodels and serves as a good proxy to compute (the amount of test-time and train-time compute per iteration being ﬁxed). The y-axis is scaled linearly and simply shifted between the two graphs (spans an equal range). We deﬁne the cumulative pass rate at iteration kas the pass rate consisting of all proof searches up to iteration k(necessarily monotonic in k). Since we set a= 16 for evaluation onmathlib-valid andminif2f-valid at each iteration, the 2 4 68687072747678 2 4 68363840424446 Expert iteration Sample only Adjusted computemathlib-valid minif2f-validFigure 2. Cumulative pass rate for our expert iteration loop as well as asample only loop where we skip re-training the model between iterations. The adjusted compute line is computed by ﬁtting the sample only curve and shifting it to approximate a setup where we would focus all the additional compute used by expert iteration (sampling training data from mathlib-train as well as re-training models at each iteration) towards running proof searches against mathlib-valid . cumulative pass rate at iteration kcan be seen as a noisy ensembled pass@16k (multiple models ( k), no averaging). In ﬁgure 2, we report this cumulative pass rate for two iteration loops, our normal one and a sampling-only loop where we skip re-training the model between iterations and solely sample from 1. This directly compares test-time compute scaling (scaling proof search attempts) to expert iteration scaling (interleaved training on new data sampled from mathlib-train ) and provides a very clear visualization of the gains of expert iteration. For a fair comparison, we also report an adjusted compute line which approximates the test-time performance we would get at each iteration if we were to focus all the additional compute used by expert iteration (sampling proofs from mathlib-train as well as re-training models at each iteration) towards solely running proof searches against mathlib-valid . As shown by ﬁgure 2, the scaling exponent of expert iteration is substantially higher than the scaling exponent associated with solely scaling test-time compute (running more proof searches), demonstrating the clear beneﬁt of expert iteration. We’ll denote the fully iterated model from this section as mathlib 9 . Even in the presence of ground-truth proofs for each of the statements in mathlib-train (tactic dataset), expert iteration generates data that further improves the performance of the model. The number of statements proved inmathlib-train goes from 17390 (67:8%) at iteration 1to 19476 (76:0%) at iteration 9, while the average proof length of these statements goes from 4:8to4:0. We hypothesizeFormal Mathematics Statement Curriculum Learning that this continuously improving performance through expert iteration stems from two effects: (i) the model ﬁnding new original proofs for the same statements and (ii) the model closing marginally harder statements at each iteration – which in turn provides more useful training data for the next iteration. By iteration 9, the model is trained on more than 90% generated data. We present in Appendix E a few examples of original proofs found by our models on mathlib-train compared with their ground-truth versions. To verify our hypothesis that expert iteration is capable of closing a curriculum of increasingly difﬁcult problems out of a set of problem statements, and that this capability is independent of having access to ground-truth proofs, we propose in the next section to study expert iteration applied to a synthetically generated set of problems for which we have ﬁne-grained control on the difﬁculty of each statement. 5. Statement curriculum learning In this section we focus on running expert iteration on synthetic statements generated by an inequality generator. The use of synthetic statements enables us to control the difﬁculty of each statement to present evidence that expert iteration can hill-climb the intrinsic difﬁculty gradient of the resulting set of statements. In particular, we show that, at ﬁxed compute budget, expert iteration eventually closes proofs of hard statements that remain completely out of reach of simply sampling proof searches without interleaved training. 5.1. Synthetic inequality generator We designed a synthetic inequality statement generator for Lean in the spirit of the INT (Wu et al., 2020) generator. The generator consists in generating inequalities from well known inequality theorems (AM-GM, Trivial inequality, Cauchy-Schwarz, Bernoulli, Young, Hölder) and composing them. It is driven by two difﬁculty parameters: ND which controls depth of composition of inequalities and NSwhich controls the complexity of the input expressions to the composed inequalities. We provide details on its implementation in Appendix D. Using this generator we generate a curriculum of 5600 inequality statements (for which we don’t have proofs), 100 for each values of 0NS7and0ND6. We denote this set of statements as synth-ineq . To bootstrap our models capabilities on this speciﬁc task, we also generate 100statements of low difﬁculty ( ND= 1and NS= 5) and formalize a proof for each of these statements. We refer to this dataset as synth-ineq-train . In the rest of this paper we adjunct this training dataset to the tactic dataset used to train our models.5.2. Expert iteration on synthetic inequality statements In this section we propose to set Stto the union of the statements in mathlib-train andsynth-ineq . Again, we uniformly seta= 1 and use0and1as described in section 4.4, except that they are now also trained on synth-ineq-train . Similarly to the previous section, we report in ﬁgure 3 the cumulative pass rate for two loops, our standard expert iteration loop, and a proof search only loop where we don’t interleave training between iterations. The pass rates are reported split by values of ND(pooling together 0NS 7) which we found to be the main driver for difﬁculty. 246801020304050 2468010203040500123456Expert iterationSample only Figure 3. Cumulative pass rate for our expert iteration loop as well as asample only loop where we skip re-training the model between iterations. Pass rates are reported for each value of ND(pooling together 0NS7). Despite the challenging nature of these synthetic inequalities, ﬁgure 3 demonstrates that expert iteration is capable of learning the intrinsic curriculum induced by synth-ineq . In particular, expert iteration is capable of closing 6problems of difﬁculty ND= 6 without having been provided with any seed ground-truth proof for this difﬁculty level. Note that difﬁculty ND= 6remains completely out of reach of simply scaling the number of attempts per statements (the sample only loop remaining stuck at 0forND= 6). This conﬁrms on our synthetic statements dataset synth-ineq that not only expert iteration is capable of learning the curricula occurring in a set of statements, but this process also enables the emergence of new capabilities without the need for ground-truth proofs (ability to close, highly challenging, deeply composed inequalities). 6. Targeting miniF2F Motivated by the results from Section 5, we curated and manually formalized a set of math exercises to target miniF2F .miniF2F statements being quite out of distribution compared to the distribution of statements present inFormal Mathematics Statement Curriculum Learning mathlib (which typically includes generic theorems and lemmas but very few exercises), we hypothesized that if the difﬁculty of this set of statements was made varied enough, expert iteration could potentially leverage it to effectively shift our models’ distribution closer to miniF2F ’s, and in turn, improve their eventual performance on it. 6.1. Formalization effort We manually formalized 327statements2drawn from the following sources: 302examples and exercises from Lehoczky & Rusczyk (a;b). The books are classic problem solving textbooks for students in grades 7-12 preparing for contests such as AMCs and AIMEs. 25problems from the MATH dataset (Hendrycks et al., 2021). All problems were drawn from the train split of the dataset, focusing on difﬁculty 5 problems ( miniF2F only contains problems from the test split). We refer to Zheng et al. (2021) for more details on the formalization procedure and the typical time needed for it as these problems were formalized in similar conditions. We denote this set of statements as miniF2F-curriculum and veriﬁed (based on problem provenance and manual inspection of statements) that it had an empty intersection with miniF2F-{test,valid} . 6.2. Transfer to miniF2F In this section we propose to set Stto the union of the statements in mathlib-train ,synth-ineq andminiF2F-curriculum . We uniformly set a= 1 onmathlib-train andsynth-ineq anda= 8 onminiF2F-curriculum and use0and1as described in section 5. Similarly to previous sections, we report in ﬁgure 4 (left) the cumulative pass rate on miniF2F-valid of our full curriculum expert iteration loop and compare them with the mathlib-train only expert iteration from section 4.6. Since more compute is deployed in our full-curriculum loop (more statements) we also report a mathlib-train only loop taking a= 2. At the end of the expert iteration, 100out of the 327 statements from miniF2F-curriculum end up being closed, suggesting a lack of density in our manually formalized set of statement. We also report in ﬁgure 4 (right) the pass@1 andpass@8 for our full curriculum expert iteration loop. The steady improvement on miniF2F-valid shows that the expert iteration procedure we propose does not overﬁt on the statements that compose the curriculum it uses. Despite the potential inefﬁciency of our curriculum, the improved performance associated with its use demonstrates, as hypothesized, an 2https://github.com/openai/miniF2F/tree/ statement_curriculum_learning/lean/src/ statement_curriculum_learning 2 4 68303540pass@1 mathlib pass@1 full pass@8 mathlib pass@8 fullminif2f-valid 2 4 684045 mathlib a=1 mathlib a=2 fullminif2f-validFigure 4. Left: cumulative pass rate on miniF2F-valid for our expert iteration loop using our full curriculum ( mathlib-train ,synthineq andminiF2F-curriculum ) compared to the expert iteration loop from section 4.6. The total number of attempts per iteration in our fullloop is 25k+ 5:6k+ 832733:2k, which means the total compute deployed is higher than in the mathlib-train only loop ( 25k). We therefore also report in dotted a mathlib-train only loop, takinga= 2, whose total number of attempts per iteration is 50k.Right: pass@1 (plain) and pass@8 (dotted) for our expert iteration loop using our full curriculum ( mathlib-train ,synth-ineq andminiF2F-curriculum ) compared to the expert iteration loop from section 4.6. effective transfer between miniF2F-curriculum, synth-ineq andminiF2F-valid through expert iteration. We’ll denote the fully iterated model from this section as full 9. 6.3. Results We report in table 2 the pass rates on mathlib-{valid, test} andminiF2F-{valid, test} for the models trained in previous sections, namely 1,mathlib 9 , andfull 9. We achieve a 47:3% pass rate (using a= 64 attempts) on miniF2F-valid and a 36:6%pass rate on miniF2F-test , substantially improving from the previous state-of-the-art (Zheng et al., 2021). These results include the resolution of 26AMC12 problems, 6AIME problems and 2problems adapted from the IMOs. Out of these statements, 4AMC12 problems ( amc12b_2020_p5 ,amc12a_2009_p9 , amc12a_2003_p24 ,amc12b_2003_p17 ),2AIME problems ( aime_1984_p1 , aime_1990_p4 ), and 2IMO-adapted problems ( imo_1961_p13, imo_1964_p2 ) are uniquely solved by expert iterated models, the two IMO-adapted and the two AIME problems being uniquely solved by full 9. We provide a selection of the proofs found by our models 3Note that this IMO-adapted statement from miniF2F-valid is a much weaker version than the original problem (see Appendix F for more context)Formal Mathematics Statement Curriculum Learning Table 2. Performance of 1(value-function based search), mathlib 9 (expert iterated on mathlib-train ) andfull 9(expert iterated on our full curriculum) on mathlib-{valid, test} andminiF2F-{valid, test} . All proof searches are run with d= 512 ande= 8. Model pass@1 pass@8 pass@64 mathlib-valid PACT (Han et al., 2021) 48.4% 1 56.3% 66.3% 72.0% mathlib 9 62.6% 70.7% 75.8% full 9 61.7% 69.8% 75.3% mathlib-test 1 56.5% 66.9% 73.7% mathlib 9 63.0% 71.5% 77.1% full 9 62.9% 71.6% 76.3% miniF2F-valid PACT (Zheng et al., 2021) 23.9% 29.3% 1 28.5% 35.5% 41.2% mathlib 9 31.3% 38.3% 44.1% full 9 33.6% 41.2% 47.3% miniF2F-test PACT (Zheng et al., 2021) 24.6% 29.2% 1 25.9% 31.1% 33.6% mathlib 9 27.2% 33.0% 35.2% full 9 29.6% 34.5% 36.6% for these statements as well as a qualitative analysis of them in Appendix F. Also, we achieve a higher than 75% pass rate (using a= 64 attempts) on mathlib-{valid, test} (a new state-of-the-art as well) suggesting that our models have the potential to be effectively leveraged as proof assistants in the formalization efforts associated with mathlib . 7. Discussion 7.1. Model Size Throughout this paper, we used a single model size (774m trainable parameters). We brieﬂy experimented with different model sizes (not reported in this paper) and found that model size scaling is not as straightforward as in the case of unsupervised learning (Kaplan et al., 2020). We found that bigger models are better, in the sense that they consistently exhibit higher pass@1 . But, they are also much more expensive to sample from. And despite their pass@1 being higher, it is often the case that for a ﬁxed amount of compute, sampling more attempts from a smaller model leads to a better ﬁnal performance. For the compute budget we had available, we estimated the model size we used to be a compelling trade-off. We leave as future work a more thorough study of these dynamics to better understand the different compute frontiers involved.Indicatively, with our 774m parameters model, running a full expert iteration to train full 9required about 2000 A100 days of compute. Running one full proof search ( a= 1 d= 512e= 8) when properly parallelised, requires on average about 0:1A100 hour of compute. 7.2. Limitations Despite our models’ capability, as discussed in Appendix F.1, to generate cuts and witnesses, we believe that their current main limitation lies in their inability (under our proposed search procedure) to chain more than 2 or 3 non-trivial steps of mathematical reasoning, preventing them from consistently (instead of exceptionally) solving challenging olympiad problems. We’ve been repeatedly impressed by the complexity of some of the proofsteps generated by our models. But, proofs requiring many of such reasoning steps remain beyond our current compute horizon. Even if we solved a selection of challenging olympiad problems, our models are still very far from being competitive with the brightest students in these competitions. While our models have demonstrated some capabilities to generate cuts, the cuts they generate are often shallow (they involve only a few proofsteps and don’t necessarily deeply change the structure of the proof–we refer the reader to the Cut-Elimination theorem and Carbone & Semmes (1996) for a discussion of the inﬂuence of cuts on proof size). We believe that studying language models’ ability to generate cuts, and designing search procedures that leverage that capability (related ideas can be found in Czechowski et al. (2021)), are interesting avenues of research to alleviate this limitation. 8. Conclusion In this paper we presented an expert iteration procedure forGPT-f (Polu & Sutskever, 2020), demonstrating that it is capable of solving a curriculum of increasingly difﬁcult problems out of a set of formal statements of sufﬁciently varied difﬁculty. Our results suggest that the lack of self-play in the formal mathematics setup can be effectively compensated for by automatically as well as manually curated sets of formal statements, which are much cheaper to formalize than full proofs. Finally, we hope that the statement curriculum learning methodology we presented in this work will help accelerate progress in automated reasoning, especially if scaled with automated generation and curation of formal statements in the future. 