Concrete Problems in AI Safety Dario Amodei Google BrainChris Olah Google BrainJacob Steinhardt Stanford UniversityPaul Christiano UC Berkeley John Schulman OpenAIDan Man e Google Brain Abstract Rapid progress in machine learning and articial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, dened as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of ve practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\avoiding side eects" and \avoiding reward hacking"), an objective function that is too expensive to evaluate frequently (\scalable supervision"), or undesirable behavior during the learning process (\safe exploration" and \distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI. 1 Introduction The last few years have seen rapid progress on long-standing, dicult problems in machine learning and articial intelligence (AI), in areas as diverse as computer vision , video game playing , autonomous vehicles , and Go . These advances have brought excitement about the positive potential for AI to transform medicine , science , and transportation , along with concerns about the privacy , security , fairness , economic , and military  implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI . The authors believe that AI technologies are likely to be overwhelmingly benecial for humanity, but we also believe that it is worth giving serious thought to potential challenges and risks. We strongly support work on privacy, security, fairness, economics, and policy, but in this document we discuss another class of problem which we believe is also relevant to the societal impacts of AI: the problem of accidents in machine learning systems. We dene accidents as unintended and harmful behavior that may emerge from machine learning systems when we specify the wrong objective function, are These authors contributed equally. 1arXiv:1606.06565v2  [cs.AI]  25 Jul 2016not careful about the learning process, or commit other machine learning-related implementation errors. There is a large and diverse literature in the machine learning community on issues related to accidents, including robustness, risk-sensitivity, and safe exploration; we review these in detail below. However, as machine learning systems are deployed in increasingly large-scale, autonomous, opendomain situations, it is worth re ecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems. There has been a great deal of public discussion around accidents. To date much of this discussion has highlighted extreme scenarios such as the risk of misspecied objective functions in superintelligent agents . However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics . We believe it is usually most productive to frame accident risk in terms of practical (though often quite general) issues with modern ML techniques. As AI capabilities advance and as AI systems take on increasingly important societal functions, we expect the fundamental challenges discussed in this paper to become increasingly important. The more successfully the AI and machine learning communities are able to anticipate and understand these fundamental technical challenges, the more successful we will ultimately be in developing increasingly useful, relevant, and important AI systems. Our goal in this document is to highlight a few concrete safety problems that are ready for experimentation today and relevant to the cutting edge of AI systems, as well as reviewing existing literature on these problems. In Section 2, we frame mitigating accident risk (often referred to as \AI safety" in public discussions) in terms of classic methods in machine learning, such as supervised classication and reinforcement learning. We explain why we feel that recent directions in machine learning, such as the trend toward deep reinforcement learning and agents acting in broader environments, suggest an increasing relevance for research around accidents. In Sections 3-7, we explore ve concrete problems in AI safety. Each section is accompanied by proposals for relevant experiments. Section 8 discusses related eorts, and Section 9 concludes. 2 Overview of Research Problems Very broadly, an accident can be described as a situation where a human designer had in mind a certain (perhaps informally specied) objective or task, but the system that was designed and deployed for that task produced harmful and unexpected results. . This issue arises in almost any engineering discipline, but may be particularly important to address when building AI systems . We can categorize safety problems according to where in the process things went wrong. First, the designer may have specied the wrong formal objective function, such that maximizing that objective function leads to harmful results, even in the limit of perfect learning and innite data. Negative side eects (Section 3) and reward hacking (Section 4) describe two broad mechanisms that make it easy to produce wrong objective functions. In \negative side eects", the designer species an objective function that focuses on accomplishing some specic task in the environment, but ignores other aspects of the (potentially very large) environment, and thus implicitly expresses indierence over environmental variables that might actually be harmful to change. In \reward hacking", the objective function that the designer writes down admits of some clever \easy" solution that formally maximizes it but perverts the spirit of the designer's intent (i.e. the objective function can be \gamed"), a generalization of the wireheading problem. 2Second, the designer may know the correct objective function, or at least have a method of evaluating it (for example explicitly consulting a human on a given situation), but it is too expensive to do so frequently, leading to possible harmful behavior caused by bad extrapolations from limited samples. \Scalable oversight" (Section 5) discusses ideas for how to ensure safe behavior even given limited access to the true objective function. Third, the designer may have specied the correct formal objective, such that we would get the correct behavior were the system to have perfect beliefs, but something bad occurs due to making decisions from insucient or poorly curated training data or an insuciently expressive model. \Safe exploration" (Section 6) discusses how to ensure that exploratory actions in RL agents don't lead to negative or irrecoverable consequences that outweigh the long-term value of exploration. \Robustness to distributional shift" (Section 7) discusses how to avoid having ML systems make bad decisions (particularly silent and unpredictable bad decisions) when given inputs that are potentially very dierent than what was seen during training. For concreteness, we will illustrate many of the accident risks with reference to a ctional robot whose job is to clean up messes in an oce using common cleaning tools. We return to the example of the cleaning robot throughout the document, but here we begin by illustrating how it could behave undesirably if its designers fall prey to each of the possible failure modes: Avoiding Negative Side Eects: How can we ensure that our cleaning robot will not disturb the environment in negative ways while pursuing its goals, e.g. by knocking over a vase because it can clean faster by doing so? Can we do this without manually specifying everything the robot should not disturb? Avoiding Reward Hacking: How can we ensure that the cleaning robot won't game its reward function? For example, if we reward the robot for achieving an environment free of messes, it might disable its vision so that it won't nd any messes, or cover over messes with materials it can't see through, or simply hide when humans are around so they can't tell it about new types of messes. Scalable Oversight: How can we eciently ensure that the cleaning robot respects aspects of the objective that are too expensive to be frequently evaluated during training? For instance, it should throw out things that are unlikely to belong to anyone, but put aside things that might belong to someone (it should handle stray candy wrappers dierently from stray cellphones). Asking the humans involved whether they lost anything can serve as a check on this, but this check might have to be relatively infrequent|can the robot nd a way to do the right thing despite limited information? Safe Exploration: How do we ensure that the cleaning robot doesn't make exploratory moves with very bad repercussions? For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea. Robustness to Distributional Shift: How do we ensure that the cleaning robot recognizes, and behaves robustly, when in an environment dierent from its training environment? For example, strategies it learned for cleaning an oce might be dangerous on a factory work oor. There are several trends which we believe point towards an increasing need to address these (and other) safety problems. First is the increasing promise of reinforcement learning (RL), which allows agents to have a highly intertwined interaction with their environment. Some of our research problems only make sense in the context of RL, and others (like distributional shift and scalable oversight) gain added complexity in an RL setting. Second is the trend toward more complex agents and environments. \Side eects" are much more likely to occur in a complex environment, and an agent may need to be quite sophisticated to hack its reward function in a dangerous way. This may explain why these problems have received so little study in the past, while also suggesting their 3importance in the future. Third is the general trend towards increasing autonomy in AI systems. Systems that simply output a recommendation to human users, such as speech systems, typically have relatively limited potential to cause harm. By contrast, systems that exert direct control over the world, such as machines controlling industrial processes, can cause harms in a way that humans cannot necessarily correct or oversee. While safety problems can exist without any of these three trends, we consider each trend to be a possible amplier on such challenges. Together, we believe these trends suggest an increasing role for research on accidents. When discussing the problems in the remainder of this document, we will focus for concreteness on either RL agents or supervised learning systems. These are not the only possible paradigms for AI or ML systems, but we believe they are sucient to illustrate the issues we have in mind, and that similar issues are likely to arise for other kinds of AI systems. Finally, the focus of our discussion will dier somewhat from section to section. When discussing the problems that arise as part of the learning process (distributional shift and safe exploration), where there is a sizable body of prior work, we devote substantial attention to reviewing this prior work, although we also suggest open problems with a particular focus on emerging ML systems. When discussing the problems that arise from having the wrong objective function (reward hacking and side eects, and to a lesser extent scalable supervision), where less prior work exists, our aim is more exploratory|we seek to more clearly dene the problem and suggest possible broad avenues of attack, with the understanding that these avenues are preliminary ideas that have not been fully  eshed out. Of course, we still review prior work in these areas, and we draw attention to relevant adjacent areas of research whenever possible. 3 Avoiding Negative Side Eects Suppose a designer wants an RL agent (for example our cleaning robot) to achieve some goal, like moving a box from one side of a room to the other. Sometimes the most eective way to achieve the goal involves doing something unrelated and destructive to the rest of the environment, like knocking over a vase of water that is in its path. If the agent is given reward only for moving the box, it will probably knock over the vase. If we're worried in advance about the vase, we can always give the agent negative reward for knocking it over. But what if there are many dierent kinds of \vase"|many disruptive things the agent could do to the environment, like shorting out an electrical socket or damaging the walls of the room? It may not be feasible to identify and penalize every possible disruption. More broadly, for an agent operating in a large, multifaceted environment, an objective function that focuses on only one aspect of the environment may implicitly express indierence over other aspects of the environment1. An agent optimizing this objective function might thus engage in major disruptions of the broader environment if doing so provides even a tiny advantage for the task at hand. Put dierently, objective functions that formalize \perform task X" may frequently give undesired results, because what the designer really should have formalized is closer to \perform task X subject to common-sense constraints on the environment," or perhaps \perform task X but avoid side eects to the extent possible." Furthermore, there is reason to expect side eects to be negative on average, since they tend to disrupt the wider environment away from a status quo state that may re 