Evolved Policy Gradients Rein Houthooft1Richard Y. Chen1Phillip Isola1 2 3Bradly C. Stadie2Filip Wolski1 Jonathan Ho1 2Pieter Abbeel1 2 Abstract We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent’s experience. Because this loss is highly ﬂexible in its ability to take into account the agent’s history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG’s learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms. 1. Introduction When a human learns to solve a new control task, such as playing the violin, they immediately have a feel for what to try. At ﬁrst, they may try a quick, rough stroke, and, producing a screech, will intuitively know this was the wrong thing to do. Just by listening to the sounds they produce, they will have a sense of whether or not they are making progress toward the goal. Effectively, humans have access to very well shaped internal reward functions, derived from prior experience on other motor tasks, or perhaps from listening to and playing other musical instruments (36; 49). In contrast, most current reinforcement learning (RL) agents approach each new task de novo. Initially, they have no notion of what actions to try out, nor which outcomes are desirable. Instead, they rely entirely on external reward signals to guide their initial behavior. Coming from such a blank slate, it is no surprise that RL agents take far longer than humans to learn simple skills (21). 1OpenAI2UC Berkeley3MIT. Correspondence to: Rein Houthooft <rein.houthooft@openai.com>. Figure 1: High-level overview of our approach. The method consists of an inner and outer optimization loop. The inner loop (boxed) optimizes the agent’s policy against a loss provided by the outer loop, using gradient descent. The outer loop optimizes the parameters of the loss function, such that the optimized inner-loop policy achieves high performance on an arbitrary task, such as solving a control task of interest. The evolved loss Lcan be viewed as a surrogate whose gradient is used to update the policy, which is similar in spirit to policy gradients, lending the name “evolved policy gradients". Our aim in this paper is to devise agents that have a prior notion of what constitutes making progress on a novel task. Rather than encoding this knowledge explicitly through a learned behavioral policy, we encode it implicitly through a learned loss function. The end goal is agents that can use this loss function to learn quickly on a novel task. This approach can be seen as a form of metalearning, in which we learn a learning algorithm. Rather than mining rules that generalize across data points, as in traditional machine learning, metalearning concerns itself with devising algorithms that generalize across tasks, by infusing prior knowledge of the task distribution (12). Our method consists of two optimization loops. In the inner loop, an agent learns to solve a task, sampled from a particular distribution over a family of tasks. The agent learns to solve this task by minimizing a loss function provided by the outer loop. In the outer loop, the parameters of the loss function are adjusted so as to maximize the ﬁnal returns achieved after inner loop learning. Figure 1 provides a high-level overview of this approach. Although the inner loop can be optimized with stochastic gradient descent (SGD), optimizing the outer loop presents arXiv:1802.04821v2  [cs.LG]  29 Apr 2018Evolved Policy Gradients substantial difﬁculty. Each evaluation of the outer objective requires training a complete inner-loop agent, and this objective cannot be written as an explicit function of the loss parameters we are optimizing over. Due to the lack of easily exploitable structure in this optimization problem, we turn to evolution strategies (ES) ( 35;46;16;37) as a blackbox optimizer. The evolved loss Lcan be viewed as a surrogate loss ( 43;44) whose gradient is used to update the policy, which is similar in spirit to policy gradients, lending the name “evolved policy gradients". In addition to encoding prior knowledge, the learned loss offers several advantages compared to current RL methods. Since RL methods optimize for short-term returns instead of accounting for the complete learning process, they may get stuck in local minima and fail to explore the full search space. Prior works add auxiliary reward terms that emphasize exploration ( 8;19;32;56;6;33) and entropy loss terms (31;42;15;26). These terms are often traded off using a separate hyperparameter that is not only task-dependent, but also dependent on which part of the state space the agent is visiting. As such, it is unclear how to include these terms into the RL algorithm in a principled way. Using ES to evolve the loss function allows us to optimize the true objective, namely the ﬁnal trained policy performance, rather than short-term returns. Our method also improves on standard RL algorithms by allowing the loss function to be adaptive to the environment and agent history, leading to faster learning and the potential for learning without external rewards. EPG can in theory be combined with policy initialization metalearning algorithms, such as MAML ( 11), since EPG imposes no restriction on the policy it optimizes. There has been a ﬂurry of recent work on metalearning policies, e.g., ( 10;59;11;25), and it is worth asking why metalearn the loss as opposed to directly metalearning the policy? Our motivation is that we expect loss functions to be the kind of object that may generalize very well across substantially different tasks. This is certainly true of handengineered loss functions: a well-designed RL loss function, such as that in ( 45), can be very generically applicable, ﬁnding use in problems ranging from playing Atari games to controlling robots ( 45). In Section 4, we ﬁnd evidence that a loss learned by EPG can train an agent to solve a task outside the distribution of tasks on which EPG was trained. This generalization behavior differs qualitatively from MAML (11) and RL2(10), methods that directly metalearn policies, providing initial indication of the generalization potential of loss learning. Our contributions include the following: •Formulating a metalearning approach that learns a differentiable loss function for RL agents, called EPG.•Optimizing the parameters of this loss function via ES, overcoming the challenge that ﬁnal returns are not explicit functions of the loss parameters. •Designing a loss architecture that takes into account agent history via temporal convolutions. •Demonstrating that EPG produces a learned loss that can train agents faster than an off-the-shelf policy gradient method. •Showing that EPG’s learned loss can generalize to outof-distribution test time tasks, exhibiting qualitatively different behavior from other popular metalearning algorithms. We set forth the notation in Section 2. Section 3 explains the main algorithm and Section 4 shows its results on several randomized continuous control environments. In Section 5, we compare our methods with the most related ideas in literature. We conclude this paper with a discussion in Section 6. An implementation of EPG is available at http://github.com/openai/EPG . 2. Notation and Background We model reinforcement learning ( 54) as a Markov decision process (MDP), deﬁned as the tuple M= (S,A,T,R,p 0,γ), whereSandAare the state and action space. The transition dynamic T:S×A×S/mapsto→ R+ determines the distribution of the next state st+1given the current state stand the action at.R:S×A/mapsto→ Ris the reward function and γ∈(0,1)is a discount factor. p0is the distribution of the initial state s0. An agent’s policy π:S/mapsto→A generates an action after observing a state. An episode τ∼ M with horizon His a sequence (s0,a0,r0,...,sH,aH,rH)of state, action, and reward at each timestep t. The discounted episodic return of τis deﬁned asRτ=/summationtextH t=0γtrt, which depends on the initial state distribution p0, the agent’s policy π, and the transition distribution T. The expected episodic return given agent’s policyπisEπ[Rτ]. The optimal policy π∗maximizes the expected episodic return π∗= arg max πEτ∼M,π[Rτ]. In high-dimensional reinforcement learning settings, the policyπis often parametrized using a deep neural network πθwith parameters θ. The goal is to solve for θ∗that attains the highest expected episodic return θ∗= arg max θEτ∼M,πθ[Rτ]. (1)Evolved Policy Gradients This objective can be optimized via policy gradient methods (60;55) by stepping in the direction of E[Rτ∇logπ(τ)]. This gradient can be transformed into a surrogate loss function (43; 44) Lpg=E[Rτlogπ(τ)] =E/bracketleftBigg RτH/summationdisplay t=0logπ(at|st)/bracketrightBigg ,(2) such that the gradient of Lpgequals the policy gradient. Through variance reduction techniques including actor-critic algorithms ( 20), the loss function Lpgis often changed into Lac=E/bracketleftbigg/summationdisplayH t=0A(st,at) logπ(at|st)/bracketrightbigg , (3) that is, the log-probability of taking action atat statestis multiplied by an advantage function A(st,at)(4). However, this procedure remains limited since it relies on a particular form of discounting the returns, and taking a ﬁxed gradient step with respect to the policy. Our approach learns a loss rather than using a hand-deﬁned function such as Lac. Thus, it may be able to discover more effective surrogates for making fast progress toward the ultimate objective of maximizing ﬁnal returns. 3. Methodology Our metalearning approach aims to learn a loss function Lφ that outperforms the usual policy gradient surrogate loss (43). This loss function consists of temporal convolutions over the agent’s recent history. In addition to internalizing environment rewards, this loss could, in principle, have several other positive effects. For example, by examining the agent’s history, the loss could incentivize desirable extended behaviors, such as exploration. Further, the loss could perform a form of system identiﬁcation, inferring environment parameters and adapting how it guides the agent as a function of these parameters (e.g., by adjusting the effective learning rate of the agent). The loss function parameters φare evolved through ES and the loss trains an agent’s policy πθin an on-policy fashion via stochastic gradient descent. 3.1. Metalearning Objective In our metalearning setup, we assume access to a distributionp(M)over MDPs. Given a sampled MDP M, the inner loop optimization problem is to minimize the loss Lφwith respect to the agent’s policy πθ: θ∗= arg min θEτ∼M,πθ[Lφ(πθ,τ)]. (4) Note that this is similar to the usual RL objectives (Eqs. (1) (2) (3) ), except that we are optimizing a learned lossAlgorithm 1: Evolved Policy Gradients (EPG) 1[Outer Loop] for epoche= 1,...,E do 2 Sample /epsilon1v∼N(0,I)and calculate the loss parameter φ+σ/epsilon1vforv= 1,...,V 3 Each worker w= 1,...,W gets assigned noise vector⌈wV/W⌉as/epsilon1w 4 foreach worker w= 1,...,W do 5 Sample MDPMw∼p(M) 6 Initialize buffer with Nzero tuples 7 Initialize policy parameter θrandomly 8 [Inner Loop] for stept= 1,...,U do 9 Sample initial state st∼p0ifMwneeds to be reset 10 Sample action at∼πθ(·|st) 11 Take actionatinMwand receivert,st+1, and termination ﬂag dt 12 Add tuple (st,at,rt,dt)to buffer 13 iftmodM= 0then 14 With loss parameter φ+σ/epsilon1w, calculate losses Lifor steps i=t−M,...,t using buffer tuples i−N,...,i 15 Sample minibatches mbfrom lastM steps shufﬂed, compute Lmb=/summationtext j∈mbLj, and update the policy parameter θand memory parameter (Eq. (6)) 16 InMw, using the trained policy πθ, sample several trajectories and compute the mean returnRw 17 Update the loss parameter φ(Eq. (7)) 18Output: LossLφthat trainsπfrom scratch according to the inner loop scheme, on MDPs from p(M) Lφrather than directly optimizing the expected episodic return EM,πθ[Rτ]or other surrogate losses. The outer loop objective is to learn Lφsuch that an agent’s policy πθ∗ trained with the loss function achieves high expected returns in the MDP distribution: φ∗= arg max φEM∼p(M)Eτ∼M,πθ∗[Rτ]. (5) 3.2. Algorithm The ﬁnal episodic return Rτof a trained policy πθ∗cannot be represented as an explicit function of the loss function Lφ. Thus we cannot use gradient-based methods to directly solve Eq.(5). Our approach, summarized in Algorithm 1, relies on evolution strategies (ES) to optimize the loss function in the outer loop. As described by Salimans et al. ( 37), ES computes the gra-Evolved Policy Gradients Algorithm 2: EPG test-time training 1[Input] : learned loss function Lφfrom EPG, MDPM 2Initialize buffer with Nzero tuples 3Initialize policy parameter θrandomly 4forstept= 1,...,U do 5 Sample initial state st∼p0ifMneeds to be reset 6 Sample action at∼πθ(·|st) 7 Take actionatinM, receivert,st+1, and termination ﬂag dt 8 Add tuple (st,at,rt,dt)to buffer 9 iftmodM= 0then 10 Calculate losses Lifor stepsi=t−M,...,t using buffer tuples i−N,...,i 11 Sample minibatches mbfrom lastMsteps shufﬂed, compute Lmb=/summationtext j∈mbLj, and update the policy parameter θand memory parameter (Eq. (6)) 12[Output] : A trained policy πθfor MDPM dient of a function F(φ)according to ∇φE/epsilon1∼N(0,I)F(φ+σ/epsilon1) =1 σE/epsilon1∼N(0,I)F(φ+σ/epsilon1)/epsilon1. Similar formulations also appear in prior works including ( 52;47;27). In our case, F(φ) = EM∼p(M)Eτ∼M,πθ∗[Rτ](Eq. (5)). Note that the dependence on φcomes through θ∗(Eq. (4)). Step by step, the algorithm works as follows. At the start of each epoch in the outer loop, for Winner-loop workers, we generate Vstandard multivariate normal vectors /epsilon1v∈N(0,I)with the same dimension as the loss function parameter φ, assigned to Vsets ofW/V workers. As such, for thew-th worker, the outer loop assigns the ⌈wV/W⌉-th perturbed loss function Lw=Lφ+σ/epsilon1vwherev=⌈wV/W⌉ with perturbed parameters φ+σ/epsilon1vandσas the standard deviation. Given a loss function Lw,w∈{1,...,W}, from the outer loop, each inner-loop worker wsamples a random MDP from the task distribution, Mw∼p(M). The worker then trains a policy πθinMwoverUsteps of experience. Whenever a termination signal is reached, the environment resets with state s0sampled from the initial state distribution p0(Mw). EveryMsteps the policy is updated through SGD on the loss function Lw, using minibatches sampled from the stepst−M,...,t : θ←θ−δin·∇θLw/parenleftbig πθ,τt−M,...,t/parenrightbig . (6) At the end of the inner-loop training, each worker returns theﬁnal returnRw1to the outer loop. The outer-loop aggregates the ﬁnal returns{Rw}W w=1from all workers and updates the loss function parameter φas follows: φ←φ+δout·1 Vσ/summationdisplayV v=1F(φ+σ/epsilon1v)/epsilon1v, (7) where F(φ+σ/epsilon1v) =R(v−1)∗W/V +1+···+Rv∗W/V W/V. As a result, each perturbed loss function Lvis evaluated on W/V randomly sampled MDPs from the task distribution using the ﬁnal returns. This achieves variance reduction by preventing the outer-loop ES update from promoting loss functions that are assigned to MDPs that consistently generate higher returns. Note that the actual implementation calculates each loss function’s relative rank for the ES update. Algorithm 1 outputs a learned loss function Lφafter Eepochs of ES updates. At test time, we evaluate the learned loss function Lφproduced by Algorithm 1 on a test MDP Mby training a policy from scratch. The test-time training schedule is the same as the inner loop of Algorithm 1 and we summarize it in Algorithm 2. 3.3. Architecture The agent is parametrized using an MLP policy with observation spaceSand action spaceA. The loss has a memory unit to assist learning in the inner loop. This memory unit is a single-layer neural network to which an invariable input vector of ones is fed. As such, it is essentially a layer of bias terms. Since this network has a constant input vector, we can view its weights as a very simple form of memory to which the loss can write via emitting the right gradient signals. An experience buffer stores the agent’s Nmost recent experience steps, in the form of a list of tuples (st,at,rt,dt), withdtthe trajectory termination ﬂag. Since this buffer is limited in the number of steps it stores, the memory unit might allow the loss function to store information over a longer period of time. The loss function Lφconsists of temporal convolutional layers which generate a context vector fcontext , and dense layers, which output the loss. The architecture is depicted in Figure 2. At stept, the dense layers output the loss Ltby taking a batch ofMsequential samples {si,ai,di,mem,fcontext,πθ(·|si)}t i=t−M, (8) whereM < N and we augment each transition with the memory output mem , a context vector fcontext generated 1More speciﬁcally, the average return over 3sampled trajectories using the ﬁnal policy for worker w.Evolved Policy Gradients Figure 2: Architecture of a loss computed for timestep t within a batch of Msequential samples (from t−Mtot), using temporal convolutions over a buffer of size N(from t−Ntot), withM≤N: dense net on the bottom is the policyπ(s), taking as input the observations (orange), while outputting action probabilities (green). The green block on the top represents the loss output. Gray blocks are evolved, yellow blocks are updated through SGD. from the loss’s temporal convolutional layers, and the policy distribution πθ(·|si). In continuous action space, πθis a Gaussian policy, i.e., πθ(·|si) =N(·;µ(si;θ0),Σ), with µ(si;θ0)the MLP output and Σa learnable parameter vector. The policy parameter vector is deﬁned as θ= [θ0,Σ]. To generate the context vector, we ﬁrst augment each transition in the buffer with the output of the memory unit mem and the policy distribution πθ(·|si)to obtain a set {si,ai,di,mem,πθ(·|si)}t i=t−N. (9) We stack these items sequentially into a matrix and the temporal convolutional layers take it as input and output the context vector fcontext . The memory unit’s parameters are updated via gradient descent at each inner-loop update (Eq. (6)). Note that both the temporal convolution layers and the dense layers do not observe the environment rewards directly. However, in cases where the reward cannot be fully inferred from the environment, such as the DirectionalHopper environment we will examine in Section 4.2, we add rewardsrito the set of inputs in Eqs. (8)and(9). In fact,any information that can be obtained from the environment could be added as an input to the loss function, e.g., exploration signals, the current timestep number, etc, and we leave further such extensions as future work. In practice, to bootstrap the learning process, we add to Lφ a guidance policy gradient surrogate loss signal Lpg, such as the REINFORCE ( 60) or PPO ( 45) surrogate loss function, making the total loss ˆLφ= (1−α)Lφ+αL pg, (10) and annealαfrom 1to0over a ﬁnite number of outerloop epochs. As such, learning is ﬁrst derived mostly from the well-structured Lpg, while over time Lφtakes over and drives learning completely after αhas been annealed to 0. 4. Experiments We apply our method to several randomized continuous control MuJoCo environments ( 5;34;9), namely RandomHopper and RandomWalker (with randomized gravity, friction, body mass, and link thickness), RandomReacher (with randomized link lengths), DirectionalHopper and DirectionalHalfCheetah (with randomized forward/backward reward function), GoalAnt (reward function based on the randomized target location), and Fetch (randomized target location). We describe these environments in detail in Appendix A. These environments are chosen because they require the agent to identify a randomly sampled environment at test time via exploratory behavior. Examples of the randomized Hopper environments are shown in Figure 4 and the Fetch environment in Figure 3. The plots in this section show the mean value of 20test-time training curves as a solid line, while the shaded area represents the interquartile range. The dotted lines plot 5randomly sampled curves. Figure 3: Examples of learning to reach random targets in the Fetch environment Implementation details In our experiments, the temporal convolutional layers of the loss function has 3layers. The ﬁrst layer has a kernel size of 8, stride of 7, and outputs 10 channels. The second layer has a kernel of 4, stride of 2, and outputs 10channels. The third layer is fully-connected with32output units. Leaky ReLU activation is applied to each convolutional layer. The fully-connected componentEvolved Policy Gradients Figure 4: Example of learning to hop forward from a randomly initialized policy in RandomHopper environments with randomized morphology and physics parameters. Each row is a different environment randomization, while from left to right, trajectories are recorded as learning progresses. takes as input the trajectory features from the convolutional component concatenated with state, action, termination signal, and policy output, as well as reward in experiments in which reward is observed. It has 1hidden layer with 16hidden units and leaky ReLU activation, followed by an output layer. The buffer size is N∈{512,1024}. The agent’s MLP policy has 2hidden layers of 64units with tanh activation. The memory unit is a 32-unit single layer withtanh activation. We useW= 256 inner-loop workers in Algorithm 1, combined withV= 64 ES noise vectors. The loss function is evolved over 5000 epochs, with α, as in Eq. (10), annealed linearly from 1to0over the ﬁrst 500epochs. The off-the-shelf PG algorithm (PPO) was moderately tuned to perform well on these tasks, however, it is important to keep in mind that these methods inherently have trouble optimizing when the number of samples drawn for each policy update batch is low. EPG’s inner loop update frequency is set toM∈{32,64,128}and the inner loop length is U∈{64×M,128×M,256×M,512×M}. At every EPG inner loop update, the policy and memory parameters are updated by the learned loss function using shufﬂed minibatches of size 32within each set of Mmost recent transition steps in the replay buffer, going over each step exactly once. We tabulate the hyperparameters for each randomized environment in Table 1 in Appendix C. Normalization according to a running mean and standard deviation were applied to the observations, actions, and rewards for each EPG inner loop worker independently (Algorithm 1) and for test-time training (Algorithm 2). Adam (18) is used for the EPG inner loop optimization and test-time training with β1= 0.9andβ2= 0.999, while the outer loop ES gradients are modiﬁed by Adam with β1= 0 andβ2= 0.999(which means momentum has been turned off) before updating the loss function. Furthermore, L2regularization over the loss function parameters with coefﬁcient 0.001is added to outer loop objective. The inner loop step size is ﬁxed to 10−3, while the outer loop step size is annealed linearly from 10−2to10−3over the ﬁrst 2000 epochs. 4.1. Performance We compare test-time training performance using the EPG loss function, Algorithm 2, against an off-the-shelf policy gradient method, PPO ( 45). Figures 5, 6, 7, and 11 show learning curves for these two methods on the RandomHopper, RandomWalker, RandomReacher, and Fetch environments respectively at test time. The top plot shows the episodic return w.r.t. the number of environment steps taken so far. The bottom plot shows how much the policy changes at every update by plotting the KL-divergence between the policy distributions before and after every update, w.r.t. the number of updates so far. In all of these environments, the PPO agent learns by observing reward signals whereas at test time, the EPG agent does not observe rewards (note that at test time, αin Eq. (10) equals 0). Observing rewards is not needed in EPG, since any piece of information the agent encounters forms an input to the EPG loss function. As long as the agent can identify which task to solve within the distribution, it does not matter whether this identiﬁcation is done through observations or rewards. Keep in mind, however, that the rewards were used in the ES objective function during the EPG evolution phase. In all experiments, EPG agents learn more quickly and obtain higher returns compared to PPO agents, as expected, since the EPG loss function is able to tailor itself to the environment distribution it is metatrained on. This indicates that our method generates an objective that is more effective at training agents, within these task distributions, than an off-the-shelf on-policy policy gradient method. This is true even though the learned loss does not observe rewards at test time. This demonstrates the potential to use EPG when rewards are only available at training time, for example, if a system were trained in simulation but deployed in the real world where reward signals are hard to measure. The correlation between the gradients of our learned loss and the PPO objective is around ρ= 0.5(Spearman’s rank correlation coefﬁcient) for the environments tested. This indicates that the gradients produced by the learned loss are related to, but different from, those produced by the PPO objective.Evolved Policy Gradients Figure 5: RandomHopper test-time training over 128(policy updates)×64 (update frequency) = 8196 timesteps: PPO vs no-reward EPG Figure 6: RandomWalker test-time training over 256 (policy updates) ×128 (update frequency) = 32768 timesteps: PPO vs no-reward EPG Figure 7: RandomReacher test-time training over 512(policy updates)×128 (update frequency) = 65536 timesteps: PG vs no-reward EPG. Figure 8: DirectionalHopper environment: each Hopper environment randomly decides whether to reward forward or backward hopping. The agent needs to identify whether to jump forward or backwards: PPO vs EPG. Here we can clearly see exploratory behavior, indicated by the negative spikes in the reward curve, the loss forces the policy to try out backwards behavior. Each subplot column corresponds to a different randomization of the environment. Figure 9: Comparison with MAML (single gradient step after metalearning a policy initialization) on the DirectionalHalfCheetah environment from Finn et al. (11) (Fig. 5)Evolved Policy Gradients Figure 10: GoalAnt test-time training over512(policy updates)×32(update frequency) = 16384 timesteps: PPO vs EPG Figure 11: Fetch reaching environment learning over 256 (policy updates)×32(update frequency) = 8192 timesteps: PPO vs no-reward EPG Figure 12: Transferring EPG (metalearned using 128policy updates on RandomHopper) to 1536 updates at test time: random policy initialization (A), initialization by sampled previous policies (B) Figures 8, 9, and 10 show experiments in which a signaling ﬂag is required to identify the environment. Generally, this is done through a reward function or an observation ﬂag, which is why EPG takes the reward as input in case the state space is partially-observed. Similar to the previous experiments, EPG signiﬁcantly outperforms PPO on the task distribution it is metatrained on. Speciﬁcally, in Figure 9, we compare EPG with both MAML (data from ( 11)) and RL2(10). This experiment shows that, at least in this experimental setup, starting from a random policy initialization can bring as much beneﬁt as learning a good policy initialization (MAML). In Section 4.2, we will investigate what the effect of evolving the policy initialization together with the loss parameters is. When comparing EPG to RL2(a method that learns a recurrent policy that does not reset the internal state upon trajectory resets), we see that RL2solves the DirectionalHalfCheetah task almost instantly through system identiﬁcation. By learning both the algorithm and the policy initialization simultaneously, it is able to significantly outperform both MAML and EPG. However, this comes at the cost of generalization power, as we will discuss in Section 4.3. 4.2. Analysis In this section, we ﬁrst analyze whether EPG produces a loss function that encourages exploration and adaptivepolicy updates during test-time training. Next, we evaluate the effect of evolving the policy initialization. Learning exploratory behavior Without additional exploratory incentives, PG methods lead to suboptimal policies. To understand whether EPG is able to train agents that explore, we test our method and PPO on the DirectionalHopper and GoalAnt environments. In DirectionalHopper, each sampled Hopper environment either rewards the agent for forward or backward hopping. Note that without observing the reward, the agent cannot infer whether the Hopper environment desires forward or backward hopping. Thus we augment the environment reward to the input batches of the loss function in this setting. Figure 8 shows learning curves of both PPO agents and agents trained with the learned loss in the DirectionalHopper environment. The learning curves give indication that the learned loss is able to train agents that exhibit exploratory behavior. We see that in most instances, PPO agents stagnate in learning, while agents trained with our learned loss manage to explore both forward and backward hopping and eventually hop in the correct direction. Figure 8 (right) demonstrates the qualitative behavior of our agent during learning and Figure 14 visualizes the exploratory behavior. We see that the hopper ﬁrst explores one hopping direction before learning to hop backwards.Evolved Policy Gradients (a) RandomWalker  (b) DirectionalHalfCheetah  (c) GoalAnt Figure 13: Effect of evolving the policy initialization (+I) on various randomized environments. test-time training curves with evolved policy initialization start at the same return value as those without evolved initialization. This is consistent with MAML trained on a wide task distribution (Figure 5 of (11)). Figure 14: Example of learning to hop backward from a randomly initialized policy in a DirectionalHopper environment. From left to right, trajectories are recorded as learning progresses. Figure 15: Trajectories sampled from test-time training on two sampled GoalAnt environments: the Ant learns how to explore various directions before going to the correct target. Lighter colors represent initial trajectories, darker colors are later trajectories, according to the agent’s learning process. The GoalAnt environment randomizes the location of the goal. We augment the goal location to the input batches of the loss function. Figure 15 demonstrates the exploratory behavior of a learning ant trained by EPG. We see that the ant ﬁrst learns to walk and explore various directions, before ﬁnally converging on the correct goal location. The ant ﬁrst explores in various directions, including the opposite direction of the target location. However, it quickly ﬁgures out in which quadrant to explore, before it fully learns the correct direction to walk in. Learning adaptive policy updates PG methods such as REINFORCE ( 60) suffer from unstable learning, such thata large learning step size leads to policy crashing during learning. To encourage smooth policy updates, methods such as TRPO ( 44) and PPO ( 45) were proposed to limit the distributional change from each policy update, through a hyperparameter constraining the KL-divergence between the policy distributions before and after each update. We demonstrate that EPG produces learned loss that adaptively scales the gradient updates. Figure 16: EPG on the RandomHopper environment: the KL-divergence between the policy before and after an update at the ﬁrst epoch (left) vs the ﬁnal epoch (right), w.r.t the number of updates so far, for a single inner loop run. These curves are generated with α= 0in Eq. (10). Figure 16 shows the KL-divergence between policies from one update to the next during the course of training in RandomHopper, using a randomly initialized loss (left) versus a learned loss produced by Algorithm 1 (right). With a learned loss function, the policy updates tend to shift the policy distribution less on each step, but sometimes produce sudden changes, indicated by the spikes. These spikes are highly noticeable in Figure 22 of Appendix B, in which we plot individual test-time training curves for several randomized environments. The loss function has evolved in such a way to adapt its gradient magnitude to the current agent state: for example in the DirectionalHalfCheetah experiments, the agent ﬁrst ramps up its velocity in one direction (visible by a increasing KL-divergence) until it realizes whether it isEvolved Policy Gradients (a)2layers of 256 tanh units  (b)2layers of 64ReLU units  (c)4layers of 64 tanh units Figure 17: Transferring EPG (metalearned using 2-layer 64 tanh -unit policies on RandomWalker as in Figure 6) to policies of unseen conﬁgurations at test time going in the right/wrong direction. Then it either further ramps up the velocity through stronger gradients, or emits a turning signal via a strong gradient spike (e.g., visible by the spikes in Figure 22 (a) in column three). In other experiments, such as Figures 8, 9, and 10, we see a similar pattern. Based on the agent’s learning history, the gradient magnitudes are scaled accordingly. Often the gradient will be small initially, and it gets increasingly larger the more environment information it has encountered. Effect of evolving policy initialization Prior works such as MAML ( 11) metalearn the policy initialization over a task distribution. While our proposed method, EPG, evolves the loss function parameters, we can also augment Algorithm 1 with simultaneously evolving the policy initialization in the ES outer loop. We investigate the beneﬁts of evolving the policy initialization on top of EPG and PPO on our randomized environments. Figure 13 shows the comparison between EPG, EPG with evolved policy initialization (EPG+I), PPO, and PPO with evolved policy initialization (PPO+I). Evolving the policy initialization seems to help the most when the environments require little exploration, such as RandomWalker. However, the initialization plays a far less important role in DirectionalHalfCheetah and especially the GoalAnt environment. Hence the smaller performance difference between EPG and EPG+I. Another interesting observation is that evolving the policy initialization, together with the EPG loss function (EPG+I),leads to qualitatively different behavior than PPO+I. In PPO+I, the initialization enables fast learning initially, before the return curves saturate. Obtaining a policy initialization that performs well without learning updates was impossible, since there is no single initialization that performs well for all tasksMsampled from the task distribution p(M). In the EPG case however, we see that the return curves are often lower initially, but higher at the end of learning. By feeding the ﬁnal return value as the objective function to the ES outer loop, the algorithm is able to avoid myopic return optimization. EPG+I sets the policy up for initial exploratory behavior which, although not beneﬁcial in the short term, improves ultimate agent behavior. 4.3. Generalization Key components of Algorithm 1 include inner-loop training horizonU, the agent’s policy architecture πθ, and the task distribution p(M). In this section, we investigate the testtime generalization properties of EPG: generalization to longer training horizons, to different policy architectures, and to out-of-distribution tasks. Longer training horizons We evaluate the effect of transferring to longer agent training periods at test time on the RandomHopper environment by increasing the test-time training steps Uin Algorithm 2 beyond the inner-loop training stepsUof Algorithm 1. Figure 12 (A) shows that the learning curve declines and eventually crashes past theEvolved Policy Gradients (a) Task illustration (b) Right direction (as metatrained)  (c) Left direction (generalization) Figure 18: Generalization in the GoalAnt experiment: the ant has only been metatrained to reach target on the positive x-axis (its right side). Can it generalize to targets on the negative x-axis (its left side)? train-time horizon, which demonstrates that Algorithm 1 has limited generalization beyond EPG’s inner-loop training steps. However, we can overcome this limitation by initializing each inner-loop policy with randomly sampled policies that have been obtained by inner-loop training in past epochs. Figure 12 (B) illustrates continued learning past the train-time horizon, validating that this modiﬁcation effectively makes the learned loss function robust to longer training length at test time. Different policy architectures We evaluate EPG’s transfer to different policy architectures by varying the number of hidden layers, the activation function, and hidden units of the agent’s policy at test time (Algorithm 2), while keeping the agent’s policy ﬁxed at 2-layer with 64 tanh units during training time (Algorithm 1) on the RandomWalker environment. The test-time training curves on varied policy architectures are shown in Figure 17. Compared to the learning curve Figure 6 with the same train-time and testtime policy architecture, the transfer performance is inferior. However, we still see that EPG produces a learned loss function that generalizes to policies other than it was trained on, achieving non-trivial walking behavior. Out-of-distribution task learning We evaluate generalization to out-of-distribution task learning on the GoalAnt environment. During metatraining, goals are randomly sampled on the positive x-axis (ant walking to the right) and at test time, we sample goals from the negative x-axis (ant walking to the left). Achieving generalization to the left side is not trivial, since it may be easy for a metalearner to overﬁt to the task metatraining distribution. Figure 18 (a) illustrates this generalization task. We compare the performance of EPG against MAML ( 11) and RL2(10). Since PPO is not metatrained, there is no difference between both directions. Therefore, the performance of PPO is the same as shown in Figure 10. First, we evaluate all metalearning methods’ performancewhen the test-time task is sampled from the training-time task distribution. Figure 18 (b) shows the test-time training curve of both RL2and EPG when the test-time goals are sampled from the positive x-axis. As expected, RL2solves this task extremely fast, since it couples both the learning algorithm and the policy. EPG performs very well on this task as well, learning an effective policy from scratch (random initialization) in 8192 steps, with ﬁnal performance matching that of RL2. MAML achieves approximately the same ﬁnal performance after taking a single SGD step (based on 8192 sampled steps). Next, we look at the generalization setting with test-time goals sampled from the negative x-axis. Figure 18 (c) displays the test-time training curves of both methods. RL2 seems to have completely overﬁt to the task distribution, it has not succeeded in learning a general learning algorithm. Note that, although the RL2agent still walks in the wrong direction, it does so at a lower speed, indicating that it notices a deviation from the expected reward signal. When looking at MAML, we see that MAML has also overﬁt to the metatraining distribution, resulting in a walking speed in the wrong direction similar to the non-generalization setting. The plot also depicts the result of performing 10gradient updates from the MAML init, denoted MAML10 (note that each gradient update uses a batch of 8192 steps). With multiple gradient steps, MAML is able to outperform RL2, consistent with ( 12), but still learns at a far slower rate than EPG (in terms of number of timesteps of experience required). MAML can achieve this because it uses a standard PG learning algorithm to make progress beyond its init, and therefore enjoys the generalization property of generic PG methods. In contrast, EPG evolves a loss function that trains agents to quickly reach goals sampled from negative x-axis, never seen during metatraining. This demonstrates rudimentary generalization properties, as may be expected from learning a loss function that is decoupled from the policy. Figure 15 also shows trajectories sampled during the EPG learningEvolved Policy Gradients process for this exact experimental setup.2 5. Relation to Existing Literature The concept of learning an algorithm for learning is quite general, and hence there exists a large body of somewhat disconnected literature on the topic. To begin with, there are several relevant and recent publications in the metalearning literature ( 11;10;59;25). In ( 11), an algorithm named MAML is introduced. MAML treats the metalearning problem as in initialization problem. More speciﬁcally, MAML attempts to ﬁnd a policy initialization from which only a minimal number of policy gradient steps are required to solve new tasks. This is accomplished by performing gradient descent on the original policy parameters with respect to the post policy update rewards. In Section 4.1 of Finn et al. ( 13), learning the MAML loss via gradient descent is proposed. Their loss has a more restricted formulation than EPG and relies on loss differentiability with respect to the objective function. In a work concurrent with ours, Yu et al. ( 62) extended the model from ( 13) to incorporate a more elaborate learned loss function. The proposed loss involves temporal convolutions over trajectories of experience, similar to the method proposed in this paper. However, unlike our work, ( 62) primarily considers the problem of behavioral cloning. Typically, this means their method will require demonstrations, in contrast to our method which does not. Further, their outer objective does not require sequential reasoning and must be differentiable and their inner loop is a single SGD step. We have no such restrictions. Our outer objective is long horizon and non-differentiable and consequently our inner loop can run over tens of thousands of timesteps. Another recent metalearning algorithm is RL2(10) (and related methods such as ( 59) and ( 25)). RL2is essentially a recurrent policy learning over a task distribution. The policy receives ﬂags from the environment marking the end of episodes. Using these ﬂags and simultaneously ingesting data for several different tasks, it learns how to compute gradient updates through its internal logic. RL2is limited by its decision to couple the policy and learning algorithm (using recurrency for both), whereas we decouple these components. Due to RL2’s policy-gradient-based optimization procedure, we see that it does not directly optimize ﬁnal policy performance nor exhibit exploration. Hence, extensions have been proposed such as E-RL2(53) in which the rewards of episodes sampled early in the learning process are deliberately set to zero to drive exploratory behavior. Further research on meta reinforcement learning comprises 2A demonstration can be viewed at http://blog.openai. com/evolved-policy-gradients/ .a vast selection. The literature’s vastness is further complicated by the fact that the research appears under many different headings. Speciﬁcally, there exist relevant literature on: life-long learning, learning to learn, continual learning, and multi-task learning. For example, ( 41;40) consider self-modifying learning machines (genetic programs). If we consider a genetic program that itself modiﬁes the learned genetic program, we can subsequently derive a meta-GP approach (See ( 53), for further discussion on how this method relates to the more recent metalearning literature discussed above). The method described above is sufﬁciently general that it encompass most modern metalearning approaches. For a further review of other metalearning approaches, see the review articles ( 48;57;58) and citation graph they generate. There are several other avenues of related work that tackle slightly different problems. For instance, several methods attempt to learn a reward function to drive learning. See (7) (which suggests learning from human feedback) and the ﬁeld of Inverse Reinforcement Learning ( 28) (which recovers the reward from demonstrations). Both of these ﬁelds relate to our ideas on loss function learning. Similarly, (29;30) apply population-based evolutionary algorithms to reward function learning in gridworld environments. This algorithm is encompassed by the algorithms we present in this paper. However, it is typically much easier since learning just the reward function is in many cases a trivial task (e.g., in learning to walk, mapping the observation of distance to a reward function). See also ( 49;50) and ( 1) for additional evolutionary perspectives on reward learning. Other reward learning methods include the work of Guo et al. ( 14), which focuses on learning reward bonuses, and the work of Sorg et al. (51), which focuses on learning reward functions through gradient descent. These bonuses are typically designed to augment but not replace the learned reward and have not been shown to easily generalize across broad task distributions. Reward bonuses are closely linked to the idea of curiosity, in which an agent attempts to learn an internal reward signal to drive future exploration. Schmidhuber ( 39) was perhaps the ﬁrst to examine the problem of intrinsic motivation in a metalearning context. The proposed algorithms make use of dynamic programming to explicitly partition experience into checkpoints. Further, there is usually little focus on metalearning the curiosity signal across several different tasks. Finally, the work of ( 17;61;2;22;24) studies metalearning over the optimization process in which metalearner makes explicit updates to a parametrized model in supervised settings. Also worth mentioning is that approaches such as UVFA (38) and HER ( 3), which learn a universal goal-directed value function, somewhat resemble EPG in the sense that their critic could be interpreted as a sort of loss function that is learned according to a speciﬁc set of rules. Furthermore, in DDPG ( 23), the critic can be interpreted in a similarEvolved Policy Gradients way since it also makes use of back-propagation through a learned function into a policy network. 6. Discussion In this paper, we introduced a new metalearning approach capable of learning a differentiable loss function over thousands of sequential environmental actions. Crucially, this learned loss is both adaptive (allowing for quicker learning of new tasks) and instructive (sometimes eliminating the need for environmental rewards at test time), while exhibiting stronger generalization properties than contemporary metalearning methods. In certain cases, the adaptability of our learned loss is appreciated. For example, consider the DirectionalHopper experiments from Section 4. Here, the rewards at test time are impossible to infer from observations of the environment alone. Therefore, they cannot be completely internalized. However, when we do get to observe a reward signal on these environments, then EPG does improve learning speed. Meanwhile, in most other cases, our loss’ instructive nature – which allows it to operate at test time without environmental rewards – is interesting and desirable. This instructive nature can be understood as the loss function’s internalization of the reward structures it has previously encountered under the training task distribution. We see this internalization as a step toward learning intrinsic motivation. A good intrinsically motivated agent would successfully infer useful actions in new situations by using heuristics it developed over its entire lifetime. This ability is likely required to achieve truly intelligent agents (39). Furthermore, through decoupling of the policy and learning algorithm, EPG shows rudimentary generalization properties that go beyond current metalearning methods such as RL2. Improving the generalization ability of EPG, as well other other metalearning algorithms, will be an important component of future work. Right now, we can train an EPG loss to be effective for one small family of tasks at a time, e.g., getting an ant to walk left and right. However, the EPG loss for this family of tasks is unlikely to be at all effective on a wildly different kind of task, like playing Space Invaders. In contrast, standard RL losses do have this level of generality – the same loss function can be used to learn a huge variety of skills. EPG gains on performance by losing on generality. There may be a long road ahead toward metalearning methods that both outperform standard RL methods and have the same level of generality. Improving computational efﬁciency is another important direction for future work. EPG demands sequential learning. That is to say, one must ﬁrst perform outer loop update i before learning about update i+ 1. This can bottleneckthe metalearning cycle and create large computational demands. Indeed, the number of sequential steps for each inner-loop worker in our algorithm is E×U, using notation from Algorithm 1. In practice, this value may be very high, for example, each inner-loop worker takes approximately 196 million steps to evolve the loss function used in the RandomReacher experiments (Figure 7). Finding ways to parallelize parts of this process, or increase sample efﬁciency, could greatly improve the practical applicability of our algorithm. Improvements in computational efﬁciency would also allow the investigation of more challenging tasks. Nevertheless, we feel the success on the environments we tested is non-trivial and provides a proof of concept of our method’s power. Acknowledgments We thank Igor Mordatch, Ilya Sutskever, John Schulman, and Karthik Narasimhan for helpful comments and conversations. We thank Maruan Al-Shedivat for assisting with the random MuJoCo environments. 