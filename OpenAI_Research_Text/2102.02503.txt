arXiv:2102.02503v1  [cs.CL]  4 Feb 2021Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models Alex Tamkin∗1, Miles Brundage∗2, Jack Clark†3, and Deep Ganguli1,3 1Stanford University2OpenAI3AI Index Introduction On October 14th, 2020, researchers from OpenAI, the Stanfor d Institute for Human-Centered Artiﬁcial Intelligence, and other universities con vened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants ca me from a variety of research backgrounds including computer science, lingu istics, philosophy, political science, communications, cyber policy, and more. Br oadly, the discussion centered around two main questions: 1.What are the technical capabilities and limitations of larg e language models? The discussion touched on several key areas including: the surprising impact of scale on model capabilities, the diﬃculty in assessing whether large language models truly understand language, the importance of training models on multiple data modalities, and challenges in aligning model objectives with human values. 2.What are the societal eﬀects of widespread use of large langu age models? The discussion touched on several key areas including: diﬃculties in scoping all possible uses (or misuses) of general purpose lang uage models, challenges organizations may face in model deployment, the potential for these models to algorithmically spread disinformation, diﬃ culties in mitigating model bias (e.g., racial, gender, religious, etc.), and t he impact of language model-based automation on the labor market. While the conversation was collegial and productive, there was a sen se of urgency to make progress sooner than later in answering these ques tions. Here, ∗Equal contribution †Work carried out while employed at OpenAI 1we provide a detailed summary of the discussion organized by the two themes above.1We conclude with a list of potential future research directions inspir ed by the discussion. 1 Technical Capabilities and Limitations Scale GPT-3 is one of the largest publicly-disclosed language models — it has 1 75 billion parameters and was trained on 570 gigabytes of text. For com parison, its predecessor, GPT-2 (which is functionally similar to GPT-3) has 1.5 billion parameters and was trained on 40 gigabytes of text. While GPT-2 dis played somezero-shotgeneralizationtodownstreamtasks, GPT-3furt herdisplayedthe ability to learn more novel tasks when given examples in context. Par ticipants found it remarkable that such capabilities emerge merely from scaling model and training data size. One person remarked that the growth in model capabilities as they s cale “feels like a law of physicsor thermodynamics” in its stability and predictability . Several participants were optimistic that these trends would continue even for models much larger than GPT-3, yielding ever-stronger models capable o f more advanced few-shot learningofnew skills from asmall number oftraining examples. One participant remarked that the scale of models like GPT-3 was rem iniscent of large particle accelerator experiments, which require many peop le with diverse backgrounds to execute. For example, when training such la rge models, diﬀerentteamswithdiverseexpertisemustcollaboratetorunexpe riments, build and maintain the computing infrastructure, develop the algorithms , and continuously interrogate the model’s capabilities for possible problems (e .g., bias, misuse, safety concerns, etc.). The latter point is referred to as “red-teaming” throughout the rest of this document. Understanding What constitutes “understanding” in a language model, and does GP T-3 fulﬁll this deﬁnition? Some leaned towards deﬁnitions based on strong n otions of intelligence, which require models to possess intentionality or the a bility to 1Since this is a summary of discussions, rather than a researc h paper, we do not include 