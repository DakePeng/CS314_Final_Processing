arXiv:2102.02503v1  [cs.CL]  4 Feb 2021Understanding the Capabilities, Limitations, and
Societal Impact of Large Language Models
Alex Tamkin∗1, Miles Brundage∗2,
Jack Clark†3, and Deep Ganguli1,3
1Stanford University2OpenAI3AI Index
Introduction
On October 14th, 2020, researchers from OpenAI, the Stanfor d Institute for
Human-Centered Artiﬁcial Intelligence, and other universities con vened to dis-
cuss open research questions surrounding GPT-3, the largest publicly-disclosed
dense language model at the time.
The meeting took place under Chatham House Rules. Discussants ca me from a
variety of research backgrounds including computer science, lingu istics, philos-
ophy, political science, communications, cyber policy, and more. Br oadly, the
discussion centered around two main questions:
1.What are the technical capabilities and limitations of larg e lan-
guage models? The discussion touched on several key areas including:
the surprising impact of scale on model capabilities, the diﬃculty in as-
sessing whether large language models truly understand language, the im-
portance of training models on multiple data modalities, and challenges
in aligning model objectives with human values.
2.What are the societal eﬀects of widespread use of large langu age
models? The discussion touched on several key areas including: diﬃcul-
ties in scoping all possible uses (or misuses) of general purpose lang uage
models, challenges organizations may face in model deployment, the po-
tential for these models to algorithmically spread disinformation, diﬃ cul-
ties in mitigating model bias (e.g., racial, gender, religious, etc.), and t he
impact of language model-based automation on the labor market.
While the conversation was collegial and productive, there was a sen se of ur-
gency to make progress sooner than later in answering these ques tions. Here,
∗Equal contribution
†Work carried out while employed at OpenAI
1we provide a detailed summary of the discussion organized by the two themes
above.1We conclude with a list of potential future research directions inspir ed
by the discussion.
1 Technical Capabilities and Limitations
Scale
GPT-3 is one of the largest publicly-disclosed language models — it has 1 75
billion parameters and was trained on 570 gigabytes of text. For com parison,
its predecessor, GPT-2 (which is functionally similar to GPT-3) has 1.5 billion
parameters and was trained on 40 gigabytes of text. While GPT-2 dis played
somezero-shotgeneralizationtodownstreamtasks, GPT-3furt herdisplayedthe
ability to learn more novel tasks when given examples in context. Par ticipants
found it remarkable that such capabilities emerge merely from scaling model
and training data size.
One person remarked that the growth in model capabilities as they s cale “feels
like a law of physicsor thermodynamics” in its stability and predictability . Sev-
eral participants were optimistic that these trends would continue even for mod-
els much larger than GPT-3, yielding ever-stronger models capable o f more ad-
vanced few-shot learningofnew skills from asmall number oftraining examples.
One participant remarked that the scale of models like GPT-3 was rem iniscent
of large particle accelerator experiments, which require many peop le with di-
verse backgrounds to execute. For example, when training such la rge models,
diﬀerentteamswithdiverseexpertisemustcollaboratetorunexpe riments, build
and maintain the computing infrastructure, develop the algorithms , and con-
tinuously interrogate the model’s capabilities for possible problems (e .g., bias,
misuse, safety concerns, etc.). The latter point is referred to as “red-teaming”
throughout the rest of this document.
Understanding
What constitutes “understanding” in a language model, and does GP T-3 ful-
ﬁll this deﬁnition? Some leaned towards deﬁnitions based on strong n otions
of intelligence, which require models to possess intentionality or the a bility to
1Since this is a summary of discussions, rather than a researc h paper, we do not include
references. Rather, we hyperlink to relevant papers that we re discussed at the workshop. For
a more comprehensive set of references related to some of the se issues, we point readers to the
original GPT-3 paper and to recent work of Bender and Gebru et al published a few months
after this workshop.
2respond to requests in the real world. Others suggested that th ere were even
weaker notions of intelligence that models had yet to satisfy, includin g robust-
ness to adversarial examples — data examples that easily confuse a n AI system
but not humans. Participants suggested that getting things “mos tly right” may
not be suﬃcient for understanding if the model performs poorly on rare but
important inputs.
Another deﬁnition of understanding centered around the notion o f causality, in
that models that truly understand should grasp the causal relatio nship between
features of the data and the desired behavior. Some argued that language
models were destined to exploit “spurious correlations” or “shortc ut features”
inherent in the data, and thus lacked a true underlying causal mode l. However,
one participant suggested a diﬀerent view — that with enough data, language
models could encounter “natural experiments” that could enable t he model to
learn causal relationships from observationaldata in a similar manne r as human
economists often do in their research.
Some participants argued against binary thresholds for understa nding, recall-
ing that children and adults gradually acquire greater mastery over time. For
example, one participant quoted a prominent physicist who quipped t hat he
only understood thermodynamics the third time he taught it. Anoth er par-
ticipant pushed back against singular notions of understanding, no ting debates
between linguists and philosophers about whether meaning is derived from the
relationship of expressions to each other or to some external gro und truth.
Finally, someparticipantsoﬀeredresistancetothefocusonunder standing, argu-
ing that humans are able to accomplish many tasks with mediocre or ev en poor
understanding, including a non-French speaker who recently won t he French
Scrabble championships. Some gently suggested that perhaps a ju dgment about
whether GPT-3 understands language in the relevant way is irreleva nt to suc-
cessful performance of tasks.
In a memorable line, one participant also remarked on the inverse pro blem of
humans’ ability to understand large language models: “GPT-3 is comp letely
alien...it’s the ﬁrst thing I’ve seen where it’s not a dumb thing to ask w hether
it’s AGI.” Here, AGI refers to Artiﬁcial General Intelligence, or the ability of a
machine to learn and understand anything a human can.
Multimodality
Much of the conversation considered the importance of multimodal models —
language models trained on text and data from other modalities, e.g., images,
audio recordings, etc. Participants largely agreed in their predictio ns that large
multimodalmodels will becomemoreprevalentandenablemorediverse capabil-
3ities.2However,somearguedthatGPT-3isalreadytrainedonmultimodald ata,
in that the training data contains prose, structured data tables, and computer
code. Others suggested that the main beneﬁt of multimodal trainin g might be
to improve the speed at which models acquire useful capabilities, as t he inter-
action between diﬀerent data modalities may provide a stronger lear ning signal
than each data modality in isolation provides. Finally, some commented that
no single additional modality was critical to language use, given that h umans
diﬀer in the range of sensory modalities they have access to.
Alignment
Participants discussed the need to better align model objectives w ith human
values. For example, one participant mentioned some language mode ls treat
all symbols (e.g., nouns, prepositions, numbers, etc.) equally, but h umans care
much more about, for example, incorrectly stating someone’s age t han about
misplacing a preposition. Several other participants emphasized th e importance
and challenge of better optimizing for factual accuracy and robus tness to adver-
sarial examples. Aligning human and model objectives was seen to be especially
importantfor“embodied”AI agentswhich learnthroughactiveinte ractionwith
their environment. Discussants emphasized the dual importance o f developing
better algorithms for “steering” agents towards human values, a s well as fos-
tering cross-disciplinary collaborations to better clarify what “hum an values”
means, especially given diversity across individuals and communities an d the
prevalence of bias in available datasets.
2 Eﬀects of Widespread Use
Capabilities
GPT-3 has an unusually large set of capabilities, including text summar ization,
chatbotbehavior, search,code generation,and essaygenerat ion. Onediscussant
stated that such a large “capability surface” makes it challenging to both scope
the full array of uses (because GPT-3 can take in arbitrary inputs , it isa priori
impossible to anticipate all potential behaviors of the model) and to e nsure
their safety to people and societies. Participants noted that, by p utting GPT-3
behind a controlled-access API, OpenAI is able to constrain the mod el’s use
more easily than if they open sourced it. However, open questions r emain.
For example, who gets access and why? How can one provide model a ccess
2In fact, shortly after the workshop, OpenAI released DALL-E , which is a multimodal
version of GPT-3 trained on both images and text.
4to support a large community to red-team (interrogate the model for potential
misuse and develop mitigation strategies) at scale?
Deployment
Participants discussed several options for deﬁning and addressin g the ethical
and societal challenges of deploying large language models. One sugg estion
was to increase the computing resources available to academia so th at it would
be easier for academics to do research that informs the deploymen t of large
language models. Someone suggested that laws requiring disclosure of when AI
is being used to generate text could be helpful in managing the eﬀect s of large
language models. Another participant asked what metrics might be u sed to
evaluate whether language models are having a societally beneﬁcial e ﬀect, and
there was general agreement that this is a challenging but importan t task.
Several participants noted that OpenAI and other organizations will not have a
monopoly on large language models forever. Participants suggeste d that devel-
opers may only have a six- to nine-month advantage until others ca n reproduce
their results. It was widely agreed upon that those on the cutting e dge should
use their position on the frontier to responsibly set norms in the eme rging ﬁeld.
Additionally, some participants pointed out that, due to standard a dvances in
technology, it will only become easier for other actors to replicate m odels like
GPT-3 over time. This further suggests the urgency of using the c urrent time
window, during which few actors possess very large language models , to develop
appropriate norms and principles for others to follow.
Disinformation
A major discussion point considered the deliberate misuse of languag e models
for purposes such as generating disinformation. More speciﬁcally, models like
GPT-3 can be used to create false, misleading, or propagandistic es says, tweets,
and news stories de novo. One participant was skeptical about the magnitude
of these likely risks since many previous technologies (e.g. photogra phy and
Photoshop) sparked similar concerns and have already raised socie tal aware-
ness of the risks of disinformation. Furthermore, while automated generation of
disinformation may be feasible in principle, human labor may still be more cost-
eﬀective for such purposes. Others disagreed, and saw automat ed generation as
much more cost-eﬀective than training and paying humans to gener ate disin-
formation. Participants agreed that empirically investigating the ec onomics of
automated vs human generated disinformation is important.
Thinkingahead,someonesuggestedconsideringafutureinwhichlan guagemod-
elscan generatetext that isnot just coherenton commonlydiscus sed topics, but
5highly persuasive on arbitrary topics. Another participant sugges ted that GPT-
3 or other future language models could make disinformation hard or impossible
to detect at the level of content, forcing reliance on metadata by online plat-
forms. Relatedly, someone suggested that the existence of syst ems like GPT-3
should spur more use of cryptography to authenticate media.
Bias
GPT-3 exhibits several racial, gender, and religious biases. One disc ussant
analogized the diﬃculty of addressing language model bias to the pro blem of
content moderation on online platforms — despite the diﬃcult normat ive issues
in both cases, there are still some areas of relative consensus and opportunities
for mitigation. For example, online platforms agree on the need to ad dress child
pornography or egregious threats of violence, and the concept o f “protected
classes” in discrimination law provides a useful initial framework for t hinking
about some language model biases.
Severalworkshopparticipantsnotedthat it isdiﬃcult todeﬁne wha tit meansto
mitigate bias in large language models in a universal manner, since appr opriate
language use is highly contextual. One participant noted that all dat asets are
biased in some ways, so the challenge is not eliminating all bias but addre ssing
harmful biases according to some set of normative and/or legal cr iteria. Some
suggested that companies like OpenAI do not have the appropriate standing
and should not aim to make such decisions on behalf of society. Someo ne else
observed that it is especially diﬃcult to think about mitigating bias for m ulti-
purpose systems like GPT-3 via changes to their training data, since bias is
typically analyzed in the context of a particular use cases.
Participants discussed a wide variety of possible means of addressin g harmful
biases in language models, including:
•Changes to the initial training data to mitigate bias a priori
•Training a separate model to ﬁlter content generated by a languag e model
•Fine-tuning a large language model on data with desired properties
•Tagging data so that the model learns to distinguish among certain f orms
of content (see e.g. CTRL)
•Training models to be more “fact-aware”
•Reinforcement learning with human feedback
•Leveraging the model’s own knowledge to improve outputs (e.g., with
careful prompt design)
6•Developing more expansive suites of “bias tests” that models can be run
through prior to deployment
•Red-teaming the model at scaleby engagingtrusted partners to w ork with
the model and through limited commercial oﬀerings.
None of these approaches was considered a panacea. For example , steering a
model with human feedback still raises the question of who the huma n labelers
are or how they should be chosen, and content ﬁlters can sometime s undermine
the agency of the very groups that they are intended to protect (e.g., marginal-
ized groups reclaiming words or phrases that are used as slurs by ma jority
groups). One participant argued that keeping a human in the loop of text gen-
erationis criticalforaddressingthese issues. Someparticipantse mphasizedthat
certain use cases should be avoided given the limitations of existing te chniques,
and that text generation applications vary widely in terms of open-e ndedness
and risk. For example, detecting regular expressions is much more t ractable to
do safely than managing a suicide hotline.
Economy
Another theme of the discussion considered the economic implication s of models
like GPT-3. Participants observed that current jobs that involve r eading or
analyzing text vary widely in their desirability, with some being more enj oyable
(e.g., creative writing or reading and summarizing reports) and othe rs often
being traumatizing or alienating (e.g., content moderation). This rais es the
question of when jobs, or what kinds of jobs, should or shouldn’t be automated
by largelanguagemodels. One participant suggestedthat leaving su ch decisions
up to companies would likely have adverse consequences. Education was also
mentioned as a societal area likely to be aﬀected by large language mo dels,
via changes to the essay writing process as well as evaluation of tex t. One
participant pointed out that providing API access to a variety of gr oups from
diﬀerent sectors of society can help provide an early signal of pote ntial societal
changes.
3 Future Research Directions
The following research questions were inspired by the discussion:
•Can we better understand why language models improve so much with
scale? Can this enable us to build models which scale more eﬃciently?
7•What are the limits of scaling? Will scale lead to strong causal reasonin g,
symbolic manipulation, commonsense understanding, and robustne ss to a
wider class of inputs? Or will diﬀerent techniques be necessary?
•How can we understand the limits of what large language models are
capable of? Can we enable models to ask for help or clariﬁcation, or
abstain when they are unsure?
•How can we developnew neural networkarchitecturesand algorith msthat
enable eﬃcient learning from diverse, multimodal data beyond text?
•What are the opportunities and tradeoﬀs involved in diﬀerent appro aches
to steering the outputs of large-scale language models to be more a ligned
with human values?
•How should access to models like GPT-3 be allocated, balancing conside r-
ations like security, replicability, and fairness? What kinds of tests d o we
need to develop in order to qualify language models like GPT-3 as being
safe or unsafe for use in particular contexts?
•What can academia do to best position itself to develop guardrails for the
industrial development of such models - including advocating for suﬃ cient
funding to replicate the compute resources required to train them ?
•How can we best foster cross-disciplinary collaboration to understand and
manage the biases in large datasets and model representations of such
datasets?
•How can we best characterize the potential “threat landscape” f or such
models; e.g., do we need to spend more time worrying about how models
like this could be used by proﬁt-driven actors to generate lots of low -grade
spam, orshouldwebe moreworriedaboutstate-basedactorsusin gmodels
to generate persuasive text for use in disinformation campaigns?
•How cost-eﬀective and skill-intensive would it be for malicious actors t o
misuse language models for various purposes, compared to alterna tive
methods of achieving the same goals?
8