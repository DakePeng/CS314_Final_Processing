arXiv:1706.01502v3  [cs.LG]  7 Nov 2017UCB Exploration via Q-Ensembles Richard Y. Chen OpenAI richardchen@openai.comSzymon Sidor OpenAI szymon@openai.comPieter Abbeel OpenAI University of California, Berkeley pabbeel@cs.berkeley.edu John Schulman OpenAI joschu@openai.com Abstract We show how an ensemble of Q∗-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well e stablished algorithms from the bandit setting, and adapt them to the Q-learning setting. We propose an exploration strategy based on upper-conﬁdence bounds (UCB ). Our experiments show signiﬁcant gains on the Atari benchmark. 1 Introduction Deep reinforcement learning seeks to learn mappings from hi gh-dimensional observations to actions. DeepQ-learning (Mnih et al. ) is a leading technique that has b een used successfully, especially for video game benchmarks. However, fundamental challenge s remain, for example, improving sample efﬁciency and ensuring convergence to high quality s olutions. Provably optimal solutions exist in the bandit setting and for small MDPs, and at the core of these solutions are exploration schemes. However these provably optimal exploration techn iques do not extend to deep RL in a straightforward way. Bootstrapped DQN (Osband et al. ) is a previous attempt a t adapting a theoretically veriﬁed approach to deep RL. In particular, it draws inspiration fro mposterior sampling for reinforcement learning (PSRL, Osband et al. , Osband and Van Roy ), which has near-optimal regret bounds. PSRL samples an MDP from its posterior each episode a nd exactly solves Q∗, its optimal Q-function. However, in high-dimensional settings, both ap proximating the posterior over MDPs and solving the sampled MDP are intractable. Bootstrapped D QN avoids having to establish and sample from the posterior over MDPs by instead approximatin g the posterior over Q∗. In addition, bootstrapped DQN uses a multi-headed neural network to repr esent the Q-ensemble. While the authors proposed bootstrapping to estimate the posterior d istribution, their empirical ﬁndings show best performance is attained by simply relying on different initializations for the different heads, not requiring the sampling-with-replacement process that is p rescribed by bootstrapping. In this paper, we design new algorithms that build on the Q-ensemble approach from Osband et al. . However, instead of using posterior sampling for expl oration, we use the uncertainty estimates from the Q-ensemble. Speciﬁcally, we propose the UCB exploration str ategy. This strategy is inspired by established UCB algorithms in the bandit settin g and constructs uncertainty estimates of theQ-values. In this strategy, agents are optimistic and take ac tions with the highest UCB. We demonstrate that our algorithms signiﬁcantly improve perf ormance on the Atari benchmark. Correspondence to: richardchen@openai.com.2 Background 2.1 Notation We model reinforcement learning as a Markov decision proces s (MDP). We deﬁne an MDP as (S,A,T,R,p 0,γ), in which both the state space Sand action spaceAare discrete, T:S×A×S/mapsto→ R+is the transition distribution, R:S×A/mapsto→ Ris the reward function, and γ∈(0,1]is a discount factor, and p0is the initial state distribution. We denote a transition ex perience as τ= (s,a,r,s′) wheres′∼T(s′|s,a)andr=R(s,a). A policy π:S /mapsto→A speciﬁes the action taken after observing a state. We denote the Q-function for policy πasQπ(s,a) :=Eπ/bracketleftbig/summationtext∞ t=0γtrt|s0= s,a0=a/bracketrightbig . The optimal Q∗-function corresponds to taking the optimal policy Q∗(s,a) := sup πQπ(s,a) and satisﬁes the Bellman equation Q∗(s,a) =Es′∼T(·|s,a)/bracketleftbig r+γ·max a′Q∗(s′,a′)/bracketrightbig . 2.2 Exploration in reinforcement learning A notable early optimality result in reinforcement learnin g was the proof by Watkins and Dayan [27, 26] that an online Q-learning algorithm is guaranteed to converge to the optima l policy, provided that every state is visited an inﬁnite number of times. Howev er, the convergence of Watkins’ Qlearning can be prohibitively slow in MDPs where ǫ-greedy action selection explores state space randomly. Later work developed reinforcement learning alg orithms with provably fast (polynomialtime) convergence (Kearns and Singh , Brafman and Tenne nholtz , Strehl et al. ). At the core of these provably-optimal learning methods is some exploration strategy, which actively encourages the agent to visit novel state-action pairs. For example, R-MAX optimistically assumes that infrequently-visited states provide maximal reward, and delayed Q-learning initializes the Qfunction with high values to ensure that each state-action i s chosen enough times to drive the value down. Since the theoretically sound RL algorithms are not computa tionally practical in the deep RL setting, deep RL implementations often use simple exploration metho ds such as ǫ-greedy and Boltzmann exploration, which are often sample-inefﬁcient and fail to ﬁnd good policies. One common approach of exploration in deep RL is to construct an exploration bonu s, which adds a reward for visiting stateaction pairs that are deemed to be novel or informative. In pa rticular, several prior methods deﬁne an exploration bonus based on a density model or dynamics mod el. Examples include VIME by Houthooft et al. , which uses variational inference on t he forward-dynamics model, and Tang et al. , Bellemare et al. , Ostrovski et al. , Fu et al. . While these methods yield successful exploration in some problems, a major drawback i s that this exploration bonus does not depend on the rewards, so the exploration may focus on irrele vant aspects of the environment, which are unrelated to reward. 2.3 Bayesian reinforcement learning Earlier works on Bayesian reinforcement learning include D earden et al. [7, 8]. Dearden et al.  studied Bayesian Q-learning in the model-free setting and learned the distrib ution ofQ∗-values through Bayesian updates. The prior and posterior speciﬁca tion relied on several simplifying assumptions, some of which are not compatible with the MDP sett ing. Dearden et al.  took a model-based approach that updates the posterior distribut ion of the MDP. The algorithm samples from the MDP posterior multiple times and solving the Q∗values at every step. This approach is only feasible for RL problems with very small state space and action space. Strens  proposed posterior sampling for reinforcement learning (PSRL). PSR L instead takes a single sample of the MDP from the posterior in each episode and solves the Q∗values. Recent works including Osband et al.  and Osband and Van Roy  established nearoptimal Bayesian regret bounds for episodic RL. Sorg et al.  models the environment and cons tructs exploration bonus from variance of model parameters. These methods are experimented on low d imensional problems only, because the computational cost of these methods is intractable for h igh dimensional RL. 22.4 Bootstrapped DQN Inspired by PSRL, but wanting to reduce computational cost, prior work developed approximate methods. Osband et al.  proposed randomized leastsquare value iteration for linearlyparameterized value functions. Bootstrapped DQN Osband et al.  applies to Q-functions parameterized by deep neural networks. Bootstrapped DQN (Osband et al. ) maintains a Q-ensemble, represented by a multi-head neural net structure to paramet erizeK∈N+Q-functions. This multihead structure shares the convolution layers but includes m ultiple “heads”, each of which deﬁnes a Q-function Qk. Bootstrapped DQN diversiﬁes the Q-ensemble through two mechanisms. The ﬁrst mechanism is independent initialization. The second mechanism applies different samples to train each Q-function. TheseQ-functions can be trained simultaneously by combining thei r loss functions with the help of a random mask mτ∈RK + L=/summationdisplay τ∈Bmini/summationdisplayK k=1mk τ·(Qk(s,a;θ)−yQkτ)2, whereyQkτis the target of the kthQ-function. Thus, the transition τupdatesQkonly ifmk τis nonzero. To avoid the overestimation issue in DQN, bootstra pped DQN calculates the target value yQkτusing the approach of Double DQN (Van Hasselt et al. ), su ch that the current Qk(·;θt) network determines the optimal action and the target networ kQk(·;θ−)estimates the value yQkτ=r+γmax aQk(s′,argmax aQk(s′,a;θt);θ−). In their experiments on Atari games, Osband et al.  set th e maskmτ= (1,...,1)such that all{Qk}are trained with the same samples and their only difference i s initialization. Bootstrapped DQN picks one Qkuniformly at random at the start of an episode and follows the greedy action at= argmaxaQk(st,a)for the whole episode. 3 Approximating Bayesian Q-learning with Q-Ensembles Ignoring computational costs, the ideal Bayesian approach to reinforcement learning is to maintain a posterior over the MDP. However, with limited computation a nd model capacity, it is more tractable to maintain a posterior of the Q∗-function. In this section, we ﬁrst derive a posterior updat e formula for theQ∗-function under full exploration assumption and this formu la turns out to depend on the transition Markov chain (Section 3.1). The Bellman equatio n emerges as an approximation of the log-likelihood. This motivates using a Q-ensemble as a particle-based approach to approximate the posterior over Q∗-function and an Ensemble V oting algorithm (Section 3.2). 3.1 Bayesian update for Q∗ An MDP is speciﬁed by the transition probability Tand the reward function R. Unlike prior works outlined in Section 2.3 which learned the posterior of the MD P, we will consider the joint distribution over(Q∗,T). Note that Rcan be recovered from Q∗givenT. So(Q∗,T)determines a unique MDP. In this section, we assume that the agent samples (s,a)according to a ﬁxed distribution. The corresponding reward rand next state s′given by the MDP append to (s,a)to form a transition τ= (s,a,r,s′), for updating the posterior of (Q∗,T). Recall that the Q∗-function satisﬁes the Bellman equation Q(s,a) =r+Es′∼T(·|s,a)/bracketleftBig γmax a′Q(s′,a′)/bracketrightBig . Denote the joint prior distribution as p(Q∗,T)and the posterior as ˜p. We apply Bayes’ formula to expand the posterior: ˜p(Q∗,T|τ) =p(τ|Q∗,T)·p(Q∗,T) Z(τ) =p(Q∗,T)·p(s′|Q∗,T,(s,a))·p(r|Q∗,T,(s,a,s′))·p(s,a) Z(τ), (1) 3whereZ(τ)is a normalizing constant and the second equality is because sandaare sampled randomly fromSandA. Next, we calculate the two conditional probabilities in (1 ). First, p(s′|Q∗,T,(s,a)) =p(s′|T,(s,a)) =T(s′|s,a), (2) where the ﬁrst equality is because given T,Q∗does not inﬂuence the transition. Second, p(r|Q∗,T,(s,a,s′)) =p(r|Q∗,T,(s,a)) = /BD{Q∗(s,a)=r+γ·Es′′∼T(·|s,a)maxa′Q∗(s′′,a′)} := /BD(Q∗,T), (3) where /BD{·}is the indicator function and in the last equation we abbrevi ate it as /BD(Q∗,T). Substituting (2) and (3) into (1), we obtain the joint posterior of Q∗andTafter observing an additional randomly sampled transition τ ˜p(Q∗,T|τ) =p(Q∗,T)·T(s′|s,a)·p(s,a) Z(τ)· /BD(Q∗,T). (4) We point out that the exact Q∗-posterior update (4) is intractable in high-dimensional R L due to the large space of (Q∗,T). 3.2Q-learning with Q-ensembles In this section, we make several approximations to the Q∗-posterior update and derive a tractable algorithm. First, we approximate the prior of Q∗by sampling K∈N+independently initialized Q∗-functions{Qk}K k=1. Next, we update them as more transitions are sampled. The re sulting {Qk}approximate samples drawn from the posterior. The agent cho oses the action by taking a majority vote from the actions determined by each Qk. We display our method, Ensemble V oting, in Algorithm 1. We derive the update rule for {Qk}after observing a new transition τ= (s,a,r,s′). At iteration i, givenQ∗=Qk,ithe joint probability of (Q∗,T)factors into p(Qk,i,T) =p(Q∗,T|Q∗=Qk,i) =p(T|Qk,i). (5) Substitute (5) into (4) and we obtain the corresponding post erior for each Qk,i+1at iteration i+1as ˜p(Qk,i+1,T|τ) =p(T|Qk,i)·T(s′|s,a)·p(s,a) Z(τ)· /BD(Qk,i+1,T). (6) ˜p(Qk,i+1|τ) =/integraldisplay T˜p(Qk,i+1,T|τ)dT=p(s,a)·/integraldisplay T˜p(T|Qk,i,τ)· /BD(Qk,i+1,T)dT. (7) We update Qk,itoQk,i+1according to Qk,i+1←argmax Qk,i+1˜p(Qk,i+1|τ). (8) We ﬁrst derive a lower bound of the the posterior ˜p(Qk,i+1|τ): ˜p(Qk,i+1|τ) =p(s,a)·ET∼˜p(T|Qk,i,τ) /BD(Qk,i+1,T) =p(s,a)·ET∼˜p(T|Qk,i,τ)lim c→+∞exp/parenleftbig −c[Qk,i+1(s,a)−r−γEs′′∼T(·|s,a)max a′Qk,i+1(s′′,a′)]2/parenrightbig =p(s,a)·lim c→+∞ET∼˜p(T|Qk,i,τ)exp/parenleftbig −c[Qk,i+1(s,a)−r−γEs′′∼T(·|s,a)max a′Qk,i+1(s′′,a′)]2/parenrightbig ≥p(s,a)·lim c→+∞exp/parenleftbig −cET∼˜p(T|Qk,i,τ)[Qk,i+1(s,a)−r−γEs′′∼T(·|s,a)max a′Qk,i+1(s′′,a′)]2/parenrightbig =p(s,a)· /BDET∼˜p(T|Qk,i,τ)[Qk,i+1(s,a)−r−γEs′′∼T(·|s,a)maxa′Qk,i+1(s′′,a′)]2=0. (9) where we apply a limit representation of the indicator funct ion in the third equation. The fourth equation is due to the bounded convergence theorem. The ineq uality is Jensen’s inequality. The last equation (9) replaces the limit with an indicator function. A sufﬁcient condition for (8) is to maximize the lower-bound of the posterior distribution in (9) by ensuring the indicator function in (9) to hold. We can replac e (8) with the following update Qk,i+1←argmin Qk,i+1ET∼˜p(T|Qk,i,τ)/bracketleftbig Qk,i+1(s,a)−/parenleftbig r+γ·Es′′∼T(·|s,a)max a′Qk,i+1(s′′,a′)/parenrightbig/bracketrightbig2. (10) 4However, (10) is not tractable because the expectation in (1 0) is taken with respect to the posterior ˜p(T|Qk,i,τ)of the transition T. To overcome this challenge, we approximate the posterior u pdate by reusing the one-sample next state s′fromτsuch that Qk,i+1←argmin Qk,i+1/bracketleftbig Qk,i+1(s,a)−/parenleftbig r+γ·max a′Qk,i+1(s′,a′)/parenrightbig/bracketrightbig2. (11) Instead of updating the posterior after each transition, we use an experience replay buffer Bto store observed transitions and sample a minibatch Bminiof transitions (s,a,r,s′)for each update. In this case, the batched update of each Qk,itoQk,i+1becomes a standard Bellman update Qk,i+1←argmin Qk,i+1E(s,a,r,s′)∈Bmini/bracketleftbig Qk,i+1(s,a)−/parenleftbig r+γ·max a′Qk,i+1(s′,a′)/parenrightbig/bracketrightbig2. (12) For stability, Algorithm 1 also uses a target network for eac hQkas in Double DQN in the batched update. We point out that the action choice of Algorithm 1 is e xploitation only. In the next section, we propose two exploration strategies. Algorithm 1 Ensemble V oting 1:Input :K∈N+copies of independently initialized Q∗-functions{Qk}K k=1. 2:LetBbe a replay buffer storing transitions for training 3:foreach episode dodo 4: Obtain initial state from environment s0 5: forstept= 1,... until end of episode do 6: Pick an action according to at= MajorityVote({argmaxaQk(st,a)}K k=1) 7: Executeat. Receive state st+1and reward rtfrom the environment 8: Add(st,at,rt,st+1)to replay buffer B 9: At learning interval, sample random minibatch and update {Qk} 10: end for 11:end for 4 UCB Exploration Strategy Using Q-Ensembles In this section, we propose optimism-based exploration by a dapting the UCB algorithms (Auer et al. , Audibert et al. ) from the bandit setting. The UCB alg orithms maintain an upper-conﬁdence bound for each arm, such that the expected reward from pullin g each arm is smaller than this bound with high probability. At every time step, the agent optimis tically chooses the arm with the highest UCB. Auer et al.  constructed the UCB based on empirical re ward and the number of times each arm is chosen. Audibert et al.  incorporated the empirica l variance of each arm’s reward into the UCB, such that at time step t, an armAtis pulled according to At= argmax i/braceleftBig ˆri,t+c1·/radicalBigg ˆVi,tlog(t) ni,t+c2·log(t) ni,t/bracerightBig whereˆri,tandˆVi,tare the empirical reward and variance of arm iat timet,ni,tis the number of times arm ihas been pulled up to time t, andc1,c2are positive constants. We extend the intuition of UCB algorithms to the RL setting. U sing the outputs of the {Qk}functions, we construct a UCB by adding the empirical standard de viation˜σ(st,a)of{Qk(st,a)}K k=1to the empirical mean ˜µ(st,a)of{Qk(st,a)}K k=1. The agent chooses the action that maximizes this UCB at∈argmax a/braceleftbig ˜µ(st,a)+λ·˜σ(st,a)/bracerightbig , (13) whereλ∈R+is a hyperparameter. We present Algorithm 2, which incorporates the UCB explorat ion. The hyperparemeter λcontrols the degrees of exploration. In Section 5, we compare the perf ormance of our algorithms on Atari games using a consistent set of parameters. 5Algorithm 2 UCB Exploration with Q-Ensembles 1:Input: Value function networks QwithKoutputs{Qk}K k=1. Hyperparameter λ. 2:LetBbe a replay buffer storing experience for training. 3:foreach episode do 4: Obtain initial state from environment s0 5: forstept= 1,... until end of episode do 6: Pick an action according to at∈argmaxa/braceleftbig ˜µ(st,a)+λ·˜σ(st,a)/bracerightbig 7: Receive state st+1and reward rtfrom environment, having taken action at 8: Add(st,at,rt,st+1)to replay buffer B 9: At learning interval, sample random minibatch and update {Qk}according to (12) 10: end for 11:end for 5 Experiment In this section, we conduct experiments to answer the follow ing questions: 1. does Ensemble V oting, Algorithm 1, improve upon existing algorithms including Double DQN and bootstrapped DQN? 2. is the proposed UCB exploration strategy of Algorithm 2 ef fective in improving learning compared to Algorithm 1? 3. how does UCB exploration compare with prior exploration m ethods such as the countbased exploration method of Bellemare et al. ? We evaluate the algorithms on each Atari game of the Arcade Le arning Environment (Bellemare et al. ). We use the multi-head neural net architecture of Osband et al. . We ﬁx the common hyperparameters of all algorithms based on a well-tuned double DQN implementation, which uses the Adam optimizer (Kingma and Ba ), different lear ning rate and exploration schedules compared to Mnih et al. . Appendix A tabulates the hyperp arameters. The number of {Qk} functions is K= 10 . Experiments are conducted on the OpenAI Gym platform (Broc kman et al. ) and trained with 40million frames and 2trials on each game. We take the following directions to evaluate the performanc e of our algorithms: 1. we compare Algorithm 1 against Double DQN and bootstrappe d DQN, 2. we isolate the impact of UCB exploration by comparing Algo rithm 2 with λ= 0.1, denoted asucb exploration , against Algorithm 1. 3. we compare Algorithm 1 and Algorithm 2 with the count-base d exploration method of Bellemare et al. . 4. we aggregate the comparison according to different categ ories of games, to understand when our methods are suprior. Figure 1 compares the normalized learning curves of all algo rithms across Atari games. Overall, Ensemble V oting, Algorithm 1, outperforms both Double DQN a nd bootstrapped DQN. With exploration,ucb exploration improves further by outperforming Ensemble V oting. In Appendix B, we tabulate detailed results that compare our algorithms, Ensemble V oting and ucb exploration , against prior methods. In Table 2, we tabulate the maximal m ean reward in 100consecutive episodes for Ensemble V oting, ucb exploration , bootstrapped DQN and Double DQN. Without exploration, Ensemble V oting already achi eves higher maximal mean reward than both Double DQN and bootstrapped DQN in a majority of Ata ri games. ucb exploration achieves the highest maximal mean reward among these four al gorithms in 30 games out of the total 49 games evaluated. Figure 2 displays the learning curves of these ﬁve algorithms on a set of six Atari games. Ensemble V oting outperforms Double DQN and bootstrapped DQN. ucb exploration outperforms Ensemble V oting. In Table 3, we compare our proposed methods with the count-ba sed exploration method A3C+ of Bellemare et al.  based on their published results of A3C+ trained with 200 million frames. We 60.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Frames 1e70.00.10.20.30.40.50.60.70.8Average Normalized Learning Curve bootstrapped dqn ucb exploration ensemble voting double dqn Figure 1: Comparison of algorithms in normalized learning c urve. The normalized learning curve is calculated as follows: ﬁrst, we normalize learning curves f or all algorithms in the same game to the interval[0,1]; next, average the normalized learning curve from all games for each algorithm. point out that even though our methods were trained with only 40 million frames, much less than A3C+’s 200 million frames, UCB exploration achieves the hig hest average reward in 28 games, Ensemble V oting in 10 games, and A3C+ in 10 games. Our approac h outperforms A3C+. Finally to understand why and when the proposed methods are s uperior, we aggregate the comparison results according to four categories: Human Optima l, Score Explicit, Dense Reward, and Sparse Reward. These categories follow the taxonomy in Tabl e 1 of Ostrovski et al. . Out of all games evaluated, 23 games are Human Optimal, 8 are Score Expl icit, 8 are Dense Reward, and 5 are Sparse Reward. The comparison results are tabulated in Tabl e 4, where we see ucb exploration achieves top performance in more games than Ensemble V oting , Double DQN, and Bootstrapped DQN in the categories of Human Optimal, Score Explicit, and D ense Reward. In Sparse Reward, bothucb exploration and Ensemble V oting achieve best performance in 2 games out o f total of 5. Thus, we conclude that ucb exploration improves prior methods consistently across different game categories within the Arcade Learning Environment. 6 Conclusion We proposed a Q-ensemble approach to deep Q-learning, a computationally practical algorithm inspired by Bayesian reinforcement learning that outperform s Double DQN and bootstrapped DQN, as evaluated on Atari. The key ingredient is the UCB explorat ion strategy, inspired by bandit algorithms. Our experiments show that the exploration strategy achieves improved learning performance on the majority of Atari games. 