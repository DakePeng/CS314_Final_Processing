DeepType: Multilingual Entity Linking by Neural Type System Evolution
Jonathan Raiman
OpenAI
San Francisco, California
raiman@openai.comOlivier Raiman
Agilience
Paris, France
or@agilience.com
Abstract
The wealth of structured (e.g. Wikidata) and unstructured
data about the world available today presents an incredible
opportunity for tomorrow‚Äôs ArtiÔ¨Åcial Intelligence. So far, in-
tegration of these two different modalities is a difÔ¨Åcult pro-
cess, involving many decisions concerning how best to repre-
sent the information so that it will be captured or useful, and
hand-labeling large amounts of data. DeepType overcomes
this challenge by explicitly integrating symbolic information
into the reasoning process of a neural network with a type
system. First we construct a type system, and second, we use
it to constrain the outputs of a neural network to respect the
symbolic structure. We achieve this by reformulating the de-
sign problem into a mixed integer problem: create a type sys-
tem and subsequently train a neural network with it. In this
reformulation discrete variables select which parent-child re-
lations from an ontology are types within the type system,
while continuous variables control a classiÔ¨Åer Ô¨Åt to the type
system. The original problem cannot be solved exactly, so we
propose a 2-step algorithm: 1) heuristic search or stochastic
optimization over discrete variables that deÔ¨Åne a type system
informed by an Oracle and a Learnability heuristic, 2) gradi-
ent descent to Ô¨Åt classiÔ¨Åer parameters. We apply DeepType to
the problem of Entity Linking on three standard datasets (i.e.
WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and Ô¨Ånd
that it outperforms all existing solutions by a wide margin,
including approaches that rely on a human-designed type sys-
tem or recent deep learning-based entity embeddings, while
explicitly using symbolic information lets it integrate new en-
tities without retraining.
1 Introduction
Online encyclopedias, knowledge bases, ontologies (e.g.
Wikipedia, Wikidata, Wordnet), alongside image and video
datasets with their associated label and category hierarchies
(e.g. Imagenet (Deng et al. 2009), Youtube-8M (Abu-El-
Haija et al. 2016), Kinetics (Kay et al. 2017)) offer an un-
precedented opportunity for incorporating symbolic repre-
sentations within distributed and neural representations in
ArtiÔ¨Åcial Intelligence systems. Several approaches exist for
integrating rich symbolic structures within the behavior of
neural networks: a label hierarchy aware loss function that
relies on the ultrametric tree distance between labels (e.g.
it is worse to confuse sheepdogs and skyscrapers than it is
to confuse sheepdogs and poodles) (Wu, Tygert, and LeCun2017), a loss function that trades off speciÔ¨Åcity for accu-
racy by incorporating hypo/hypernymy relations (Deng et
al. 2012), using NER types to constrain the behavior of an
Entity Linking system (Ling, Singh, and Weld 2015), or
more recently integrating explicit type constraints within a
decoder‚Äôs grammar for neural semantic parsing (Krishna-
murthy, Dasigi, and Gardner 2017). However, current ap-
proaches face several difÔ¨Åculties:
Selection of the right symbolic information based on the
utility or information gain for a target task.
Design of the representation for symbolic information (hi-
erarchy, grammar, constraints).
Hand-labelling large amounts of data.
DeepType overcomes these difÔ¨Åculties by explicitly in-
tegrating symbolic information into the reasoning process
of a neural network with a type system that is automati-
cally designed without human effort for a target task. We
achieve this by reformulating the design problem into a
mixed integer problem: create a type system by selecting
roots and edges from an ontology that serve as types in a type
system, and subsequently train a neural network with it. The
original problem cannot be solved exactly, so we propose a
2-step algorithm:
1. heuristic search or stochastic optimization over the dis-
crete variable assignments controlling type system design,
using an Oracle and a Learnability heuristic to ensure that
design decisions will be easy to learn by a neural network,
and will provide improvements on the target task,
2. gradient descent to Ô¨Åt classiÔ¨Åer parameters to predict the
behavior of the type system.
In order to validate the beneÔ¨Åts of our approach, we fo-
cus on applying DeepType to Entity Linking (EL), the task
of resolving ambiguous mentions of entities to their referent
entities in a knowledge base (KB) (e.g. Wikipedia). Specif-
ically we compare our results to state of the art systems on
three standard datasets (WikiDisamb30, CoNLL (YAGO),
TAC KBP 2010). We verify whether our approach can work
in multiple languages, and whether optimization of the type
system for a particular language generalizes to other lan-
guages1by training our full system in a monolingual (En-
glish) and bilingual setup (English and French), and also
1e.g. Do we overÔ¨Åt to a particular set of symbolic structures use-arXiv:1802.01021v1  [cs.CL]  3 Feb 2018evaluate our Oracle (performance upper bound) on German
and Spanish test datasets. We compare stochastic optimiza-
tion and heuristic search to solve our mixed integer problem
by comparing the Ô¨Ånal performance of systems whose type
systems came from different search methodologies. We also
investigate whether symbolic information is captured by us-
ing DeepType as pretraining for Named Entity Recognition
(NER) on two standard datasets (i.e. CoNLL 2003 (Sang and
Meulder 2003), OntoNotes 5.0 (CoNLL 2012) (Pradhan et
al. 2012)).
Our key contributions in this work are as follows:
A system for integrating symbolic knowledge into the rea-
soning process of a neural network through a type system,
to constrain the behavior to respect the desired symbolic
structure, and automatically design the type system with-
out human effort.
An approach to EL that uses type constraints, reduces
disambiguation resolution complexity from O(N2)to
O(N), incorporates new entities into the system with-
out retraining, and outperforms all existing solutions by
a wide margin.
We release code for designing, evolving, and training neural
type systems2. Moreover, we observe that disambiguation
accuracy reaches 99.0% on CoNLL (YAGO) and 98.6% on
TAC KBP 2010 when entity types are predicted by an Or-
acle, suggesting that EL would be almost solved if we can
improve type prediction accuracy.
The rest of this paper is structured as follows. In Sec-
tion 2 we introduce EL and EL with Types, in Section 3
we describe DeepType for EL, In Section 4 we provide ex-
perimental results for DeepType applied to EL and evidence
of cross-lingual and cross-domain transfer of the represen-
tation learned by a DeepType system. In Section 5 we relate
our work to existing approaches. Conclusions and directions
for future work are given in Section 6.
2 Task
Before we deÔ¨Åne how DeepType can be used to constrain
the outputs of a neural network using a type system, we will
Ô¨Årst deÔ¨Åne the goal task of Entity Linking.
Entity Linking The goal is to recover the ground truth en-
tities in a KB referred to in a document by locating mentions
(text spans), and for each mention properly disambiguating
the referent entity. Commonly, a lookup table that maps each
mention to a proposal set of entities for each mention m:
Em=fe1;:::;eng(e.g. ‚ÄúWashington‚Äù could mean Wash-
ington, D.C. orGeorge Washington ). Disambiguation is
Ô¨Ånding for each mention mthe a ground truth entity eGTin
Em. Typically, disambiguation operates according to two cri-
teria: in a large corpus, how often does a mention point to an
entity, LinkCount( m;e), and how often does entity e1co-
occur with entity e2, anO(N2)process, often named coher-
ful only in English, or can we discover a knowledge representation
that works across languages?
2http://github.com/openai/deeptypeence (Milne and Witten 2008; Ferragina and Scaiella 2010;
Yamada et al. 2016).
Entity Linking with Types
In this work we extend the EL task to associate with each
entity a series of types (e.g. Person ,Place , etc.) that if
known, would rule out invalid answers, and therefore ease
linking (e.g. the context now enables types to disambiguate
‚ÄúWashington‚Äù). Knowledge of the types Tassociated with a
mention can also help prune entities from the the proposal
set, to produce a constrained set: Em;T Em. In a proba-
bilistic setting it is also possible to rank an entity ein doc-
umentxaccording to its likelihood under the type system
prediction and under the entity model:
P(ejx)/Ptype(types(e)jx)Pentity (ejx;types(e)):(1)
In prior work, the 112 FIGER Types (Ling and Weld 2012)
were associated with entities to combine an NER tagger with
an EL system (Ling, Singh, and Weld 2015). In their work,
they found that regular NER types were unhelpful, while
Ô¨Åner grain FIGER types improved system performance.
3 DeepType for Entity Linking
DeepType is a strategy for integrating symbolic knowledge
into the reasoning process of a neural network through a type
system. When we apply this technique to EL, we constrain
the behavior of an entity prediction model to respect the
symbolic structure deÔ¨Åned by types. As an example, when
we attempt to disambiguate ‚ÄúJaguar‚Äù the beneÔ¨Åts of this ap-
proach are apparent: our decision can be based on whether
the predicted type is Animal or Road Vehicle as shown visu-
ally in Figure 1.
In this section, we will Ô¨Årst deÔ¨Åne key terminology, then
explain the model and its sub-components separately.
Terminology
Relation Given some knowledge graph or feature set, a re-
lation is a set of inheritance rules that deÔ¨Åne membership or
exclusion from a particular group. For instance the relation
instance of( city)selects all children of the root city
connected by instance of as members of the group, de-
picted by outlined boxes in Figure 2.
Type In this work a type is a label deÔ¨Åned by a relation
(e.g.IsHuman is the type applied to all children of Human
connected by instance of ).
Type Axis A set of mutually exclusive types (e.g.
IsHuman^IsPlant =fg).
Type System A grouping of type axes, A, along with a
type labelling function: ft1;:::;tkg= TypeLabeler( e;A).
For instance a type system with two axes fIsA,Topicg
assigns to George Washington :fPerson ,Politicsg,
and to Washington, D.C. :fPlace ,Geographyg).
Model
To construct an EL system that uses type constraints we re-
quire: a type system, the associated type classiÔ¨Åer, and aThe prey saw a jaguar cross in the jungle.ajaguarcrosstheJunglesawpreyWord embeddingFully-connected layerconcatdropoutFCIs Animal?/u1D70EStacked Bi-LSTMdropout‚Ä¶Word 1Word nFCFC‚Ä¶softmaxsoftmaxType 1Type k‚Ä¶
Type relation rootChild entity, membernon-member entityactive edge‚Ä¶
inactive edge
The man saw a Jaguar speed on the highway.
AnimalRoad vehicleRegionPhysical Object1.00.0Type Probability0.9995e-72e-700.5AnimalRoad vehicleRegionPhysical Object4e-34e-30.350.24AnimalRoad vehicleRegionPhysical Object0.250.561e-52e-3AnimalRoad vehicleRegionPhysical Object1e-43e-21e-30.80‚Äújungle‚Äù‚Äújaguar‚Äù‚ÄúJaguar‚Äù‚Äúhighway‚ÄùSentence
EntityjaguarJaguar junglejunglejaguarJaguarhighwayHighwayTypeAnimalRoad vehicleRegionMusicAnimalRoad vehiclePhysical ObjectFilmonly link Prob.0.290.600.350.170.290.600.850.04Prob. w/. types1.00.01.00.00.01.01.00.0instance ofcityParisFortalezaAlhambrainstance ofMona Lisalocated ininstance of
subclass ofhuman settlementneighborhoodkibbutzsubclass ofUpper East Sideinstance ofFigure 1: Example model output: ‚Äújaguar‚Äù refers to different entities depending on context. Predicting the type associated with
each word (e.g. animal, region, etc.) helps eliminate options that do not match, and recover the true entity. Bar charts give the
system‚Äôs belief over the type-axis ‚Äú IsA‚Äù, and the table shows how types affects entity probabilities given by Wikipedia links.
The prey saw a jaguar cross in the jungle.ajaguarcrosstheJunglesawpreyWord embeddingFully-connected layerconcatdropoutFCIs Animal?/u1D70EStacked Bi-LSTMdropout‚Ä¶Word 1Word nFCFC‚Ä¶softmaxsoftmaxType 1Type k‚Ä¶
Type relation rootChild entity, membernon-member entityactive edge‚Ä¶
inactive edge
The man saw a Jaguar speed on the highway.
AnimalRoad vehicleRegionPhysical Object1.00.0Type Probability0.9995e-72e-700.5AnimalRoad vehicleRegionPhysical Object4e-34e-30.350.24AnimalRoad vehicleRegionPhysical Object0.250.561e-52e-3AnimalRoad vehicleRegionPhysical Object1e-43e-21e-30.80‚Äújungle‚Äù‚Äújaguar‚Äù‚ÄúJaguar‚Äù‚Äúhighway‚ÄùSentence
Entityjaguar(feline)JaguarMotorsjungle(forest)jungle(music)jaguar(feline)JaguarMotorshighway(road)Highway(Ô¨Ålm)only link Prob.0.290.600.350.170.290.600.850.04Prob. w/. types1.00.01.00.00.01.01.00.0instance ofcityParisFortalezaAlhambrainstance ofMona Lisalocated ininstance of
subclass ofhuman settlementneighborhoodkibbutzsubclass ofUpper East Sideinstance of
Figure 2: DeÔ¨Åning group membership with a knowledge
graph relation: children of root (city) via edge (instance of).
model for predicting and ranking entities given a mention.
Instead of assuming we receive a type system, classiÔ¨Åer, en-
tity prediction model, we will instead create the type sys-
tem and its classiÔ¨Åer starting from a given entity predic-
tion model and ontology with text snippets containing entity
mentions (e.g. Wikidata and Wikipedia). For simplicity we
useLinkCount( e;m)as our entity prediction model.
We restrict the types in our type systems to use a set of
parent-child relations over the ontology in Wikipedia and
Wikidata, where each type axis has a root node rand an
edge typeg, that sets membership or exclusion from the axis
(e.g.r=human;e=instance of , splits entities into:
human vs. non-human3).
We then reformulate the problem into a mixed inte-
ger problem, where discrete variables control which roots
r1;:::;rkand edge types g1;:::;gkamong all rootsRand
edge typesGwill deÔ¨Åne type axes, while the continuous
variablesparametrize a classiÔ¨Åer Ô¨Åt to the type system.
Our goal in type system design is to select parent-child re-
lations that a classiÔ¨Åer easily predicts, and where the types
improve disambiguation accuracy.
Objective
To formally deÔ¨Åne our mixed integer problem, let us
Ô¨Årst denoteAas the assignment for the discrete vari-
3Type ‚Äú instance of :human ‚Äù mimics the NER PER label.ables that deÔ¨Åne our type system (i.e. boolean variables
deÔ¨Åning if a parent-child relation gets included in our
type system), as the parameters for our entity predic-
tion model and type classiÔ¨Åer, and Smodel (A;)as the
disambiguation accuracy given a test corpus containing
mentionsM=
(m0;eGT
0;Em0);:::; (mn;eGT
n;Emn)	
.
We now assume our model produces some score for
each proposed entity egiven a mention min a doc-
umentD, deÔ¨Åned EntityScore( e;m;D;A;). The pre-
dicted entity for a given mention is thus: e=
argmaxe2EmEntityScore( e;m;D;A;). Ife=eGT, the
mention is disambiguated. Our problem is thus deÔ¨Åned as:
max
Amax
Smodel (A;) =P
(m;eGT;Em)2M1eGT(e)
jMj:(2)
This original formulation cannot be solved exactly4. To
make this problem tractable we propose a 2-step algorithm:
1.Discrete Optimization of Type System : Heuristic search
or stochastic optimization over the discrete variables of
the type system,A, informed by a Learnability heuristic
and an Oracle.
2.Type classiÔ¨Åer : Gradient descent over continuous vari-
ablesto Ô¨Åt type classiÔ¨Åer and entity prediction model.
We will now explain in more detail discrete optimization
of a type system, our heuristics (Oracle and Learnability
heuristic), the type classiÔ¨Åer, and inference in this model.
Discrete Optimization of a Type System
The original objective Smodel (A;)cannot be solved ex-
actly, thus we rely on heuristic search or stochastic opti-
mization to Ô¨Ånd suitable assignments for A. To avoid train-
ing an entire type classiÔ¨Åer and entity prediction model for
each evaluation of the objective function, we instead use a
4There are22:4107choices if each Wikipedia article can be
a type within our type system.proxy objective Jfor the discrete optimization5. To ensure
that maximizing J(A)also maximizes Smodel (A;), we in-
troduce a Learnability heuristic and an Oracle that quantify
the disambiguation power of a proposed type system, an es-
timate of how learnable the type axes in the selected solution
will be. We measure an upper bound for the disambiguation
power by measuring disambiguation accuracy Soracle for a
type classiÔ¨Åer Oracle over a test corpus.
To ensure that the additional disambiguation power of a
solutionAtranslates in practice we weigh by an estimate of
solution‚Äôs learnability Learnability(A)improvements be-
tweenSoracle and the accuracy of a system that predicts only
according to the entity prediction model6,Sgreedy .
Selecting a large number of type axes will provide strong
disambiguation power, but may lead to degenerate solu-
tions that are harder to train, slow down inference, and lack
higher-level concepts that provide similar accuracy with less
axes. We prevent this by adding a per type axis penalty of .
Combining these three terms gives us the equation for J:
J(A) =(Soracle Sgreedy )Learnability(A) +
Sgreedy jAj:(3)
Oracle Our Oracle is a methodology for abstracting away
machine learning performance from the underlying repre-
sentational power of a type system A. It operates on a test
corpus with a set of mentions, entities, and proposal sets:
mi;eGT
i;Emi. The Oracle prunes each proposal set to only
contain entities whose types match those of eGT
i, yielding
Em;oracle . Types fully disambiguate when jEm;oraclej= 1,
otherwise we use the entity prediction model to select the
right entity in the remainder set Emi;oracle :
Oracle(m) = argmax
e2Em;oraclePentity (ejm;types(x)): (4)
IfOracle(m) =eGT, the mention is disambiguated.
Oracle accuracy is denoted Soracle given a type sys-
tem over a test corpus containing mentions M = 
(m0;eGT
0;Em0);:::; (mn;eGT
n;Emn)	
:
Soracle =P
(m;eGT;Em)2M1eGT(Oracle(m))
jMj:(5)
Learnability To ensure that disambiguation gains ob-
tained during the discrete optimization are available when
we train our type classiÔ¨Åer, we want to ensure that the types
selected are easy to predict. The Learnability heuristic em-
pirically measures the average performance of classiÔ¨Åers at
predicting the presence of a type within some Learnability-
speciÔ¨Åc training set.
To efÔ¨Åciently estimate Learnability for a full type sys-
tem we make an independence assumption and model it as
the mean of the Learnability for each individual axis, ig-
noring positive or negative transfer effects between differ-
ent type axes. This assumption lets us parallelize training of
5Training of the type classiÔ¨Åer takes 3 days on a Titan X Pas-
cal, while our Oracle can run over the test set in 100ms.
6For an entity prediction model based only on link counts, this
means always picking the most linked entity.
The prey saw a jaguar cross in the jungle.ajaguarcrosstheJunglesawpreyWord embeddingFully-connected layerconcatdropoutFCIs Animal?/u1D70EStacked Bi-LSTMdropout‚Ä¶Word 1Word nFCFC‚Ä¶softmaxsoftmaxType Axis 1Type Axis k‚Ä¶
Type relation rootChild entity, membernon-member entityactive edge‚Ä¶
inactive edge
The man saw a Jaguar speed on the highway.
AnimalRoad vehicleRegionPhysical Object1.00.0Type Probability0.9995e-72e-700.5AnimalRoad vehicleRegionPhysical Object4e-34e-30.350.24AnimalRoad vehicleRegionPhysical Object0.250.561e-52e-3AnimalRoad vehicleRegionPhysical Object1e-43e-21e-30.80‚Äújungle‚Äù‚Äújaguar‚Äù‚ÄúJaguar‚Äù‚Äúhighway‚ÄùSentence
EntityjaguarJaguar junglejunglejaguarJaguarhighwayHighwayTypeAnimalRoad vehicleRegionMusicAnimalRoad vehiclePhysical ObjectFilmonly link Prob.0.290.600.350.170.290.600.850.04Prob. w/. types1.00.01.00.00.01.01.00.0instance ofcityParisFortalezaAlhambrainstance ofMona Lisalocated ininstance of
subclass ofhuman settlementneighborhoodkibbutzsubclass ofUpper East Sideinstance of
Knowledge Base / Relations(e.g. Wikidata)greedybeam	searchCross-Entropy	MethodGenetic	AlgorithmhumanType Axis 1Type Axis k‚Ä¶Type Axis 1Type Axis k‚Ä¶Corpus with labels/entities‚Ä®(e.g. Wikipedia)Labeled DataClassiÔ¨Åer(a)
The prey saw a jaguar cross in the jungle.ajaguarcrosstheJunglesawpreyWord embeddingFully-connected layerconcatdropoutFCIs Animal?/u1D70EStacked Bi-LSTMdropout‚Ä¶Word 1Word nFCFC‚Ä¶softmaxsoftmaxType Axis 1Type Axis k‚Ä¶
Type relation rootChild entity, membernon-member entityactive edge‚Ä¶
inactive edge
The man saw a Jaguar speed on the highway.
AnimalRoad vehicleRegionPhysical Object1.00.0Type Probability0.9995e-72e-700.5AnimalRoad vehicleRegionPhysical Object4e-34e-30.350.24AnimalRoad vehicleRegionPhysical Object0.250.561e-52e-3AnimalRoad vehicleRegionPhysical Object1e-43e-21e-30.80‚Äújungle‚Äù‚Äújaguar‚Äù‚ÄúJaguar‚Äù‚Äúhighway‚ÄùSentence
EntityjaguarJaguar junglejunglejaguarJaguarhighwayHighwayTypeAnimalRoad vehicleRegionMusicAnimalRoad vehiclePhysical ObjectFilmonly link Prob.0.290.600.350.170.290.600.850.04Prob. w/. types1.00.01.00.00.01.01.00.0instance ofcityParisFortalezaAlhambrainstance ofMona Lisalocated ininstance of
subclass ofhuman settlementneighborhoodkibbutzsubclass ofUpper East Sideinstance of(b)
Figure 3: Text window classiÔ¨Åer in (a) serves as type Learn-
ability estimator, while the network in (b) takes longer to
train, but discovers long-term dependencies to predict types
and jointly produces a distribution for multiple type axes.
simpler classiÔ¨Åers for each type axis. We measure the area
under its receiver operating characteristics curve (AUC) for
each classiÔ¨Åer and compute the type system‚Äôs learnability:
Learnability(A) =P
t2AAUC(t)
jAj. We use a text window
classiÔ¨Åer trained over windows of 10 words before and after
a mention. Words are represented with randomly initialized
word embeddings; the classiÔ¨Åer is illustrated in Figure 3a.
AUC is averaged over 4 training runs for each type axis.
Type ClassiÔ¨Åer
After the discrete optimization has completed we now have
a type systemA. We can now use this type system to label
data in multiple languages from text snippets associated with
the ontology7, and supervize a Type classiÔ¨Åer.
The goal for this classiÔ¨Åer is to discover long-term de-
pendencies in the input data that let it reliably predict types
across many contexts and languages. For this reason we se-
lect a bidirectional-LSTM (Lample et al. 2016) with word,
preÔ¨Åx, and sufÔ¨Åx embeddings as done in (Andor et al. 2016).
Our network is shown pictorially in Figure 3b. Our clas-
siÔ¨Åer is trained to minimize the negative log likelihood of
the per-token types for each type axis in the document D
withLtokens: Pk
i=1logPi(ti;1;:::;ti;LjD). When us-
ing Wikipedia as our source of text snippets our label super-
vision is partial8, so we make a conditional independence
assumption about our predictions and use Softmax as our
output activation: Pk
i=1PL
j=1logPi(ti;jjwj;D).
Inference
At inference-time we incorporate classiÔ¨Åer belief into our
decision process by Ô¨Årst running it over the full context
and obtaining a belief over each type axis for each in-
put wordw0;:::;wL. For each mention mcovering words
wx;:::;wy, we obtain the type conditional probability for
all type axes i:fPi(jwx;D);:::;Pi(jwy;D)g. In multi-
word mentions we must combine beliefs over multiple to-
kensx:::y : the product of the beliefs over the mention‚Äôs
tokens is correct but numerically unstable and slightly less
7Wikidata‚Äôs ontology has cross-links with Wikipedia, IMDB,
Discogs, MusicBrainz, and other encyclopaedias with snippets.
8We obtain type labels only on the intra-wiki link anchor text.performant than max-over-time9, which we denote for the
i-th type axis: Pi;(jm;D ).
The scorese;m;D;A;= EntityScore( e;m;D;A;)of
an entityegiven these conditional probability distributions
P1;(jm;D );:::;Pk;(jm;D ), and the entities‚Äô types in
each axist1;:::;tkcan then be combined to rank entities
according to how predicted they were by both the entity pre-
diction model and the type system. The chosen entity e
for a mention mis chosen by taking the option that maxi-
mizes the score among the Empossible entities; the equa-
tion for scoring and eis given below, with PLink(ejm) =
LinkCount( m;e)P
j2EmLinkCount( m;j),ia per type axis smoothing param-
eter,is a smoothing parameter over all types:
se;m;D;A;=PLink(ejm)
1 +
(kY
i=1(1 i+iPi;(tijm;D )))
:(6)
4 Results
Type System Discovery
In the following experiments we evaluate the behavior
of different search methodologies for type system discov-
ery: which method best scales to large numbers of types,
achieves high accuracy on the target EL task, and whether
the choice of search impacts learnability by a classiÔ¨Åer or
generalisability to held-out EL datasets.
For the following experiments we optimize DeepType‚Äôs
type system over a held-out set of 1000 randomly sam-
pled articles taken from the Feb. 2017 English Wikipedia
dump, with the Learnability heuristic text window classiÔ¨Åers
trained only on those articles. The type classiÔ¨Åer is trained
jointly on English and French articles, totalling 800 million
tokens for training, 1 million tokens for validation, sampled
equally from either language.
We restrict rootsRand edgesGto the most common
1:5105entities that are entity parents through wikipedia
category orinstance of edges, and eliminate type
axes where Learnability()is 0, leaving 53,626 type axes.
Human Type System Baseline To isolate discrete opti-
mization from system performance and gain perspective on
the difÔ¨Åculty and nature of the type system design we in-
corporate a human-designed type system. Human design-
ers have access to the full set of entities and relations
in Wikipedia and Wikidata, and compose different inheri-
tance rules through Boolean algebra to obtain higher level
concepts (e.g. woman =IsHuman^IsFemale , or
animal =IsTaxon^:fIsHuman_IsPlantg10). The
Ô¨Ånal human system uses 5 type axes11, and 1218 inheritance
rules.
9The choice of max-over-time is empirically motivated: we
compared product mean, min, max, and found that max was com-
parable to mean, and slightly better than the alternatives.
10Taxon is the general parent of living items in Wikidata.
11IsA,Topic ,Location ,Continent , andTime .Search methodologies
Beam Search and Greedy selection We iteratively con-
struct a type system by choosing among all remaining type
axes and evaluating whether the inclusion of a new type axis
improves our objective: J(A[ftjg)> J(A). We use a
beam size of band stop the search when all solutions stop
growing.
Cross-Entropy Method (CEM) (Rubinstein 1999) is a
stochastic optimization procedure applicable to the selection
of types. We begin with a probability vector ~P0set topstart,
and at each iteration we sample MCEM vectors~ sfrom the
Bernoulli distribution given by ~Pi, and measure each sam-
ple‚Äôs Ô¨Åtness with Eq. 3. The NCEM highest Ô¨Åtness elements
are our winning population Stat iterationt. Our probabili-
ties are Ô¨Åt toStgivingPt+1=P
~ s2St~ s
NCEM. The optimization is
complete when the probability vector is binary.
Genetic Algorithm The best subset of type axes can
be found by representing type axes as genes carried by
Npopulation individuals in a population undergoing muta-
tions and crossovers (Harvey 2009) over Ggenerations. We
select individuals using Eq. 3 as our Ô¨Åtness function.
Search Methodology Performance Impact To validate
thatcontrols type system size, and Ô¨Ånd the best trade-
off between size and accuracy, we experiment with a range
of values and Ô¨Ånd that accuracy grows more slowly below
0.00007, while system size still increases.
From this point on we keep = 0:00007 , and we com-
pare the number of iterations needed by different search
methods to converge, against two baselines: the empty set
and the mean performance of 100 randomly sampled sets of
128 types (Table 1a). We observe that the performance of
stochastic optimizers GA and CEM is similar to heuristic
search, but requires orders of magnitude less function eval-
uations.
Next, we compare the behavior of the different search
methods to a human designed system and state of the art ap-
proaches on three standard datasets (i.e. W IKI-DISAMB 30
(WKD30) (Ferragina and Scaiella 2010)12, CoNLL(YAGO)
(Hoffart et al. 2011), and TAC KBP 2010 (Ji et al. 2010)),
along with test sets built by randomly sampling 1000 ar-
ticles from Wikipedia‚Äôs February 2017 dump in English,
French, German, and Spanish which were excluded from
training the classiÔ¨Åers. Table 1c has Oracle performance for
the different search methods on the test sets, where we re-
port disambiguation accuracy per annotation. A LinkCount
baseline is included that selects the mention‚Äôs most fre-
quently linked entity13. All search techniques‚Äô Oracle ac-
12We apply the preprocessing and link pruning as (Ferragina and
Scaiella 2010) to ensure the comparison is fair.
13Note that LinkCount accuracy is stronger than the one found
in (Ferragina and Scaiella 2010) or (Milne and Witten 2008) be-
cause newer Wikipedia dumps improve link coverage and reduce
link distribution noisiness.Table 1: Method comparisons. Highest value in bold , excluding oracles.
(a) Type system discovery method comparison
Approach Evals Accuracy Items
BeamSearch 5:1210797.84 130
Greedy 6:4010697.83 130
GA 116;000 96.959 128
CEM 43;000 96.26 89
Random N/A 92:90:28 128
No types 0 92:10 0(b) NER F1 score comparison for DeepType pretraining vs. baselines.
ModelCoNLL 2003 OntoNotes
Dev Test Dev Test
Bi-LSTM-76.29 -77.77(Chiu and Nichols 2015)
Bi-LSTM-CNN + emb + lex94.31 91.62 84.57 86.28(Chiu and Nichols 2015)
Bi-LSTM (Ours) 89.49 83.40 82.75 81.03
Bi-LSTM-CNN (Ours) 90.54 84.74 83.17 82.35
Bi-LSTM-CNN (Ours) + types 93.54 88.67 85.11 83.12
(c) Entity Linking model Comparison. SigniÔ¨Åcant improvements over prior work denoted byforp< 0:05, andforp< 0:01.
Model enwiki frwiki dewiki eswiki WKD30 CoNLL TAC 2010
M&W(Milne and Witten 2008) 84.6 - -
TagMe (Ferragina and Scaiella 2010) 83.224 80.711 90.9 - -
(Globerson et al. 2016) - 91.7 87.2
(Yamada et al. 2016) - 91.5 85.2
NTEE (Yamada et al. 2017) - - 87.7
LinkCount only 89.06492.013 92.01389.980 82.710 68.614 81.485Oursmanual 94.33192.967 91.88893.10890.743
manual (oracle) 97.734 98.026 98.632 98.178 95.872 98.217 98.601
greedy 93.72592.984 92.37594.15190.850
greedy (oracle) 98.002 97.222 97.915 98.246 97.293 98.982 98.278
CEM 93.70792.415 92.24793.96290.302
CEM (oracle) 97.500 96.648 97.480 97.599 96.481 99.005 96.767
GA 93.68492.027 92.06294.87990.312
GA (oracle) 97.297 96.783 97.408 97.609 96.268 98.461 96.663
GA (English only) 93.02991.74393.701-
curacy signiÔ¨Åcantly improve over LinkCount , and achieve
near perfect accuracy on all datasets (97-99%); furthermore
we notice that performance between the held-out Wikipedia
sets and standard datasets sets is similar, supporting the
claim that the discovered type systems generalize well. We
note that machine discovered type systems outperform hu-
man designed systems: CEM beats the human type sys-
tem on English Wikipedia, and all search method‚Äôs type
systems outperform human systems on W IKI-DISAMB 30,
CoNLL(YAGO), and TAC KBP 2010.
Search Methodology Learnability Impact To under-
stand whether the type systems produced by different search
methods can be trained similarly well we compare the type
system built by GA, CEM, greedy, and the one constructed
manually. EL Disambiguation accuracy is shown in Table
1c, where we compare with recent deep-learning based ap-
proaches (Globerson et al. 2016), or recent work by Ya-
mada et al. for embedding word and entities (Yamada et
al. 2016), or documents and entities (Yamada et al. 2017),
along with count and coherence based techniques Tagme
(Ferragina and Scaiella 2010) and Milne & Witten (Milne
and Witten 2008). To obtain Tagme‚Äôs Feb. 2017 Wikipediaaccuracy we query the public web API14available in Ger-
man and English, while other methods can be compared on
CoNLL(YAGO) and TAC KBP 2010. Models trained on a
human type system outperform all previous approaches to
entity linking, while type systems discovered by machines
lead to even higher performance on all datasets except En-
glish Wikipedia.
Cross-Lingual Transfer
Type systems are deÔ¨Åned over Wikidata/Wikipedia, a multi-
lingual knowledge base/encyclopaedia, thus type axes are
language independent and can produce cross-lingual super-
vision. To verify whether this cross-lingual ability is use-
ful we train a type system on an English dataset and ver-
ify whether it can successfully supervize French data. We
also measure using the Oracle (performance upper bound)
whether the type system is useful in Spanish or German. Or-
acle performance across multiple languages does not appear
to degrade when transferring to other languages (Table 1c).
We also notice that training in French with an English type
system still yields improvements over LinkCount for CEM,
greedy, and human systems.
14https://tagme.d4science.org/tagme/Because multi-lingual training might oversubscribe the
model, we veriÔ¨Åed if monolingual would outperform bilin-
gual training: we compare GA in English + French with only
English (last row of Table 1c). Bilingual training does not
appear to hurt, and might in fact be helpful.
We follow-up by inspecting whether the bilingual word
vector space led to shared representations: common nouns
have their English-French translation close-by, while proper
nouns do not (French and US politicians cluster separately).
Named Entity Recognition Transfer
The goal of our NER experiment is to verify whether Deep-
Type produces a type sensitive language representation use-
ful for transfer to other downstream tasks. To measure this
we pre-train a type classiÔ¨Åer with a character-CNN and word
embeddings as inputs, following (Kim et al. 2015), and re-
place the output layer with a linear-chain CRF (Lample et al.
2016) to Ô¨Åne-tune to NER data. Our model‚Äôs F1 scores when
transferring to the CoNLL 2003 NER task and OntoNotes
5.0 (CoNLL 2012) split are given in Table 1b. We com-
pare with two baselines that share the architecture but are
not pre-trained, along with the current state of the art (Chiu
and Nichols 2015).
We see positive transfer on Ontonotes and CoNLL: our
baseline Bi-LSTM strongly outperforms (Chiu and Nichols
2015)‚Äôs baseline, while pre-training gives an additional 3-4
F1 points, with our best model outperforming the state of
the art on the OntoNotes development split. While our base-
line LSTM-CRF performs better than in the literature, our
strongest baseline (CNN+LSTM+CRF) does not match the
state of the art with a lexicon. We Ô¨Ånd that DeepType al-
ways improves over baselines and partially recovers lexicon
performance gains, but does not fully replace lexicons.
5 Related Work
Neural Network Reasoning with Symbolic structures
Several approaches exist for incorporating symbolic struc-
tures into the reasoning process of a neural network by de-
signing a loss function that is deÔ¨Åned with a label hierar-
chy. In particular the work of (Deng et al. 2012) trades off
speciÔ¨Åcity for accuracy, by leveraging the hyper/hyponymy
relation to make a model aware of different granularity lev-
els. Our work differs from this approach in that we design
our type system within an ontology to meet speciÔ¨Åc accu-
racy goals, while they make the accuracy/speciÔ¨Åcity tradeoff
at training time, with a Ô¨Åxed structure. More recently (Wu,
Tygert, and LeCun 2017) use a hierarchical loss to increase
the penalty for distant branches of a label hierarchy using the
ultrametric tree distance. We also aim to capture the most
important aspects of the symbolic structure and shape our
loss function accordingly, however our loss shaping is a re-
sult of discrete optimization and incorporates a learnability
heuristic to choose aspects that can easily be acquired.
A different direction for integrating structure stems from
constraining model outputs, or enforcing a grammar. In the
work of (Ling, Singh, and Weld 2015), the authors use
NER and FIGER types to ensure that an EL model follows
the constraints given by types. We also use a type systemand constrain our model‚Äôs output, however our type system
is task-speciÔ¨Åc and designed by a machine with a disam-
biguation accuracy objective, and unlike the authors we Ô¨Ånd
that types improve accuracy. The work of (Krishnamurthy,
Dasigi, and Gardner 2017) uses a type-aware grammar to
constrain the decoding of a neural semantic parser. Our work
makes use of type constraints during decoding, however the
grammar and types in their system require human engineer-
ing to Ô¨Åt each individual semantic parsing task, while our
type systems are based on online encyclopaedias and on-
tologies, with applications beyond EL.
Neural Entity Linking Current approaches to entity link-
ing make extensive use of deep neural networks, distributed
representations. In (Globerson et al. 2016) a neural net-
work uses attention to focus on contextual entities to dis-
ambiguate. While our work does not make use of attention,
RNNs allow context information to affect disambiguation
decisions. In the work of (Yamada et al. 2016) and (Yamada
et al. 2017), the authors adopt a distributed representation
of context which either models words and entities, or doc-
uments and entities such that distances between vectors in-
forms disambiguation. We also rely on word and document
vectors produced by RNNs, however entities are not explic-
itly represented in our neural network, and we use context to
predict entity types, thereby allowing us to incorporate new
entities without retraining.
6 Conclusion
In this work we introduce DeepType, a method for integrat-
ing symbolic knowledge into the reasoning process of a neu-
ral network. We‚Äôve proposed a mixed integer reformulation
for jointly designing type systems and training a classiÔ¨Åer
for a target task, and empirically validated that when this
technique is applied to EL it is effective at integrating sym-
bolic information in the neural network reasoning process.
When pre-training with DeepType for NER, we observe im-
proved performance over baselines and a new state of the art
on the OntoNotes dev set, suggesting there is cross-domain
transfer: symbolic information is incorporated in the neural
network‚Äôs distributed representation. Furthermore we Ô¨Ånd
that type systems designed by machines outperform those
designed by humans on three benchmark datasets, which
is attributable to incorporating learnability and target task
performance goals within the design process. Our approach
naturally enables multilingual training, and our experiments
show that bilingual training improves over monolingual, and
type systems optimized for English operate at similar accu-
racies in French, German, and Spanish, supporting the claim
that the type system optimization leads to the discovery of
high level cross-lingual concepts useful for knowledge rep-
resentation. We compare different search techniques, and
observe that stochastic optimization has comparable perfor-
mance to heuristic search, but with orders of magnitude less
objective function evaluations.
The main contributions of this work are a joint formu-
lation for designing and integrating symbolic information
into neural networks, that enable us to constrain the out-puts to obey symbolic structure, and an approach to EL that
uses type constraints. Our approach reduces EL resolution
complexity from O(N2)toO(N), while allowing new en-
tities to be incorporated without retraining, and we Ô¨Ånd on
three standard datasets (WikiDisamb30, CoNLL (YAGO),
TAC KBP 2010) that our approach outperforms all existing
solutions by a wide margin, including approaches that rely
on a human-designed type system (Ling, Singh, and Weld
2015) and the more recent work by Yamada et al. for embed-
ding words and entities (Yamada et al. 2016), or document
and entities (Yamada et al. 2017). As a result of our experi-
ments, we observe that disambiguation accuracy using Ora-
cles reaches 99.0% on CoNLL (YAGO) and 98.6% on TAC
KBP 2010, suggesting that EL would be almost solved if we
can close the gap between type classiÔ¨Åers and the Oracle.
The results presented in this work suggest many direc-
tions for future research: we may test how DeepType can
be applied to other problems where incorporating symbolic
structure is beneÔ¨Åcial, whether making type system design
more expressive by allowing hierarchies can help close the
gap between model and Oracle accuracy, and seeing if addi-
tional gains can be obtained by relaxing the classiÔ¨Åer‚Äôs con-
ditional independence assumption.
Acknowledgments We would like to thank the anony-
mous reviewers for their valuable feedback. In addition, we
thank John Miller, Andrew Gibiansky, and Szymon Sidor for
thoughtful comments and fruitful discussion.
Appendices
A Training details and hyperparameters
Optimization
Our models are implemented in TensorÔ¨Çow and optimized
with Adam with a learning rate of 10 4,1= 0:9;2=
0:999;= 10 8, annealed by 0.99 every 10,000 iterations.
To reduce over-Ô¨Åtting and make our system more robust
to spelling changes we apply Dropout to input embeddings
and augment our data with noise: swap input words with
a special <UNK> word, remove capitalization or a trailing
‚Äús.‚Äù In our NER experiments we add Gaussian noise during
training to the LSTM weights with = 10 6.
We use early stopping in our NER experiments when val-
idation F1 score stops increasing. Type classiÔ¨Åcation model
selection is different as the models did not overÔ¨Åt, thus we
instead stop training when no more improvements in F1 are
observed on held-out type-training data ( 3days on one
Titan X Pascal).
Architecture
Character representation Our character-convolutions
have character Ô¨Ålters with (width, channels):
f(1;50);(2;75);(3;75);(4;100);(5;200);(6;200);(7;200)g;
a maximum word length of 40, and 15-dimensional char-
acter embeddings followed by 2 highway layers. We learn
6-dimensional embeddings for 2 and 3 character preÔ¨Åxes
and sufÔ¨Åxes.Table 2: Hyperparameters for type system discovery search.
Method Parameter Value
Greedy b 1
Beam Search b 8CEMMCEM 1000
pstart50
jRj0:00115
NCEM 200GAG 200
Npopulation 1000
mutation probability 0.5
crossover probability 0.2
Text Window ClassiÔ¨Åer The text window classiÔ¨Åers have
5-dimensional word embeddings, and use Dropout of 0.5.
Empirically we Ô¨Ånd that two passes through the dataset with
a batch size of 128 is sufÔ¨Åcient for the window classiÔ¨Åers to
converge. Additionally we train multiple type axes in a sin-
gle batch, reaching a training speed of 2.5 type axes/second.
B Wikipedia Link SimpliÔ¨Åcation
Link statistics collected on large corpuses of entity mentions
are extensively used in entity linking. These statistics pro-
vide a noisy estimate of the conditional probability of an en-
tityefor a mention mP(ejm). Intra-wiki links in Wikipedia
provide a multilingual and broad coverage source of links,
however annotators often create link anaphoras: ‚Äúking‚Äù !
Charles I of England . This behavior increases polysemy
(‚Äúking‚Äù mention has 974 associated entities) and distorts link
frequencies (‚Äúqueen‚Äù links to the band Queen 4920 times,
Elizabeth II 1430 times, and monarch only 32 times).
Problems with link sparsity or anaphora were previously
identiÔ¨Åed, however present solutions rely on pruning rare
links and thus lose track of the original statistics (Ferrag-
ina and Scaiella 2010; Hasibi, Balog, and Bratsberg 2016;
Ling, Singh, and Weld 2015). We propose instead to de-
tect anaphoras and recover the generic meaning through
the Wikidata property graph: if a mention points to en-
tities A and B, with A being more linked than B, and
A is B‚Äôs parent in the Wikidata property graph, then re-
place B with A. We deÔ¨Åne A to be the parent of B if
they connect through a sequence of Wikidata properties
finstance of ,subclass of ,is a list ofg, or
through a single edge in foccupation ,position
held ,series16g. The simpliÔ¨Åcation process is repeated
until no more updates occur. This transformation reduces
the number of associated entities for each mention (‚Äúking‚Äù
senses drop from 974 to 143) and ensures that the semantics
15The choice of pstart affects the system size at the Ô¨Årst step of
the CEM search: setting it too low leads to poor search space ex-
ploration, while too high increase the cost of the objective function
evaluation. Empirically we know that for a given the solution will
have an expected size s. Settingpstart =s
jRjleads to sufÔ¨Åcient ex-
ploration to reach the performance of larger pstart.
16e.g.Return of the Jedi !
seriesStar WarsTable 3: Link change statistics per iteration during English
Wikipedia Anaphora SimpliÔ¨Åcation.
Step Replacements Links changed
1 1,109,408 9,212,321
2 13922 1,027,009
3 1229 364,500
4 153 40,488
5 74 25,094
6 4 1,498
of multiple speciÔ¨Åc links are aggregated (number of ‚Äúqueen‚Äù
links to monarch increase from 32 to 3553).
0 2000 4000 6000 8000 10000 12000
Polysemy100101102103104105106Count (log-scale)Original (Mean = 4.73)
Simplified (Mean = 3.93)
Figure 4: Mention Polysemy change after simpliÔ¨Åcation.
After simpliÔ¨Åcation we Ô¨Ånd that the mean number of
senses attached to polysemous mentions drops from 4.73 to
3.93, while over 10,670,910 links undergo changes in this
process (Figure 4). Table 3 indicates that most changes result
from mentions containing entities and their immediate par-
ents. This simpliÔ¨Åcation method strongly reduces the num-
ber of entities tied to each Wikipedia mention in an auto-
matic fashion across multiple languages.
C Multilingual Training Representation
Multilingual data creation is a side-effect of the ontology-
based automatic labeling scheme. In Table 4 we present
nearest-neighbor words for words in multiple languages.
We note that common words (he, Argentinian, hypothesis)
remain close to their foreign language counterpart, while
proper nouns group with country/language-speciÔ¨Åc terms.
We hypothesize that common words, by not fulÔ¨Ålling a role
as a label, can therefore operate in a language independent
way to inform the context of types, while proper nouns will
have different type requirements based on their labels, and
thus will not converge to the same representation.
D Effect of System Size Penalty
We measure the effect of varying on type system discov-
ery when using CEM for our search. The effect averaged
on 10 trials for a variety of penalties is shown in Figure
6. In particular we notice that there is a crossover point in
the performance characteristics when selecting , where a
looser penalty has diminishing returns in accuracy around
= 10 4.
105
104
103
102
Penalty 
0.9200.9250.9300.9350.9400.9450.9500.955Accuracy
Figure 5: Effect of varying on CEM type system discovery
E Learnability Heuristic behavior
To better understand the behavior of the population of classi-
Ô¨Åers used to obtain AUC scores for the Learnability heuris-
tic we investigate whether certain type axes are systemati-
cally easier or harder to predict, and summarize our results
in Figure 7. We Ô¨Ånd that type axes with a instance of
edge have on average higher AUC scores than type axes re-
lying on wikipedia category . Furthermore, we also
wanted to ensure that our methodology for estimating learn-
ability was not Ô¨Çawed or if variance in our measurement was
correlated with AUC for a type axis. We Ô¨Ånd that there is no
obvious relation between the standard deviation of the AUC
scores for a type axis and the AUC score itself.
F Multilingual Part of Speech Tagging
Finally the usage of multilingual data allows some amount
of subjective experiments. For instance in Figure 8 we show
some samples from the model trained jointly on english and
french correctly detecting the meaning of the word ‚Äúcar‚Äù
across three possible meanings.
G Human Type System
To assist humans with the design of the system, the rules are
built interactively in a REPL, and execute over the 24 million
entities in under 10 seconds, allowing for real time feedback
in the form of statistics or error analysis over an evaluation
corpus. On the evaluation corpus, disambiguation mistakes
can be grouped according to the ground truth type, allowing
a per type error analysis to easily detect areas where more
granularity would help. Shown below are the 5 different type
axes designed by humans.
References
[Abu-El-Haija et al. 2016] Abu-El-Haija, S.; Kothari, N.;
Lee, J.; Natsev, P.; Toderici, G.; Varadarajan, B.; and Vi-
jayanarasimhan, S. 2016. Youtube-8m: A large-scale video
classiÔ¨Åcation benchmark. arXiv preprint arXiv:1609.08675 .
[Andor et al. 2016] Andor, D.; Alberti, C.; Weiss, D.; Sev-
eryn, A.; Presta, A.; Ganchev, K.; Petrov, S.; and Collins,
M. 2016. Globally normalized transition-based neural net-
works. arXiv preprint arXiv:1603.06042 .Table 4: Top- kNearest neighbors (cosine distance) in shared English-French word vector space.
kArgentinian lui Sarkozy Clinton hypothesis
1argentin (0.259) he (0.333) Bayron (0.395) Reagan (0.413) paradox (0.388)
2Argentina (0.313) il (0.360) Peillon (0.409) Trump (0.441) Hypothesis (0.459)
3Argentine (0.315) him (0.398) Montebourg (0.419) Cheney (0.495) hypoth `ese (0.497)
Table 5: Additional set of Top- kNearest neighbors (cosine distance) in shared English-French word vector space.
kfeu computer
1killing (0.585) Computer (0.384)
2terrible (0.601) computers (0.446)
3beings (0.618) informatique (0.457)
[Chiu and Nichols 2015] Chiu, J. P., and Nichols, E. 2015.
Named entity recognition with bidirectional lstm-cnns.
arXiv preprint arXiv:1511.08308 .
[Deng et al. 2009] Deng, J.; Dong, W.; Socher, R.; Li, L.-J.;
Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on , 248‚Äì
255. IEEE.
[Deng et al. 2012] Deng, J.; Krause, J.; Berg, A. C.; and Fei-
Fei, L. 2012. Hedging your bets: Optimizing accuracy-
speciÔ¨Åcity trade-offs in large scale visual recognition. In
Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on , 3450‚Äì3457. IEEE.
[Ferragina and Scaiella 2010] Ferragina, P., and Scaiella, U.
2010. Tagme: on-the-Ô¨Çy annotation of short text fragments
(by wikipedia entities). In Proceedings of the 19th ACM in-
ternational conference on Information and knowledge man-
agement , 1625‚Äì1628. ACM.
[Globerson et al. 2016] Globerson, A.; Lazic, N.;
Chakrabarti, S.; Subramanya, A.; Ringaard, M.; and
Pereira, F. 2016. Collective entity resolution with multi-
focal attention. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume
1: Long Papers) , volume 1, 621‚Äì631.
[Harvey 2009] Harvey, I. 2009. The microbial genetic algo-
rithm. In European Conference on ArtiÔ¨Åcial Life , 126‚Äì133.
Springer.
[Hasibi, Balog, and Bratsberg 2016] Hasibi, F.; Balog, K.;
and Bratsberg, S. E. 2016. On the reproducibility of the
tagme entity linking system. In European Conference on
Information Retrieval , 436‚Äì449. Springer.
[Hoffart et al. 2011] Hoffart, J.; Yosef, M. A.; Bordino, I.;
F¬®urstenau, H.; Pinkal, M.; Spaniol, M.; Taneva, B.; Thater,
S.; and Weikum, G. 2011. Robust Disambiguation of Named
Entities in Text. In Conference on Empirical Methods in
Natural Language Processing, EMNLP 2011, Edinburgh,
Scotland , 782‚Äì792.
[Ji et al. 2010] Ji, H.; Grishman, R.; Dang, H. T.; GrifÔ¨Ått, K.;
and Ellis, J. 2010. Overview of the tac 2010 knowledgebase population track. In Third Text Analysis Conference
(TAC 2010) , volume 3, 3‚Äì3.
[Kay et al. 2017] Kay, W.; Carreira, J.; Simonyan, K.; Zhang,
B.; Hillier, C.; Vijayanarasimhan, S.; Viola, F.; Green, T.;
Back, T.; Natsev, P.; et al. 2017. The kinetics human action
video dataset. arXiv preprint arXiv:1705.06950 .
[Kim et al. 2015] Kim, Y .; Jernite, Y .; Sontag, D.; and Rush,
A. M. 2015. Character-aware neural language models. arXiv
preprint arXiv:1508.06615 .
[Krishnamurthy, Dasigi, and Gardner 2017] Krishnamurthy,
J.; Dasigi, P.; and Gardner, M. 2017. Neural semantic
parsing with type constraints for semi-structured tables. In
EMNLP , volume 17, 1532‚Äì1543.
[Lample et al. 2016] Lample, G.; Ballesteros, M.; Subrama-
nian, S.; Kawakami, K.; and Dyer, C. 2016. Neural ar-
chitectures for named entity recognition. arXiv preprint
arXiv:1603.01360 .
[Ling and Weld 2012] Ling, X., and Weld, D. S. 2012. Fine-
grained entity recognition. In AAAI . Citeseer.
[Ling, Singh, and Weld 2015] Ling, X.; Singh, S.; and Weld,
D. S. 2015. Design challenges for entity linking. Trans-
actions of the Association for Computational Linguistics
3:315‚Äì328.
[Milne and Witten 2008] Milne, D., and Witten, I. H. 2008.
Learning to link with wikipedia. In Proceedings of the 17th
ACM conference on Information and knowledge manage-
ment , 509‚Äì518. ACM.
[Pradhan et al. 2012] Pradhan, S.; Moschitti, A.; Xue, N.;
Uryupina, O.; and Zhang, Y . 2012. Conll-2012 shared
task: Modeling multilingual unrestricted coreference in
ontonotes. In EMNLP-CoNLL Shared Task .
[Rubinstein 1999] Rubinstein, R. 1999. The cross-
entropy method for combinatorial and continuous optimiza-
tion. Methodology and computing in applied probability
1(2):127‚Äì190.
[Sang and Meulder 2003] Sang, E. F. T. K., and Meulder,
F. D. 2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In CoNLL .
[Wu, Tygert, and LeCun 2017] Wu, C.; Tygert, M.; and Le-105
104
103
102
Penalty 
010203040506070Solution size
(a)
105
104
103
102
Penalty 
1015202530Iterations to convergence
 (b)
105
104
103
102
Penalty 
0.9200.9250.9300.9350.9400.9450.9500.955Accuracy
 (c)
105
104
103
102
Penalty 
0.900.910.920.930.94J
 (d)
Figure 6: Effect of varying on CEM type system discovery: Solution size (a) and iterations to convergence (b) grow exponen-
tially with penalty decrease, while accuracy plateaus (c) around = 10 4. Objective function increases as penalty decreases,
since solution size is less penalized (d). Standard deviation is shown as the red region around the mean.
0.0 0.2 0.4 0.6 0.8 1.0
AUC0.00.51.01.52.02.53.03.5Frequencywikipedia category
instance of
(a)
0.0 0.1 0.2 0.3 0.4
standard deviation AUC020004000600080001000012000Frequency (b)
0.0 0.2 0.4 0.6 0.8 1.0
AUC0.00.10.20.30.4std
 (c)
Figure 7: Most instance of type-axes have higher AUC scores than wikipedia categories (a). The standard deviation for AUC
scoring with text window classiÔ¨Åers is below 0.1 (b), AUC is not correlated with AUC‚Äôs standard deviation.
This
PRONis
VERBa
DETcar
NOUN,
PCTceci
PRONn‚Äô
PARTest
VERBpas
ADVune
DETvoiture
NOUNcar
CONJc‚Äô
PRONest
VERBun
DETcar
NOUN.
PCT
Figure 8: Model trained jointly on monolingual POS corpora
detecting the multiple meanings of ‚Äúcar‚Äù (shown in bold) in
a mixed English-French sentence.
Cun, Y . 2017. Hierarchical loss for classiÔ¨Åcation. arXiv
preprint arXiv:1709.01062 .
[Yamada et al. 2016] Yamada, I.; Shindo, H.; Takeda, H.;
and Takefuji, Y . 2016. Joint learning of the embedding of
words and entities for named entity disambiguation. arXiv
preprint arXiv:1601.01343 .
[Yamada et al. 2017] Yamada, I.; Shindo, H.; Takeda, H.;
and Takefuji, Y . 2017. Learning distributed representations
of texts and entities from knowledge base. arXiv preprint
arXiv:1705.02494 .Table 6: Human Type Axis: IsA
Activity
Aircraft
Airport
Algorithm
Alphabet
Anatomical structure
Astronomical object
Audio visual work
Award
Award ceremony
Battle
Book magazine article
Brand
Bridge
Character
Chemical compound
Clothing
Color
Concept
Country
Crime
Currency
Data format
Date
Developmental biology period
Disease
Electromagnetic wave
Event
Facility
Family
Fictional character
Food
Gas
Gene
Genre
Geographical object
Geometric shape
Hazard
Human
Human female
Human male
International relationsTable 7: Human Type Axis: IsA (continued)
Kinship
Lake
Language
Law
Legal action
Legal case
Legislative term
Mathematical object
Mind
Molecule
Monument
Mountain
Musical work
Name
Natural phenomenon
Number
Organization
Other art work
People
Person role
Physical object
Physical quantity
Plant
Populated place
Position
Postal code
Radio program
Railroad
Record chart
Region
Religion
Research
River
Road vehicle
Sea
Sexual orientation
Software
Song
Speech
Sport
Sport event
Sports terminology
Strategy
Taxon
Taxonomic rank
Title
Train station
Union
Unit of massTable 8: Human Type Axis: IsA (continued)
Value
Vehicle
Vehicle brand
V olcano
War
Watercraft
Weapon
Website
OtherTable 9: Human Type Axis: Topic
Archaeology
Automotive industry
Aviation
Biology
Botany
Business other
Construction
Culture
Culture-comics
Culture-dance
Culture-movie
Culture-music
Culture-painting
Culture-photography
Culture-sculpture
Culture-theatre
Culture arts other
Culture ceramic art
Culture circus
Culture literature
Economics
Education
Electronics
Energy
Engineering
Environment
Family
Fashion
Finance
Food
Health-alternative-medicine
Health-science-audiology
Health-science-biotechnology
Healthcare
Health cell
Health childbrith
Health drug
Health gene
Health hospital
Health human gene
Health insurance
Health life insurance
Health medical
Health med activism
Health med doctors
Health med society
Health organisations
Health people in health
Health pharma
Health proteinTable 10: Human Type Axis: Topic (continued)
Health protein wkp
Health science medicine
Heavy industry
Home
Hortculture and gardening
Labour
Law
Media
Military war crime
Nature
Nature-ecology
Philosophy
Politics
Populated places
Religion
Retail other
Science other
Science-anthropology
Science-astronomy
Science-biophysics
Science-chemistry
Science-computer science
Science-geography
Science-geology
Science-history
Science-mathematics
Science-physics
Science-psychology
Science-social science other
Science chronology
Science histology
Science meteorology
Sex industry
Smoking
Sport-air-sport
Sport-american football
Sport-athletics
Sport-australian football
Sport-baseball
Sport-basketball
Sport-climbing
Sport-combat sport
Sport-cricket
Sport-cue sport
Sport-cycling
Sport-darts
Sport-dog-sport
Sport-equestrian sport
Sport-Ô¨Åeld hockey
Sport-golfTable 11: Human Type Axis: Topic (continued)
Sport-handball
Sport-ice hockey
Sport-mind sport
Sport-motor sport
Sport-multisports
Sport-other
Sport-racquet sport
Sport-rugby
Sport-shooting
Sport-soccer
Sport-strength-sport
Sport-swimming
Sport-volleyball
Sport-winter sport
Sport water sport
Toiletry
Tourism
Transportation
Other
Table 12: Human Type Axis: Time
Post-1950
Pre-1950
Other
Table 13: Human Type Axis: Location
Africa
Antarctica
Asia
Europe
Middle East
North America
Oceania
Outer Space
Populated place unlocalized
South America
Other