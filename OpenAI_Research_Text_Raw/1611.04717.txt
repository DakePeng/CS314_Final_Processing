#Exploration: A Study of Count-Based Exploration
for Deep Reinforcement Learning
Haoran Tang1∗, Rein Houthooft34∗, Davis Foote2, Adam Stooke2, Xi Chen2†,
Yan Duan2†, John Schulman4, Filip De Turck3, Pieter Abbeel2†
1UC Berkeley, Department of Mathematics
2UC Berkeley, Department of Electrical Engineering and Computer Sciences
3Ghent University – imec, Department of Information Technology
4OpenAI
Abstract
Count-based exploration algorithms are known to perform near-optimally when
used in conjunction with tabular reinforcement learning (RL) methods for solving
small discrete Markov decision processes (MDPs). It is generally thought that
count-based methods cannot be applied in high-dimensional state spaces, since
most states will only occur once. Recent deep RL exploration strategies are able to
deal with high-dimensional continuous state spaces through complex heuristics,
often relying on optimism in the face of uncertainty orintrinsic motivation . In
this work, we describe a surprising ﬁnding: a simple generalization of the classic
count-based approach can reach near state-of-the-art performance on various high-
dimensional and/or continuous deep RL benchmarks. States are mapped to hash
codes, which allows to count their occurrences with a hash table. These counts
are then used to compute a reward bonus according to the classic count-based
exploration theory. We ﬁnd that simple hash functions can achieve surprisingly
good results on many challenging tasks. Furthermore, we show that a domain-
dependent learned hash code may further improve these results. Detailed analysis
reveals important aspects of a good hash function: 1) having appropriate granularity
and 2) encoding information relevant to solving the MDP. This exploration strategy
achieves near state-of-the-art performance on both continuous control tasks and
Atari 2600 games, hence providing a simple yet powerful baseline for solving
MDPs that require considerable exploration.
1 Introduction
Reinforcement learning (RL) studies an agent acting in an initially unknown environment, learning
through trial and error to maximize rewards. It is impossible for the agent to act near-optimally until
it has sufﬁciently explored the environment and identiﬁed all of the opportunities for high reward, in
all scenarios. A core challenge in RL is how to balance exploration—actively seeking out novel states
and actions that might yield high rewards and lead to long-term gains; and exploitation—maximizing
short-term rewards using the agent’s current knowledge. While there are exploration techniques
for ﬁnite MDPs that enjoy theoretical guarantees, there are no fully satisfying techniques for high-
dimensional state spaces; therefore, developing more general and robust exploration techniques is an
active area of research.
∗These authors contributed equally. Correspondence to: Haoran Tang <hrtang@math.berkeley.edu>, Rein
Houthooft <rein.houthooft@openai.com>
†Work done at OpenAI
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
arXiv:1611.04717v3  [cs.AI]  5 Dec 2017Most of the recent state-of-the-art RL results have been obtained using simple exploration strategies
such as uniform sampling [ 21] and i.i.d./correlated Gaussian noise [ 19,30]. Although these heuristics
are sufﬁcient in tasks with well-shaped rewards, the sample complexity can grow exponentially (with
state space size) in tasks with sparse rewards [ 25]. Recently developed exploration strategies for
deep RL have led to signiﬁcantly improved performance on environments with sparse rewards. Boot-
strapped DQN [24] led to faster learning in a range of Atari 2600 games by training an ensemble of
Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance
on Montezuma’s Revenge, an extremely challenging Atari 2600 game [ 4]. Variational Information
Maximizing Exploration (VIME, [ 13]) encourages the agent to explore by acquiring information
about environment dynamics, and performs well on various robotic locomotion problems with sparse
rewards. However, we have not seen a very simple and fast method that can work across different
domains.
Some of the classic, theoretically-justiﬁed exploration methods are based on counting state-action
visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB
algorithm of [ 18] chooses the action atat timetthat maximizes ˆr(at) +/radicalBig
2 logt
n(at)where ˆr(at)is
the estimated reward, and n(at)is the number of times action atwas previously chosen. In the
MDP setting, some of the algorithms have similar structure, for example, Model Based Interval
Estimation–Exploration Bonus (MBIE-EB) of [ 34] counts state-action pairs with a table n(s,a)and
adding a bonus reward of the formβ√
n(s,a)to encourage exploring less visited pairs. [ 16] show
that the inverse-square-root dependence is optimal. MBIE and related algorithms assume that the
augmented MDP is solved analytically at each timestep, which is only practical for small ﬁnite state
spaces.
This paper presents a simple approach for exploration, which extends classic counting-based methods
to high-dimensional, continuous state spaces. We discretize the state space with a hash function and
apply a bonus based on the state-visitation count. The hash function can be chosen to appropriately
balance generalization across states, and distinguishing between states. We select problems from rllab
[8] and Atari 2600 [ 3] featuring sparse rewards, and demonstrate near state-of-the-art performance on
several games known to be hard for naïve exploration strategies. The main strength of the presented
approach is that it is fast, ﬂexible and complementary to most existing RL algorithms.
In summary, this paper proposes a generalization of classic count-based exploration to high-
dimensional spaces through hashing (Section 2); demonstrates its effectiveness on challenging deep
RL benchmark problems and analyzes key components of well-designed hash functions (Section 4).
2 Methodology
2.1 Notation
This paper assumes a ﬁnite-horizon discounted Markov decision process (MDP), deﬁned by
(S,A,P,r,ρ 0,γ,T ), in whichSis the state space, Athe action space,Pa transition probabil-
ity distribution, r:S×A→ Ra reward function, ρ0an initial state distribution, γ∈(0,1]a
discount factor, and Tthe horizon. The goal of RL is to maximize the total expected discounted
reward Eπ,P/bracketleftBig/summationtextT
t=0γtr(st,at)/bracketrightBig
over a policy π, which outputs a distribution over actions given a
state.
2.2 Count-Based Exploration via Static Hashing
Our approach discretizes the state space with a hash function φ:S→Z. An exploration bonus
r+:S→Ris added to the reward function, deﬁned as
r+(s) =β/radicalbig
n(φ(s)), (1)
whereβ∈R≥0is the bonus coefﬁcient. Initially the counts n(·)are set to zero for the whole range
ofφ. For every state stencountered at time step t,n(φ(st))is increased by one. The agent is trained
with rewards (r+r+), while performance is evaluated as the sum of rewards without bonuses.
2Algorithm 1: Count-based exploration through static hashing, using SimHash
1Deﬁne state preprocessor g:S→RD
2(In case of SimHash) Initialize A∈Rk×Dwith entries drawn i.i.d. from the standard Gaussian
distributionN(0,1)
3Initialize a hash table with values n(·)≡0
4foreach iteration jdo
5 Collect a set of state-action samples {(sm,am)}M
m=0with policyπ
6 Compute hash codes through any LSH method, e.g., for SimHash, φ(sm) = sgn(Ag(sm))
7 Update the hash table counts ∀m: 0≤m≤Masn(φ(sm))←n(φ(sm)) + 1
8 Update the policy πusing rewards/braceleftbigg
r(sm,am) +β√
n(φ(sm))/bracerightbiggM
m=0with any RL algorithm
Note that our approach is a departure from count-based exploration methods such as MBIE-EB
since we use a state-space count n(s)rather than a state-action count n(s,a). State-action counts
n(s,a)are investigated in the Supplementary Material, but no signiﬁcant performance gains over
state counting could be witnessed. A possible reason is that the policy itself is sufﬁciently random to
try most actions at a novel state.
Clearly the performance of this method will strongly depend on the choice of hash function φ. One
important choice we can make regards the granularity of the discretization: we would like for “distant”
states to be be counted separately while “similar” states are merged. If desired, we can incorporate
prior knowledge into the choice of φ, if there would be a set of salient state features which are known
to be relevant. A short discussion on this matter is given in the Supplementary Material.
Algorithm 1 summarizes our method. The main idea is to use locality-sensitive hashing (LSH) to
convert continuous, high-dimensional data to discrete hash codes. LSH is a popular class of hash
functions for querying nearest neighbors based on certain similarity metrics [ 2]. A computationally
efﬁcient type of LSH is SimHash [ 6], which measures similarity by angular distance. SimHash
retrieves a binary code of state s∈Sas
φ(s) = sgn(Ag(s))∈{− 1,1}k, (2)
whereg:S→RDis an optional preprocessing function and Ais ak×Dmatrix with i.i.d. entries
drawn from a standard Gaussian distribution N(0,1). The value for kcontrols the granularity: higher
values lead to fewer collisions and are thus more likely to distinguish states.
2.3 Count-Based Exploration via Learned Hashing
When the MDP states have a complex structure, as is the case with image observations, measuring
their similarity directly in pixel space fails to provide the semantic similarity measure one would desire.
Previous work in computer vision [ 7,20,36] introduce manually designed feature representations
of images that are suitable for semantic tasks including detection and classiﬁcation. More recent
methods learn complex features directly from data by training convolutional neural networks [ 12,
17,31]. Considering these results, it may be difﬁcult for a method such as SimHash to cluster states
appropriately using only raw pixels.
Therefore, rather than using SimHash, we propose to use an autoencoder (AE) to learn meaningful
hash codes in one of its hidden layers as a more advanced LSH method. This AE takes as input
statessand contains one special dense layer comprised of Dsigmoid functions. By rounding the
sigmoid activations b(s)of this layer to their closest binary number ⌊b(s)⌉∈{ 0,1}D, any states
can be binarized. This is illustrated in Figure 1 for a convolutional AE.
A problem with this architecture is that dissimilar inputs si,sjcan map to identical hash codes
⌊b(si)⌉=⌊b(sj)⌉, but the AE still reconstructs them perfectly. For example, if b(si)andb(sj)have
values 0.6 and 0.7 at a particular dimension, the difference can be exploited by deconvolutional
layers in order to reconstruct siandsjperfectly, although that dimension rounds to the same binary
value. One can imagine replacing the bottleneck layer b(s)with the hash codes ⌊b(s)⌉, but then
gradients cannot be back-propagated through the rounding function. A solution is proposed by Gregor
et al. [10] and Salakhutdinov & Hinton [28] is to inject uniform noise U(−a,a)into the sigmoid
36×6 6×6 6×6 6×6 6×6 6×6⌊·⌉ codedownsample
softmax linear
64×52×52 1×52×5296×24×2496×10×1096×5×5
2400b(·)
512
102496×5×5
96×11×11
96×24×24
1×52×52
Figure 1: The autoencoder (AE) architecture for ALE; the solid block represents the dense sigmoidal
binary code layer, after which noise U(−a,a)is injected.
Algorithm 2: Count-based exploration using learned hash codes
1Deﬁne state preprocessor g:S→{ 0,1}Das the binary code resulting from the autoencoder
(AE)
2InitializeA∈Rk×Dwith entries drawn i.i.d. from the standard Gaussian distribution N(0,1)
3Initialize a hash table with values n(·)≡0
4foreach iteration jdo
5 Collect a set of state-action samples {(sm,am)}M
m=0with policyπ
6 Add the state samples {sm}M
m=0to a FIFO replay pool R
7 ifjmodjupdate = 0then
8 Update the AE loss function in Eq. (3) using samples drawn from the replay pool
{sn}N
n=1∼R , for example using stochastic gradient descent
9 Computeg(sm) =⌊b(sm)⌉, theD-dim rounded hash code for smlearned by the AE
10 Projectg(sm)to a lower dimension kvia SimHash as φ(sm) = sgn(Ag(sm))
11 Update the hash table counts ∀m: 0≤m≤Masn(φ(sm))←n(φ(sm)) + 1
12 Update the policy πusing rewards/braceleftbigg
r(sm,am) +β√
n(φ(sm))/bracerightbiggM
m=0with any RL algorithm
activations. By choosing uniform noise with a>1
4, the AE is only capable of (always) reconstructing
distinct state inputs si/negationslash=sj, if it has learned to spread the sigmoid outputs sufﬁciently far apart,
|b(si)−b(sj)|>/epsilon1, in order to counteract the injected noise.
As such, the loss function over a set of collected states {si}N
i=1is deﬁned as
L/parenleftbig
{sn}N
n=1/parenrightbig
=−1
NN/summationdisplay
n=1/bracketleftBig
logp(sn)−λ
K/summationtextD
i=1min/braceleftBig
(1−bi(sn))2,bi(sn)2/bracerightBig/bracketrightBig
, (3)
withp(sn)the AE output. This objective function consists of a negative log-likelihood term and a
term that pressures the binary code layer to take on binary values, scaled by λ∈R≥0. The reasoning
behind this latter term is that it might happen that for particular states, a certain sigmoid unit is never
used. Therefore, its value might ﬂuctuate around1
2, causing the corresponding bit in binary code
⌊b(s)⌉to ﬂip over the agent lifetime. Adding this second loss term ensures that an unused bit takes
on an arbitrary binary value.
For Atari 2600 image inputs, since the pixel intensities are discrete values in the range [0,255],
we make use of a pixel-wise softmax output layer [ 37] that shares weights between all pixels. The
architectural details are described in the Supplementary Material and are depicted in Figure 1. Because
the code dimension often needs to be large in order to correctly reconstruct the input, we apply a
downsampling procedure to the resulting binary code ⌊b(s)⌉, which can be done through random
projection to a lower-dimensional space via SimHash as in Eq. (2).
On the one hand, it is important that the mapping from state to code needs to remain relatively
consistent over time, which is nontrivial as the AE is constantly updated according to the latest data
(Algorithm 2 line 8). A solution is to downsample the binary code to a very low dimension, or by
slowing down the training process. On the other hand, the code has to remain relatively unique
4for states that are both distinct and close together on the image manifold. This is tackled both by
the second term in Eq. (3)and by the saturating behavior of the sigmoid units. States already well
represented by the AE tend to saturate the sigmoid activations, causing the resulting loss gradients to
be close to zero, making the code less prone to change.
3 Related Work
Classic count-based methods such as MBIE [ 33], MBIE-EB and [ 16] solve an approximate Bellman
equation as an inner loop before the agent takes an action [ 34]. As such, bonus rewards are propagated
immediately throughout the state-action space. In contrast, contemporary deep RL algorithms
propagate the bonus signal based on rollouts collected from interacting with environments, with
value-based [ 21] or policy gradient-based [ 22,30] methods, at limited speed. In addition, our
proposed method is intended to work with contemporary deep RL algorithms, it differs from classical
count-based method in that our method relies on visiting unseen states ﬁrst, before the bonus reward
can be assigned, making uninformed exploration strategies still a necessity at the beginning. Filling
the gaps between our method and classic theories is an important direction of future research.
A related line of classical exploration methods is based on the idea of optimism in the face of
uncertainty [5] but not restricted to using counting to implement “optimism”, e.g., R-Max [ 5], UCRL
[14], and E3[15]. These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in
tabular settings.
Bayesian RL methods [ 9,11,16,35], which keep track of a distribution over MDPs, are an alternative
to optimism-based methods. Extensions to continuous state space have been proposed by [ 27] and
[25].
Another type of exploration is curiosity-based exploration. These methods try to capture the agent’s
surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers
novel states. We refer the reader to [ 29] and [ 26] for an extensive review on curiosity and intrinsic
rewards.
Several exploration strategies for deep RL have been proposed to handle high-dimensional state
space recently. [ 13] propose VIME, in which information gain is measured in Bayesian neural
networks modeling the MDP dynamics, which is used an exploration bonus. [ 32] propose to use the
prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through
bootstrapping is proposed by [24], using bootstrapped Q-functions.
The most related exploration strategy is proposed by [ 4], in which an exploration bonus is added
inversely proportional to the square root of a pseudo-count quantity. A state pseudo-count is derived
from its log-probability improvement according to a density model over the state space, which in the
limit converges to the empirical count. Our method is similar to pseudo-count approach in the sense
that both methods are performing approximate counting to have the necessary generalization over
unseen states. The difference is that a density model has to be designed and learned to achieve good
generalization for pseudo-count whereas in our case generalization is obtained by a wide range of
simple hash functions (not necessarily SimHash). Another interesting connection is that our method
also implies a density model ρ(s) =n(φ(s))
Nover all visited states, where Nis the total number of
states visited. Another method similar to hashing is proposed by [ 1], which clusters states and counts
cluster centers instead of the true states, but this method has yet to be tested on standard exploration
benchmark problems.
4 Experiments
Experiments were designed to investigate and answer the following research questions:
1.Can count-based exploration through hashing improve performance signiﬁcantly across
different domains? How does the proposed method compare to the current state of the art in
exploration for deep RL?
2.What is the impact of learned or static state preprocessing on the overall performance when
image observations are used?
5To answer question 1, we run the proposed method on deep RL benchmarks (rllab and ALE) that
feature sparse rewards, and compare it to other state-of-the-art algorithms. Question 2 is answered by
trying out different image preprocessors on Atari 2600 games. Trust Region Policy Optimization
(TRPO, [ 30]) is chosen as the RL algorithm for all experiments, because it can handle both discrete
and continuous action spaces, can conveniently ensure stable improvement in the policy performance,
and is relatively insensitive to hyperparameter changes. The hyperparameters settings are reported in
the Supplementary Material.
4.1 Continuous Control
The rllab benchmark [ 8] consists of various control tasks to test deep RL algorithms. We selected
several variants of the basic and locomotion tasks that use sparse rewards, as shown in Figure 2, and
adopt the experimental setup as deﬁned in [ 13]—a description can be found in the Supplementary
Material. These tasks are all highly difﬁcult to solve with naïve exploration strategies, such as adding
Gaussian noise to the actions.
Figure 2: Illustrations of the rllab tasks used in the continuous control experiments, namely Moun-
tainCar, CartPoleSwingup, SimmerGather, and HalfCheetah; taken from [8].
(a) MountainCar
 (b) CartPoleSwingup
 (c) SwimmerGather
 (d) HalfCheetah
Figure 3: Mean average return of different algorithms on rllab tasks with sparse rewards. The solid
line represents the mean average return, while the shaded area represents one standard deviation, over
5seeds for the baseline and SimHash (the baseline curves happen to overlap with the axis).
Figure 3 shows the results of TRPO (baseline), TRPO-SimHash, and VIME [ 13] on the classic tasks
MountainCar and CartPoleSwingup, the locomotion task HalfCheetah, and the hierarchical task
SwimmerGather. Using count-based exploration with hashing is capable of reaching the goal in all
environments (which corresponds to a nonzero return), while baseline TRPO with Gaussia n control
noise fails completely. Although TRPO-SimHash picks up the sparse reward on HalfCheetah, it does
not perform as well as VIME. In contrast, the performance of SimHash is comparable with VIME on
MountainCar, while it outperforms VIME on SwimmerGather.
4.2 Arcade Learning Environment
The Arcade Learning Environment (ALE, [ 3]), which consists of Atari 2600 video games, is an
important benchmark for deep RL due to its high-dimensional state space and wide variety of
games. In order to demonstrate the effectiveness of the proposed exploration strategy, six games are
selected featuring long horizons while requiring signiﬁcant exploration: Freeway, Frostbite, Gravitar,
Montezuma’s Revenge, Solaris, and Venture. The agent is trained for 500iterations in all experiments,
with each iteration consisting of 0.1 Msteps (the TRPO batch size, corresponds to 0.4 Mframes).
Policies and value functions are neural networks with identical architectures to [ 22]. Although the
policy and baseline take into account the previous four frames, the counting algorithm only looks at
the latest frame.
6Table 1: Atari 2600: average total reward after training for 50 M time steps. Boldface numbers
indicate best results. Italic numbers are the best among our methods.
Freeway Frostbite Gravitar Montezuma Solaris Venture
TRPO (baseline) 16.5 2869 486 0 2758 121
TRPO-pixel-SimHash 31.6 4683 468 0 2897 263
TRPO-BASS-SimHash 28.4 3150 604 238 1201 616
TRPO-AE-SimHash 33.5 5214 482 75 4467 445
Double-DQN 33.3 1683 412 0 3068 98 .0
Dueling network 0.0 4672 588 0 2251 497
Gorila 11.7 605 1054 4 N/A 1245
DQN Pop-Art 33.4 3469 483 0 4544 1172
A3C+ 27.3 507 246 142 2175 0
pseudo-count 29.2 1450 – 3439 – 369
BASS To compare with the autoencoder-based learned hash code, we propose using Basic Ab-
straction of the ScreenShots (BASS, also called Basic; see [ 3]) as a static preprocessing function g.
BASS is a hand-designed feature transformation for images in Atari 2600 games. BASS builds on the
following observations speciﬁc to Atari: 1) the game screen has a low resolution, 2) most objects are
large and monochrome, and 3) winning depends mostly on knowing object locations and motions.
We designed an adapted version of BASS3, that divides the RGB screen into square cells, computes
the average intensity of each color channel inside a cell, and assigns the resulting values to bins that
uniformly partition the intensity range [0,255]. Mathematically, let Cbe the cell size (width and
height),Bthe number of bins, (i,j)cell location, (x,y)pixel location, and zthe channel, then
feature(i,j,z ) =/floorleftBig
B
255C2/summationtext
(x,y)∈cell(i,j)I(x,y,z )/floorrightBig
. (4)
Afterwards, the resulting integer-valued feature tensor is converted to an integer hash code ( φ(st)in
Line 6 of Algorithm 1). A BASS feature can be regarded as a miniature that efﬁciently encodes object
locations, but remains invariant to negligible object motions. It is easy to implement and introduces
little computation overhead. However, it is designed for generic Atari game images and may not
capture the structure of each speciﬁc game very well.
We compare our results to double DQN [ 39], dueling network [ 40], A3C+ [ 4], double DQN with
pseudo-counts [ 4], Gorila [ 23], and DQN Pop-Art [ 38] on the “null op” metric4. We show training
curves in Figure 4 and summarize all results in Table 1. Surprisingly, TRPO-pixel-SimHash already
outperforms the baseline by a large margin and beats the previous best result on Frostbite. TRPO-
BASS-SimHash achieves signiﬁcant improvement over TRPO-pixel-SimHash on Montezuma’s
Revenge and Venture, where it captures object locations better than other methods.5TRPO-AE-
SimHash achieves near state-of-the-art performance on Freeway, Frostbite and Solaris.
As observed in Table 1, preprocessing images with BASS or using a learned hash code through the
AE leads to much better performance on Gravitar, Montezuma’s Revenge and Venture. Therefore, a
static or adaptive preprocessing step can be important for a good hash function.
In conclusion, our count-based exploration method is able to achieve remarkable performance gains
even with simple hash functions like SimHash on the raw pixel space. If coupled with domain-
dependent state preprocessing techniques, it can sometimes achieve far better results.
A reason why our proposed method does not achieve state-of-the-art performance on all games is that
TRPO does not reuse off-policy experience, in contrast to DQN-based algorithms [ 4,23,38]), and is
3The original BASS exploits the fact that at most 128colors can appear on the screen. Our adapted version
does not make this assumption.
4The agent takes no action for a random number (within 30) of frames at the beginning of each episode.
5We provide videos of example game play and visualizations of the difference bewteen Pixel-SimHash and
BASS-SimHash at https://www.youtube.com/playlist?list=PLAd-UMX6FkBQdLNWtY8nH1-pzYJA_1T55
70 100 200 300 400 500−505101520253035(a) Freeway
0 100 200 300 400 5000200040006000800010000 (b) Frostbite
0 100 200 300 400 5001002003004005006007008009001000
TRPO-AE-SimHash
TRPO
TRPO-BASS-SimHash
TRPO-pixel-SimHash (c) Gravitar
0 100 200 300 400 5000100200300400500
(d) Montezuma’s Revenge
0 100 200 300 400 500−100001000200030004000500060007000 (e) Solaris
0 100 200 300 400 500−200020040060080010001200 (f) Venture
Figure 4: Atari 2600 games: the solid line is the mean average undiscounted return per iteration,
while the shaded areas represent the one standard deviation, over 5seeds for the baseline, TRPO-
pixel-SimHash, and TRPO-BASS-SimHash, while over 3seeds for TRPO-AE-SimHash.
hence less efﬁcient in harnessing extremely sparse rewards. This explanation is corroborated by the
experiments done in [ 4], in which A3C+ (an on-policy algorithm) scores much lower than DQN (an
off-policy algorithm), while using the exact same exploration bonus.
5 Conclusions
This paper demonstrates that a generalization of classical counting techniques through hashing is able
to provide an appropriate signal for exploration, even in continuous and/or high-dimensional MDPs
using function approximators, resulting in near state-of-the-art performance across benchmarks. It
provides a simple yet powerful baseline for solving MDPs that require informed exploration.
Acknowledgments
We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This
research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a
Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a
Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through
grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through
the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Adam Stooke gratefully
acknowledges funding from a Fannie and John Hertz Foundation fellowship. Rein Houthooft was
supported by a Ph.D. Fellowship of the Research Foundation - Flanders (FWO).
References
[1]Abel, David, Agarwal, Alekh, Diaz, Fernando, Krishnamurthy, Akshay, and Schapire, Robert E.
Exploratory gradient boosting for reinforcement learning in complex domains. arXiv preprint
arXiv:1603.04119 , 2016.
[2]Andoni, Alexandr and Indyk, Piotr. Near-optimal hashing algorithms for approximate near-
est neighbor in high dimensions. In Proceedings of the 47th Annual IEEE Symposium on
Foundations of Computer Science (FOCS) , pp. 459–468, 2006.
[3]Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning
environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence
Research , 47:253–279, 06 2013.
8[4]Bellemare, Marc G, Srinivasan, Sriram, Ostrovski, Georg, Schaul, Tom, Saxton, David, and
Munos, Remi. Unifying count-based exploration and intrinsic motivation. In Advances in
Neural Information Processing Systems 29 (NIPS) , pp. 1471–1479, 2016.
[5]Brafman, Ronen I and Tennenholtz, Moshe. R-max-a general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research , 3:213–231, 2002.
[6]Charikar, Moses S. Similarity estimation techniques from rounding algorithms. In Proceedings
of the 34th Annual ACM Symposium on Theory of Computing (STOC) , pp. 380–388, 2002.
[7]Dalal, Navneet and Triggs, Bill. Histograms of oriented gradients for human detection. In
Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition
(CVPR) , pp. 886–893, 2005.
[8]Duan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John, and Abbeel, Pieter. Benchmarking
deep reinforcement learning for continous control. In Proceedings of the 33rd International
Conference on Machine Learning (ICML) , pp. 1329–1338, 2016.
[9]Ghavamzadeh, Mohammad, Mannor, Shie, Pineau, Joelle, and Tamar, Aviv. Bayesian rein-
forcement learning: A survey. Foundations and Trends in Machine Learning , 8(5-6):359–483,
2015.
[10] Gregor, Karol, Besse, Frederic, Jimenez Rezende, Danilo, Danihelka, Ivo, and Wierstra, Daan.
Towards conceptual compression. In Advances in Neural Information Processing Systems 29
(NIPS) , pp. 3549–3557. 2016.
[11] Guez, Arthur, Heess, Nicolas, Silver, David, and Dayan, Peter. Bayes-adaptive simulation-based
search with value function approximation. In Advances in Neural Information Processing
Systems (Advances in Neural Information Processing Systems (NIPS)) , pp. 451–459, 2014.
[12] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image
recognition. 2015.
[13] Houthooft, Rein, Chen, Xi, Duan, Yan, Schulman, John, De Turck, Filip, and Abbeel, Pieter.
VIME: Variational information maximizing exploration. In Advances in Neural Information
Processing Systems 29 (NIPS) , pp. 1109–1117, 2016.
[14] Jaksch, Thomas, Ortner, Ronald, and Auer, Peter. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research , 11:1563–1600, 2010.
[15] Kearns, Michael and Singh, Satinder. Near-optimal reinforcement learning in polynomial time.
Machine Learning , 49(2-3):209–232, 2002.
[16] Kolter, J Zico and Ng, Andrew Y . Near-bayesian exploration in polynomial time. In Proceedings
of the 26th International Conference on Machine Learning (ICML) , pp. 513–520, 2009.
[17] Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet classiﬁcation with deep
convolutional neural networks. In Advances in Neural Information Processing Systems 25
(NIPS) , pp. 1097–1105, 2012.
[18] Lai, Tze Leung and Robbins, Herbert. Asymptotically efﬁcient adaptive allocation rules.
Advances in Applied Mathematics , 6(1):4–22, 1985.
[19] Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa,
Yuval, Silver, David, and Wierstra, Daan. Continuous control with deep reinforcement learning.
arXiv preprint arXiv:1509.02971 , 2015.
[20] Lowe, David G. Object recognition from local scale-invariant features. In Proceedings of the
7th IEEE International Conference on Computer Vision (ICCV) , pp. 1150–1157, 1999.
[21] Mnih, V olodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,
Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al.
Human-level control through deep reinforcement learning. Nature , 518(7540):529–533, 2015.
9[22] Mnih, V olodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi, Graves, Alex, Lillicrap, Timo-
thy P, Harley, Tim, Silver, David, and Kavukcuoglu, Koray. Asynchronous methods for deep
reinforcement learning. arXiv preprint arXiv:1602.01783 , 2016.
[23] Nair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alcicek, Cagdas, Fearon, Rory, De Maria,
Alessandro, Panneershelvam, Vedavyas, Suleyman, Mustafa, Beattie, Charles, Petersen,
Stig, et al. Massively parallel methods for deep reinforcement learning. arXiv preprint
arXiv:1507.04296 , 2015.
[24] Osband, Ian, Blundell, Charles, Pritzel, Alexander, and Van Roy, Benjamin. Deep exploration
via bootstrapped DQN. In Advances in Neural Information Processing Systems 29 (NIPS) , pp.
4026–4034, 2016.
[25] Osband, Ian, Van Roy, Benjamin, and Wen, Zheng. Generalization and exploration via random-
ized value functions. In Proceedings of the 33rd International Conference on Machine Learning
(ICML) , pp. 2377–2386, 2016.
[26] Oudeyer, Pierre-Yves and Kaplan, Frederic. What is intrinsic motivation? A typology of
computational approaches. Frontiers in Neurorobotics , 1:6, 2007.
[27] Pazis, Jason and Parr, Ronald. PAC optimal exploration in continuous space Markov decision
processes. In Proceedings of the 27th AAAI Conference on Artiﬁcial Intelligence (AAAI) , 2013.
[28] Salakhutdinov, Ruslan and Hinton, Geoffrey. Semantic hashing. International Journal of
Approximate Reasoning , 50(7):969 – 978, 2009.
[29] Schmidhuber, Jürgen. Formal theory of creativity, fun, and intrinsic motivation (1990–2010).
IEEE Transactions on Autonomous Mental Development , 2(3):230–247, 2010.
[30] Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael I, and Abbeel, Pieter. Trust
region policy optimization. In Proceedings of the 32nd International Conference on Machine
Learning (ICML) , 2015.
[31] Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 , 2014.
[32] Stadie, Bradly C, Levine, Sergey, and Abbeel, Pieter. Incentivizing exploration in reinforcement
learning with deep predictive models. arXiv preprint arXiv:1507.00814 , 2015.
[33] Strehl, Alexander L and Littman, Michael L. A theoretical analysis of model-based interval
estimation. In Proceedings of the 21st International Conference on Machine Learning (ICML) ,
pp. 856–863, 2005.
[34] Strehl, Alexander L and Littman, Michael L. An analysis of model-based interval estimation for
Markov decision processes. Journal of Computer and System Sciences , 74(8):1309–1331, 2008.
[35] Sun, Yi, Gomez, Faustino, and Schmidhuber, Jürgen. Planning to be surprised: Optimal
Bayesian exploration in dynamic environments. In Proceedings of the 4th International Confer-
ence on Artiﬁcial General Intelligence (AGI) , pp. 41–51. 2011.
[36] Tola, Engin, Lepetit, Vincent, and Fua, Pascal. DAISY: An efﬁcient dense descriptor applied to
wide-baseline stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence , 32(5):
815–830, 2010.
[37] van den Oord, Aaron, Kalchbrenner, Nal, and Kavukcuoglu, Koray. Pixel recurrent neural
networks. In Proceedings of the 33rd International Conference on Machine Learning (ICML) ,
pp. 1747–1756, 2016.
[38] van Hasselt, Hado, Guez, Arthur, Hessel, Matteo, and Silver, David. Learning functions across
many orders of magnitudes. arXiv preprint arXiv:1602.07714 , 2016.
[39] van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep reinforcement learning with double
Q-learning. In Proceedings of the 30th AAAI Conference on Artiﬁcial Intelligence (AAAI) , 2016.
[40] Wang, Ziyu, de Freitas, Nando, and Lanctot, Marc. Dueling network architectures for deep
reinforcement learning. In Proceedings of the 33rd International Conference on Machine
Learning (ICML) , pp. 1995–2003, 2016.
10Supplementary Material
Anonymous Author(s)
Afﬁliation
Address
email
1 Hyperparameter Settings 1
Throughout all experiments, we use Adam [ 8] for optimizing the baseline function and the autoen- 2
coder. Hyperparameters for rllab experiments are summarized in Table 1. Here the policy takes 3
a statesas input, and outputs a Gaussian distribution N(µ(s),σ2), whereµ(s)is the output of a 4
multi-layer perceptron (MLP) with tanh nonlinearity, and σ>0is a state-independent parameter. 5
Table 1: TRPO hyperparameters for rllab experiments
Experiment MountainCar CartPoleSwingUp HalfCheetah SwimmerGatherer
TRPO batch size 5k 5k 5k 50k
TRPO step size 0.01
Discount factor γ 0.99
Policy hidden units (32, 32) (32, ) (32, 32) (64, 32)
Baseline function linear linear linear MLP: 32 units
Exploration bonus β= 0.01
SimHash dimension k= 32
Hyperparameters for Atari 2600 experiments are summarized in Table 2 and 3. By default, all 6
convolutional layers are followed by ReLU nonlinearity. 7
Table 2: TRPO hyperparameters for Atari experiments with image input
Experiment TRPO-pixel-SimHash TRPO-BASS-SimHash TRPO-AE-SimHash
TRPO batch size 100k
TRPO step size 0.01
Discount factor 0.995
# random seeds 5 5 3
Input preprocessing grayscale; downsampled to 52×52; each pixel rescaled to [−1,1]
4previous frames are concatenated to form the input state
Policy structure 16conv ﬁlters of size 8×8, stride 4
32conv ﬁlters of size 4×4, stride 2
fully-connect layer with 256units
linear transform and softmax to output action probabilities
(use batch normalization[7] at every layer)
Baseline structure (same as policy, except that the last layer is a single scalar)
Exploration bonus β= 0.01
Hashing parameters k= 256 cell sizeC= 20 b(s)size: 256bits
B= 20 bins downsampled to 64bits
The autoencoder architecture was shown in Figure 1 of Section 2.3. Speciﬁcally, uniform noise 8
U(−a,a)witha= 0.3is added to the sigmoid activations. The loss function Eq.(3) (in the main 9
Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017). Do not distribute.Table 3: TRPO hyperparameters for Atari experiments with RAM input
Experiment TRPO-RAM-SimHash
TRPO batch size 100k
TRPO step size 0.01
Discount factor 0.995
# random seeds 10
Input preprocessing vector of length 128in the range [0,255]; downsampled to [−1,1]
Policy structure MLP: ( 32,32, number_of_actions), tanh
Baseline structure MLP: ( 32,32,1),tanh
Exploration bonus β= 0.01
SimHash dimension k= 256
text), using λ= 10 , is updated every jupdate = 3 iterations. The architecture looks as follows: an 10
input layer of size 52×52, representing the image luminance is followed by 3consecutive 6×6 11
convolutional layers with stride 2and96ﬁlters feed into a fully connected layer of size 1024 , which 12
connects to the binary code layer. This binary code layer feeds into a fully-connected layer of 1024 13
units, connecting to a fully-connected layer of 2400 units. This layer feeds into 3consecutive 6×6 14
transposed convolutional layers of which the ﬁnal one connects to a pixel-wise softmax layer with 64 15
bins, representing the pixel intensities. Moreover, label smoothing is applied to the different softmax 16
bins, in which the log-probability of each of the bins is increased by 0.003, before normalizing. The 17
softmax weights are shared among each pixel. 18
In addition, we apply counting Bloom ﬁlters [ 5] to maintain a small hash table. Details can be found 19
in Appendix 4. 20
2 Description of the Adapted rllab Tasks 21
This section describes the continuous control environments used in the experiments. The tasks are 22
implemented as described in [ 4], following the sparse reward adaptation of [ 6]. The tasks have the 23
following state and action dimensions: CartPoleSwingup, S⊆R4,A⊆R; MountainCarS⊆R3, 24
A⊆R1; HalfCheetah,S⊆R20,A⊆R6; SwimmerGather, S⊆R33,A⊆R2. For the sparse 25
reward experiments, the tasks have been modiﬁed as follows. In CartPoleSwingup, the agent receives 26
a reward of +1when cos(β)>0.8, withβthe pole angle. In MountainCar, the agent receives 27
a reward of +1when the goal state is reached, namely escaping the valley from the right side. 28
Therefore, the agent has to ﬁgure out how to swing up the pole in the absence of any initial external 29
rewards. In HalfCheetah, the agent receives a reward of +1whenxbody>5. As such, it has to ﬁgure 30
out how to move forward without any initial external reward. The time horizon is set to T= 500 for 31
all tasks. 32
3 Analysis of Learned Binary Representation 33
Figure 1 shows the downsampled codes learned by the autoencoder for several Atari 2600 games 34
(Frostbite, Freeway, and Montezuma’s Revenge). Each row depicts 50consecutive frames (from 0to 35
49, going from left to right, top to bottom). The pictures in the right column depict the binary codes 36
that correspond with each of these frames (one frame per row). Figure 2 shows the reconstructions 37
of several subsequent images according to the autoencoder. Some binaries stay consistent across 38
frames, and some appear to respond to speciﬁc objects or events. Although the precise meaning of 39
each binary number is not immediately obvious, the ﬁgure suggests that the learned hash code is a 40
reasonable abstraction of the game state. 41
4 Counting Bloom Filter/Count-Min Sketch 42
We experimented with directly building a hashing dictionary with keys φ(s)and values the state 43
counts, but observed an unnecessary increase in computation time. Our implementation converts the 44
integer hash codes into binary numbers and then into the “bytes” type in Python. The hash table is a 45
dictionary using those bytes as keys. 46
2Figure 1: Frostbite, Freeway, and Montezuma’s Revenge: subsequent frames (left) and corresponding
code (right); the frames are ordered from left (starting with frame number 0) to right, top to bottom;
the vertical axis in the right images correspond to the frame number.
3However, an alternative technique called Count-Min Sketch [ 3], with a data structure identical 47
to counting Bloom ﬁlters [ 5], can count with a ﬁxed integer array and thus reduce computation 48
time. Speciﬁcally, let p1,...,plbe distinct large prime numbers and deﬁne φj(s) =φ(s) modpj. 49
The count of state sis returned as min 1≤j≤lnj/parenleftbig
φj(s)/parenrightbig
. To increase the count of s, we increment 50
nj/parenleftbig
φj(s)/parenrightbig
by1for allj. Intuitively, the method replaces φby weaker hash functions, while it reduces 51
the probability of over-counting by reporting counts agreed by all such weaker hash functions. The 52
ﬁnal hash code is represented as/parenleftbig
φ1(s),...,φl(s)/parenrightbig
. 53
Throughout all experiments above, the prime numbers for the counting Bloom ﬁlter are 999931 , 54
999953 ,999959 ,999961 ,999979 , and 999983 , which we abbreviate as “ 6 M”. In addition, we 55
experimented with 6other prime numbers, each approximately 15 M , which we abbreviate as “ 90 M ”. 56
As we can see in Figure 3, counting states with a dictionary or with Bloom ﬁlters lead to similar 57
performance, but the computation time of latter is lower. Moreover, there is little difference between 58
direct counting and using a very larger table for Bloom ﬁlters, as the average bonus rewards are 59
almost the same, indicating the same degree of exploration-exploitation trade-off. On the other hand, 60
Bloom ﬁlters require a ﬁxed table size, which may not be known beforehand. 61
Theory of Bloom Filters Bloom ﬁlters [ 2] are popular for determining whether a data sample s/prime62
belongs to a dataset D. Suppose we have lfunctionsφjthat independently assign each data sample 63
to an integer between 1andpuniformly at random. Initially 1,2,...,p are marked as 0. Then every 64
s∈D is “inserted” through marking φj(s)as1for allj. A new sample s/primeis reported as a member 65
ofDonly ifφj(s)are marked as 1for allj. A bloom ﬁlter has zero false negative rate (any s∈D is 66
reported a member), while the false positive rate (probability of reporting a nonmember as a member) 67
decays exponentially in l. 68
Though Bloom ﬁlters support data insertion, it does not allow data deletion. Counting Bloom ﬁlters 69
[5] maintain a counter n(·)for each number between 1andp. Inserting/deleting scorresponds 70
to incrementing/decrementing n/parenleftbig
φj(s)/parenrightbig
by 1 for all j. Similarly, sis considered a member if 71
∀j:n/parenleftbig
φj(s)/parenrightbig
= 0. 72
Count-Min sketch is designed to support memory-efﬁcient counting without introducing too many 73
over-counts. It maintains a separate count njfor each hash function φjdeﬁned asφj(s) =φ(s) 74
modpj, wherepjis a large prime number. For simplicity, we may assume that pj≈p∀jandφj75
assignssto any of 1,...,p with uniform probability. 76
We now derive the probability of over-counting. Let sbe a ﬁxed data sample (not necessarily 77
inserted yet) and suppose a dataset DofNsamples are inserted. We assume that pl/greatermuchN. Let 78
n:= min 1≤j≤lnj/parenleftbig
φj(s)/parenrightbig
be the count returned by the Bloom ﬁlter. We are interested in computing 79
Prob(n > 0|s /∈D). Due to assumptions about φj, we knownj(φ(s))∼Binomial/parenleftBig
N,1
p/parenrightBig
. 80
Therefore, 81
Prob(n>0|s /∈D) =Prob(n>0,s /∈D)
Prob(s/negationslash∈D)
=Prob(n>0)−Prob(s∈D)
Prob(s /∈D)
≈Prob(n>0)
Prob(s /∈D)
=/producttextl
j=1Prob(nj(φj(s))>0)
(1−1/pl)N
=(1−(1−1/p)N)l
(1−1/pl)N
≈(1−e−N/p)l
e−N/pl
≈(1−e−N/p)l.(1)
In particular, the probability of over-counting decays exponentially in l. We refer the readers to [ 3] 82
for other properties of the Count-Min sketch. 83
45 Robustness Analysis 84
5.1 Granularity 85
While our proposed method is able to achieve remarkable results without requiring much tuning, 86
the granularity of the hash function should be chosen wisely. Granularity plays a critical role in 87
count-based exploration, where the hash function should cluster states without under-generalizing 88
or over-generalizing. Table 4 summarizes granularity parameters for our hash functions. In Table 5 89
we summarize the performance of TRPO-pixel-SimHash under different granularities. We choose 90
Frostbite and Venture on which TRPO-pixel-SimHash outperforms the baseline, and choose as reward 91
bonus coefﬁcient β= 0.01×256
kto keep average bonus rewards at approximately the same scale. 92
k= 16 only corresponds to 65536 distinct hash codes, which is insufﬁcient to distinguish between 93
semantically distinct states and hence leads to worse performance. We observed that k= 512 tends 94
to capture trivial image details in Frostbite, leading the agent to believe that every state is new and 95
equally worth exploring. Similar results are observed while tuning the granularity parameters for 96
TRPO-BASS-SimHash and TRPO-AE-SimHash. 97
Table 4: Granularity parameters of various hash functions
SimHash k: size of the binary code
BASS C: cell size
B: number of bins for each color channel
AE k: downstream SimHash parameter
λ: binarization parameter
SmartHash s: grid size agent (x,y)coordinates
Table 5: Average score at 50 M time steps achieved by TRPO-pixel-SimHash
k 16 64 128 256 512
Frostbite 3326 4029 3932 4683 1117
Venture 0 218 142 263 306
The best granularity depends on both the hash function and the MDP. While adjusting granularity 98
parameter, we observed that it is important to lower the bonus coefﬁcient as granularity is increased. 99
This is because a higher granularity is likely to cause lower state counts, leading to higher bonus 100
rewards that may overwhelm the true rewards. 101
Apart from the experimental results shown in Table 1 in the main text and Table 5, additional 102
experiments have been performed to study several properties of our algorithm. 103
5.2 Hyperparameter sensitivity 104
To study the performance sensitivity to hyperparameter changes, we focus on evaluating TRPO- 105
RAM-SimHash on the Atari 2600 game Frostbite, where the method has a clear advantage over the 106
baseline. Because the ﬁnal scores can vary between different random seeds, we evaluated each set of 107
hyperparameters with 30 seeds. To reduce computation time and cost, RAM states are used instead 108
of image observations. 109
The results are summarized in Table 6. Herein, krefers to the length of the binary code for hashing 110
whileβis the multiplicative coefﬁcient for the reward bonus, as deﬁned in Section 2.2 of the main 111
text. This table demonstrates that most hyperparameter settings outperform the baseline ( β= 0) 112
signiﬁcantly. Moreover, the ﬁnal scores show a clear pattern in response to changing hyperparameters. 113
Smallβ-values lead to insufﬁcient exploration, while large β-values cause the bonus rewards to 114
overwhelm the true rewards. With a ﬁxed k, the scores are roughly concave in β, peaking at around 115
0.2. Higher granularity kleads to better performance. Therefore, it can be concluded that the 116
proposed exploration method is robust to hyperparameter changes in comparison to the baseline, and 117
that the best parameter settings can be obtained from a relatively coarse-grained grid search. 118
5Table 6: TRPO-RAM-SimHash performance robustness to hyperparameter changes on Frostbite
β
k 0 0.01 0.05 0.1 0.2 0.4 0.8 1.6
– 397 – – – – – – –
64 – 879 2464 2243 2489 1587 1107 441
128 – 1475 4248 2801 3239 3621 1543 395
256 – 2583 4497 4437 7849 3516 2260 374
Table 7: Average score at 50 M time steps achieved by TRPO-SmartHash on Montezuma’s Revenge
(RAM observations)
s 1 5 10 20 40 60
score 2598 2500 3533 3025 2500 1921
Table 8: Interpretation of particular RAM entries in Montezuma’s Revenge
ID Group Meaning
3 room room number
42 agentxcoordinate
43 agentycoordinate
52 agent orientation (left/right)
27 beams on/off
83 beams beam countdown (on: 0, off: 36→0)
0 counter counts from 0to255and repeats
55 counter death scene countdown
67 objects Doors, skull, and key in 1st room
47 skullxcoordinate (1st and 2nd room)
5.3 A Case Study of Montezuma’s Revenge 119
Montezuma’s Revenge is widely known for its extremely sparse rewards and difﬁcult exploration 120
[1]. While our method does not outperform [ 1] on this game, we investigate the reasons behind this 121
through various experiments. The experiment process below again demonstrates the importance of a 122
hash function having the correct granularity and encoding relevant information for solving the MDP. 123
Our ﬁrst attempt is to use game RAM states instead of image observations as inputs to the policy, 124
which leads to a game score of 2500 with TRPO-BASS-SimHash. Our second attempt is to manually 125
design a hash function that incorporates domain knowledge, called SmartHash , which uses an 126
integer-valued vector consisting of the agent’s (x,y)location, room number and other useful RAM 127
information as the hash code. The best SmartHash agent is able to obtain a score of 3500 . Still 128
the performance is not optimal. We observe that a slight change in the agent’s coordinates does 129
not always result in a semantically distinct state, and thus the hash code may remain unchanged. 130
Therefore we choose grid size sand replace the xcoordinate by⌊(x−xmin)/s⌋(similarly for y). 131
The bonus coefﬁcient is chosen as β= 0.01√sto maintain the scale relative to the true reward1(see 132
Table 7). Finally, the best agent is able to obtain 6600 total rewards after training for 1000 iterations 133
(1000 M time steps), with a grid size s= 10 . 134
1The bonus scaling is chosen by assuming all states are visited uniformly and the average bonus reward
should remain the same for any grid size.
6Table 9: Performance comparison between state counting (left of the slash) and state-action counting
(right of the slash) using TRPO-RAM-SimHash on Frostbite
β
k 0.01 0.05 0.1 0.2 0.4 0.8 1.6
64 879 / 976 2464 / 1491 2243 / 3954 2489 / 5523 1587 / 5985 1107 / 2052 441 / 742
128 1475 / 808 4248 / 4302 2801 / 4802 3239 / 7291 3621 / 4243 1543 / 1941 395 / 362
256 2583 / 1584 4497 / 5402 4437 / 5431 7849 / 4872 3516 / 3175 2260 / 1238 374 / 96
Table 8 lists the semantic interpretation of certain RAM entries in Montezuma’s Revenge. SmartHash, 135
as described in Section 5.3, makes use of RAM indices 3,42,43,27, and 67. “Beam walls” are 136
deadly barriers that occur periodically in some rooms. 137
During our pursuit, we had another interesting discovery that the ideal hash function should not 138
simply cluster states by their visual similarity, but instead by their relevance to solving the MDP. We 139
experimented with including enemy locations in the ﬁrst two rooms into SmartHash ( s= 10 ), and 140
observed that average score dropped to 1672 (at iteration 1000 ). Though it is important for the agent 141
to dodge enemies, the agent also erroneously “enjoys” watching enemy motions at distance (since 142
new states are constantly observed) and “forgets” that his main objective is to enter other rooms. An 143
alternative hash function keeps the same entry “enemy locations”, but instead only puts randomly 144
sampled values in it, which surprisingly achieves better performance ( 3112 ). However, by ignoring 145
enemy locations altogether, the agent achieves a much higher score ( 5661 ) (see Figure 4). In retrospect, 146
we examine the hash codes generated by BASS-SimHash and ﬁnd that codes clearly distinguish 147
between visually different states (including various enemy locations), but fails to emphasize that the 148
agent needs to explore different rooms. Again this example showcases the importance of encoding 149
relevant information in designing hash functions. 150
5.4 State and state-action counting 151
Continuing the results in Table 6, the performance of state-action counting is studied using the same 152
experimental setup, summarized in Table 9. In particular, a bonus reward r+(s,a) =β√
n(s,a)instead 153
ofr+(s) =β√
n(s)is assigned. These results show that the relative performance of state counting 154
compared to state-action counting depends highly on the selected hyperparameter settings. However, 155
we notice that the best performance is achieved using state counting with k= 256 andβ= 0.2. 156
References 157
[1]Bellemare, Marc G, Srinivasan, Sriram, Ostrovski, Georg, Schaul, Tom, Saxton, David, and 158
Munos, Remi. Unifying count-based exploration and intrinsic motivation. In Advances in Neural 159
Information Processing Systems 29 (NIPS) , pp. 1471–1479, 2016. 160
[2]Bloom, Burton H. Space/time trade-offs in hash coding with allowable errors. Communications 161
of the ACM , 13(7):422–426, 1970. 162
[3]Cormode, Graham and Muthukrishnan, S. An improved data stream summary: the count-min 163
sketch and its applications. Journal of Algorithms , 55(1):58–75, 2005. 164
[4]Duan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John, and Abbeel, Pieter. Benchmarking 165
deep reinforcement learning for continous control. In Proceedings of the 33rd International 166
Conference on Machine Learning (ICML) , pp. 1329–1338, 2016. 167
[5]Fan, Li, Cao, Pei, Almeida, Jussara, and Broder, Andrei Z. Summary cache: A scalable wide-area 168
web cache sharing protocol. IEEE/ACM Transactions on Networking , 8(3):281–293, 2000. 169
[6]Houthooft, Rein, Chen, Xi, Duan, Yan, Schulman, John, De Turck, Filip, and Abbeel, Pieter. 170
VIME: Variational information maximizing exploration. In Advances in Neural Information 171
Processing Systems 29 (NIPS) , pp. 1109–1117, 2016. 172
7[7]Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training 173
by reducing internal covariate shift. In Proceedings of the 32nd International Conference on 174
Machine Learning (ICML) , pp. 448–456, 2015. 175
[8]Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. In Proceedings 176
of the International Conference on Learning Representations (ICLR) , 2015. 177
8Figure 2: Freeway: subsequent frames and corresponding code (top); the frames are ordered from left
(starting with frame number 0) to right, top to bottom; the vertical axis in the right images correspond
to the frame number. Within each image, the left picture is the input frame, the middle picture the
reconstruction, and the right picture, the reconstruction error.
0 100 200 300 400 500−1000010002000300040005000600070008000
direct count
Bloom 6M
Bloom 90M
(a) Mean average undiscounted re-
turn
0 100 200 300 400 5000.0000.0020.0040.0060.0080.0100.012
direct count
Bloom 6M
Bloom 90M(b) Average bonus reward
Figure 3: Statistics of TRPO-pixel-SimHash ( k= 256 ) on Frostbite. Solid lines are the mean, while
the shaded areas represent the one standard deviation. Results are derived from 10 random seeds.
Direct counting with a dictionary uses 2.7 times more computations than counting Bloom ﬁlters ( 6 M
or90 M ).
90 200 400 600 800 1000−1000010002000300040005000600070008000
exact enemy locations
ignore enemies
random enemy locationsFigure 4: SmartHash results on Montezuma’s Revenge (RAM observations): the solid line is the
mean average undiscounted return per iteration, while the shaded areas represent the one standard
deviation, over 5seeds.
10