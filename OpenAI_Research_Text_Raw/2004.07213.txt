arXiv:2004.07213v2  [cs.CY]  20 Apr 2020Toward Trustworthy AI Development:
Mechanisms for Supporting Veriﬁable Claims∗
Miles Brundage1†, Shahar Avin3,2†, Jasmine Wang4,29†‡, Haydn Belﬁeld3,2†, Gretchen Krueger1†,
Gillian Hadﬁeld1,5,30, Heidy Khlaaf6, Jingying Yang7, Helen Toner8, Ruth Fong9,
Tegan Maharaj4,28, Pang Wei Koh10, Sara Hooker11, Jade Leung12, Andrew Trask9,
Emma Bluemke9, Jonathan Lebensold4,29, Cullen O’Keefe1, Mark Koren13, Théo Ryffel14,
JB Rubinovitz15, Tamay Besiroglu16, Federica Carugati17, Jack Clark1, Peter Eckersley7,
Sarah de Haas18, Maritza Johnson18, Ben Laurie18, Alex Ingerman18, Igor Krawczuk19,
Amanda Askell1, Rosario Cammarota20, Andrew Lohn21, David Krueger4,27, Charlotte Stix22,
Peter Henderson10, Logan Graham9, Carina Prunkl12, Bianca Martin1, Elizabeth Seger16,
Noa Zilberman9, Seán Ó hÉigeartaigh2,3, Frens Kroeger23, Girish Sastry1, Rebecca Kagan8,
Adrian Weller16,24, Brian Tse12,7, Elizabeth Barnes1, Allan Dafoe12,9, Paul Scharre25,
Ariel Herbert-Voss1, Martijn Rasser25, Shagun Sodhani4,27, Carrick Flynn8,
Thomas Krendl Gilbert26, Lisa Dyer7, Saif Khan8, Yoshua Bengio4,27, Markus Anderljung12
1OpenAI,2Leverhulme Centre for the Future of Intelligence,3Centre for the Study of Existential Risk,
4Mila,5University of Toronto,6Adelard,7Partnership on AI,8Center for Security and Emerging Technology ,
9University of Oxford,10Stanford University ,11Google Brain,12Future of Humanity Institute,
13Stanford Centre for AI Safety ,14École Normale Supérieure (Paris),15Remedy .AI,
16University of Cambridge,17Center for Advanced Study in the Behavioral Sciences,18Google Research,
19École Polytechnique Fédérale de Lausanne,20Intel,21RAND Corporation,
22Eindhoven University of Technology ,23Coventry University ,24Alan Turing Institute,
25Center for a New American Security ,26University of California, Berkeley ,
27University of Montreal,28Montreal Polytechnic,29McGill University ,
30Schwartz Reisman Institute for Technology and Society
April 2020
∗Listed authors are those who contributed substantive ideas and/or work to this report. Contributions include writing,
research, and/or review for one or more sections; some authors also contrib uted content via participation in an April 2019
workshop and/or via ongoing discussions. As such, with the exception of th e primary/corresponding authors, inclusion as
author does not imply endorsement of all aspects of the repor t.
†Miles Brundage (miles@openai.com), Shahar Avin (sa478@ca m.ac.uk), Jasmine Wang (jasminewang76@gmail.com),
Haydn Belﬁeld (hb492@cam.ac.uk), and Gretchen Krueger (gr etchen@openai.com) contributed equally and are correspon d-
ing authors. Other authors are listed roughly in order of con tribution.
‡Work conducted in part while at OpenAI.Contents
Executive Summary 1
List of Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1 Introduction 4
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Institutional, Software, and Hardware Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 Scope and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 Outline of the Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Institutional Mechanisms and Recommendations 8
2.1 Third Party Auditing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2 Red Team Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3 Bias and Safety Bounties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.4 Sharing of AI Incidents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3 Software Mechanisms and Recommendations 21
3.1 Audit Trails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.2 Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.3 Privacy-Preserving Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4 Hardware Mechanisms and Recommendations 31
4.1 Secure Hardware for Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.2 High-Precision Compute Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.3 Compute Support for Academia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
5 Conclusion 39
Acknowledgements 41
References 42
Appendices 60
I Workshop and Report Writing Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
II Key Terms and Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
III The Nature and Importance of Veriﬁable Claims . . . . . . . . . . . . . . . . . . . . . . . . . . 64
IV AI, Veriﬁcation, and Arms Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
V Cooperation and Antitrust Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
VI Supplemental Mechanism Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
A Formal Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
B Veriﬁable Data Policies in Distributed Computing Systems . . . . . . . . . . . . . . . 74
C Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76Executive Summary
Recent progress in artiﬁcial intelligence (AI) has enabled a diverse array of applications across commer-
cial, scientiﬁc, and creative domains. With this wave of app lications has come a growing awareness of
the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry
and academia are insufﬁcient to ensure responsible AI devel opment[1][2][3].
Steps have been taken by the AI community to acknowledge and a ddress this insufﬁciency , including
widespread adoption of ethics principles by researchers an d technology companies. However, ethics
principles are non-binding, and their translation to actio ns is often not obvious. Furthermore, those
outside a given organization are often ill-equipped to asse ss whether an AI developer’s actions are con-
sistent with their stated principles. Nor are they able to ho ld developers to account when principles and
behavior diverge, fueling accusations of "ethics washing" [4]. In order for AI developers to earn trust
from system users, customers, civil society , governments, and other stakeholders that they are building
AI responsibly , there is a need to move beyond principles to a focus on mechanisms for demonstrat-
ing responsible behavior [5]. Making and assessing veriﬁable claims, to which developer s can be held
accountable, is one crucial step in this direction.
With the ability to make precise claims for which evidence ca n be brought to bear, AI developers can more
readily demonstrate responsible behavior to regulators, t he public, and one another. Greater veriﬁability
of claims about AI development would help enable more effect ive oversight and reduce pressure to
cut corners for the sake of gaining a competitive edge [1]. Conversely , without the capacity to verify
claims made by AI developers, those using or affected by AI sy stems are more likely to be put at risk by
potentially ambiguous, misleading, or false claims.
This report suggests various steps that different stakehol ders in AI development can take to make it easier
to verify claims about AI development, with a focus on provid ing evidence about the safety , security ,
fairness, and privacy protection of AI systems. Implementa tion of such mechanisms can help make
progress on the multifaceted problem of ensuring that AI dev elopment is conducted in a trustworthy
fashion.1The mechanisms outlined in this report deal with questions t hat various parties involved in AI
development might face, such as:
•Can I (as a user) verify the claims made about the level of priv acy protection guaranteed by a new
AI system I’d like to use for machine translation of sensitiv e documents?
•Can I (as a regulator) trace the steps that led to an accident c aused by an autonomous vehicle?
Against what standards should an autonomous vehicle compan y’s safety claims be compared?
•Can I (as an academic) conduct impartial research on the impa cts associated with large-scale AI
systems when I lack the computing resources of industry?
•Can I (as an AI developer) verify that my competitors in a give n area of AI development will follow
best practices rather than cut corners to gain an advantage?
Even AI developers who have the desire and /or incentives to make concrete, veriﬁable claims may not
be equipped with the appropriate mechanisms to do so. The AI d evelopment community needs a ro-
bust "toolbox" of mechanisms to support the veriﬁcation of c laims about AI systems and development
processes.
1The capacity to verify claims made by developers, on its own, would be insufﬁcient to ensure responsible AI development.
Not all important claims admit veriﬁcation, and there is als o a need for oversight agencies such as governments and stand ards
organizations to align developers’ incentives with the pub lic interest.
1This problem framing led some of the authors of this report to hold a workshop in April 2019, aimed at
expanding the toolbox of mechanisms for making and assessin g veriﬁable claims about AI development.2
This report builds on the ideas proposed at that workshop. Th e mechanisms outlined do two things:
•They increase the options available to AI developers for sub stantiating claims they make about AI
systems’ properties.
•They increase the speciﬁcity and diversity of demands that c an be made of AI developers by other
stakeholders such as users, policymakers, and members of ci vil society .
Each mechanism and associated recommendation discussed in this report addresses a speciﬁc gap pre-
venting effective assessment of developers’ claims today . Some of these mechanisms exist and need to
be extended or scaled up in some way , and others are novel. The report is intended as an incremental
step toward improving the veriﬁability of claims about AI de velopment.
The report organizes mechanisms under the headings of Institutions ,Software , and Hardware , which are
three intertwined components of AI systems and development processes.
•Institutional Mechanisms : These mechanisms shape or clarify the incentives of people involved
in AI development and provide greater visibility into their behavior, including their efforts to en-
sure that AI systems are safe, secure, fair, and privacy-pre serving. Institutional mechanisms play a
foundational role in veriﬁable claims about AI development , since it is people who are ultimately
responsible for AI development. We focus on third party auditing , to create a robust alternative
to self-assessment of claims; red teaming exercises , to demonstrate AI developers’ attention to
the ways in which their systems could be misused; bias and safety bounties , to strengthen incen-
tives to discover and report ﬂaws in AI systems; and sharing of AI incidents , to improve societal
understanding of how AI systems can behave in unexpected or u ndesired ways.
•Software Mechanisms : These mechanisms enable greater understanding and oversi ght of speciﬁc
AI systems’ properties. We focus on audit trails , to enable accountability for high-stakes AI systems
by capturing critical information about the development an d deployment process; interpretabil-
ity, to foster understanding and scrutiny of AI systems’ charac teristics; and privacy-preserving
machine learning , to make developers’ commitments to privacy protection mor e robust.
•Hardware Mechanisms : Mechanisms related to computing hardware can play a key rol e in sub-
stantiating strong claims about privacy and security , enab ling transparency about how an orga-
nization’s resources are put to use, and inﬂuencing who has t he resources necessary to verify
different claims. We focus on secure hardware for machine learning , to increase the veriﬁabil-
ity of privacy and security claims; high-precision compute measurement , to improve the value
and comparability of claims about computing power usage; an dcompute support for academia ,
to improve the ability of those outside of industry to evalua te claims about large-scale AI systems.
Each mechanism provides additional paths to verifying AI de velopers’ commitments to responsible AI
development, and has the potential to contribute to a more tr ustworthy AI ecosystem. The full list of
recommendations associated with each mechanism is found on the following page and again at the end
of the report.
2SeeAppendix I, "Workshop and Report Writing Process."
2List of Recommendations
Institutional Mechanisms and Recommendations
1. A coalition of stakeholders should create a task force to r esearch options for conducting and fund-
ingthird party auditing of AI systems.
2. Organizations developing AI should run red teaming exercises to explore risks associated with
systems they develop, and should share best practices and to ols for doing so.
3. AI developers should pilot bias and safety bounties for AI systems to strengthen incentives and
processes for broad-based scrutiny of AI systems.
4. AI developers should share more information about AI incidents , including through collaborative
channels.
Software Mechanisms and Recommendations
5. Standards setting bodies should work with academia and in dustry to develop audit trail require-
ments for safety-critical applications of AI systems.
6. Organizations developing AI and funding bodies should su pport research into the interpretability
of AI systems, with a focus on supporting risk assessment and auditing.
7. AI developers should develop, share, and use suites of too ls for privacy-preserving machine
learning that include measures of performance against common standa rds.
Hardware Mechanisms and Recommendations
8. Industry and academia should work together to develop hardware security features for AI ac-
celerators or otherwise establish best practices for the us e of secure hardware (including secure
enclaves on commodity hardware) in machine learning contex ts.
9. One or more AI labs should estimate the computing power inv olved in a single project in great
detail ( high-precision compute measurement ), and report on the potential for wider adoption
of such methods.
10. Government funding bodies should substantially increa sefunding of computing power resources
for researchers in academia, in order to improve the ability of those researchers to verify claims
made by industry .
31 Introduction
1.1 Motivation
With rapid technical progress in artiﬁcial intelligence (A I)3and the spread of AI-based applications
over the past several years, there is growing concern about h ow to ensure that the development and
deployment of AI is beneﬁcial – and not detrimental – to human ity . In recent years, AI systems have
been developed in ways that are inconsistent with the stated values of those developing them. This
has led to a rise in concern, research, and activism relating to the impacts of AI systems [2][3]. AI
development has raised concerns about ampliﬁcation of bias [6], loss of privacy[7], digital addictions
[8], social harms associated with facial recognition and crimi nal risk assessment [9], disinformation
[10], and harmful changes to the quality [11]and availability of gainful employment [12].
In response to these concerns, a range of stakeholders, incl uding those developing AI systems, have
articulated ethics principles to guide responsible AI deve lopment. The amount of work undertaken to
articulate and debate such principles is encouraging, as is the convergence of many such principles on a
set of widely-shared concerns such as safety , security , fai rness, and privacy .4
However, principles are only a ﬁrst step in the effort to ensure beneﬁc ial societal outcomes from AI
[13]. Indeed, studies [17], surveys[18], and trends in worker and community organizing [2][3]make
clear that large swaths of the public are concerned about the risks of AI development, and do not trust
the organizations currently dominating such development t o self-regulate effectively . Those potentially
affected by AI systems need mechanisms for ensuring respons ible development that are more robust
than high-level principles. People who get on airplanes don ’t trust an airline manufacturer because of its
PR campaigns about the importance of safety - they trust it be cause of the accompanying infrastructure
of technologies, norms, laws, and institutions for ensurin g airline safety .5Similarly , along with the
growing explicit adoption of ethics principles to guide AI d evelopment, there is mounting skepticism
about whether these claims and commitments can be monitored and enforced[19].
Policymakers are beginning to enact regulations that more d irectly constrain AI developers’ behavior
[20]. We believe that analyzing AI development through the lens o f veriﬁable claims can help to inform
such efforts. AI developers, regulators, and other actors a ll need to understand which properties of AI
systems and development processes can be credibly demonstr ated, through what means, and with what
tradeoffs.
We deﬁne veriﬁable claims6as falsiﬁable statements for which evidence and arguments c an be brought
3We deﬁne AI as digital systems that are capable of performing tasks commonly thought to require intelligence, with these
tasks typically learned via data and /or experience.
4Note, however, that many such principles have been articula ted by Western academics and technology company employees,
and as such are not necessarily representative of humanity’ s interests or values as a whole. Further, they are amenable t o various
interpretations[13][14]and agreement on them can mask deeper disagreements [5]. See also Beijing AI Principles [15]and
Zeng et. al.[16]for examples of non-Western AI principles.
5Recent commercial airline crashes also serve as a reminder t hat even seemingly robust versions of such infrastructure a re
imperfect and in need of constant vigilance.
6While this report does discuss the technical area of formal v eriﬁcation at several points, and several of our recommenda tions
are based on best practices from the ﬁeld of information secu rity, the sense in which we use "veriﬁable" is distinct from h ow
the term is used in those contexts. Unless otherwise speciﬁe d by the use of the adjective "formal" or other context, this r eport
uses the word veriﬁcation in a looser sense. Formal veriﬁcat ion seeks mathematical proof that a certain technical claim is
4to bear on the likelihood of those claims being true. While th e degree of attainable certainty will vary
across different claims and contexts, we hope to show that gr eater degrees of evidence can be provided
for claims about AI development than is typical today . The na ture and importance of veriﬁable claims
is discussed in greater depth in Appendix III , and we turn next to considering the types of mechanisms
that can make claims veriﬁable.
1.2 Institutional, Software, and Hardware Mechanisms
AI developers today have many possible approaches for incre asing the veriﬁability of their claims. De-
spite the availability of many mechanisms that could help AI developers demonstrate their claims and
help other stakeholders scrutinize their claims, this tool box has not been well articulated to date.
We view AI development processes as sociotechnical systems ,7with institutions, software, and hardware
all potentially supporting (or detracting from) the veriﬁa bility of claims about AI development. AI de-
velopers can make claims about, or take actions related to, e ach of these three interrelated pillars of AI
development.
In some cases, adopting one of these mechanisms can increase the veriﬁability of one’s own claims,
whereas in other cases the impact on trust is more indirect (i .e., a mechanism implemented by one actor
enabling greater scrutiny of other actors). As such, collab oration across sectors and organizations will be
critical in order to build an ecosystem in which claims about responsible AI development can be veriﬁed.
•Institutional mechanisms largely pertain to values, incentives, and accountability . Institutional
mechanisms shape or clarify the incentives of people involv ed in AI development and provide
greater visibility into their behavior, including their ef forts to ensure that AI systems are safe, se-
cure, fair, and privacy-preserving. These mechanisms can a lso create or strengthen channels for
holding AI developers accountable for harms associated wit h AI development. In this report, we
provide an overview of some such mechanisms, and then discus sthird party auditing ,red team
exercises ,safety and bias bounties , and sharing of AI incidents in more detail.
•Software mechanisms largely pertain to speciﬁc AI systems and their properties . Software mecha-
nisms can be used to provide evidence for both formal and info rmal claims regarding the properties
of speciﬁc AI systems, enabling greater understanding and o versight. The software mechanisms
we highlight below are audit trails ,interpretability , and privacy-preserving machine learning .
•Hardware mechanisms largely pertain to physical computational resources and their properties .
Hardware mechanisms can support veriﬁable claims by provid ing greater assurance regarding the
privacy and security of AI systems, and can be used to substan tiate claims about how an organi-
zation is using their general-purpose computing capabilit ies. Further, the distribution of resources
across different actors can inﬂuence the types of AI systems that are developed and which ac-
tors are capable of assessing other actors’ claims (includi ng by reproducing them). The hardware
mechanisms we focus on in this report are hardware security features for machine learning ,
high-precision compute measurement , and computing power support for academia .
true with certainty (subject to certain assumptions). In co ntrast, this report largely focuses on claims that are unlik ely to be
demonstrated with absolute certainty, but which can be show n to be likely or unlikely to be true through relevant argumen ts
and evidence.
7Broadly, a sociotechnical system is one whose "core interfa ce consists of the relations between a nonhuman system and a
human system", rather than the components of those systems i n isolation. See Trist [21].
51.3 Scope and Limitations
This report focuses on a particular aspect of trustworthy AI development: the extent to which organiza-
tions developing AI systems can and do make veriﬁable claims about the AI systems they build, and the
ability of other parties to assess those claims. Given the ba ckgrounds of the authors, the report focuses
in particular on mechanisms for demonstrating claims about AI systems being safe, secure, fair, and /or
privacy-preserving, without implying that those are the on ly sorts of claims that need to be veriﬁed.
We devote particular attention to mechanisms8that the authors have expertise in and for which concrete
and beneﬁcial next steps were identiﬁed at an April 2019 work shop. These are not the only mechanisms
relevant to veriﬁable claims; we survey some others at the be ginning of each section, and expect that
further useful mechanisms have yet to be identiﬁed.
Making veriﬁable claims is part of, but not equivalent to, tr ustworthy AI development, broadly deﬁned.
An AI developer might also be more or less trustworthy based o n the particular values they espouse, the
extent to which they engage affected communities in their de cision-making, or the extent of recourse
that they provide to external parties who are affected by the ir actions. Additionally , the actions of AI
developers, which we focus on, are not all that matters for tr ustworthy AI development–the existence
and enforcement of relevant laws matters greatly , for examp le.
Appendix I discusses the reasons for the report’s scope in more detail, andAppendix II discusses the
relationship between different deﬁnitions of trust and ver iﬁable claims. When we use the term "trust"
as a verb in the report, we mean that one party (party A) gains c onﬁdence in the reliability of another
party’s claims (party B) based on evidence provided about th e accuracy of those claims or related ones.
We also make reference to this claim-oriented sense of trust when we discuss actors "earning" trust,
(providing evidence for claims made), or being "trustworth y" (routinely providing sufﬁcient evidence
for claims made). This use of language is intended to concise ly reference an important dimension of
trustworthy AI development, and is not meant to imply that ve riﬁable claims are sufﬁcient for attaining
trustworthy AI development.
1.4 Outline of the Report
The next three sections of the report, Institutional Mechanisms and Recommendations ,Software
Mechanisms and Recommendations , and Hardware Mechanisms and Recommendations , each be-
gin with a survey of mechanisms relevant to that category . Ea ch section then highlights several mech-
anisms that we consider especially promising. We are uncert ain which claims are most important to
verify in the context of AI development, but strongly suspec t that some combination of the mechanisms
we outline in this report are needed to craft an AI ecosystem i n which responsible AI development can
ﬂourish.
The way we articulate the case for each mechanism is problem-centric : each mechanism helps address
a potential barrier to claim veriﬁcation identiﬁed by the au thors. Depending on the case, the recom-
mendations associated with each mechanism are aimed at impl ementing a mechanism for the ﬁrst time,
researching it, scaling it up, or extending it in some way .
8We use the term mechanism generically to refer to processes, systems, or approaches for providing or generating evidenc e
about behavior.
6TheConclusion puts the report in context, discusses some important caveat s, and reﬂects on next steps.
TheAppendices provide important context, supporting material, and suppl emental analysis. Appendix
Iprovides background on the workshop and the process that wen t into writing the report; Appendix II
serves as a glossary and discussion of key terms used in the re port; Appendix III discusses the nature
and importance of veriﬁable claims; Appendix IV discusses the importance of veriﬁable claims in the
context of arms control; Appendix V provides context on antitrust law as it relates to cooperati on among
AI developers on responsible AI development; and Appendix VI offers supplemental analysis of several
mechanisms.
72 Institutional Mechanisms and Recommendations
"Institutional mechanisms" are processes that shape or cla rify the incentives of the people involved in
AI development, make their behavior more transparent, or en able accountability for their behavior.
Institutional mechanisms help to ensure that individuals o r organizations making claims regarding AI
development are incentivized to be diligent in developing A I responsibly and that other stakeholders can
verify that behavior. Institutions9can shape incentives or constrain behavior in various ways.
Several clusters of existing institutional mechanisms are relevant to responsible AI development, and
we characterize some of their roles and limitations below. T hese provide a foundation for the subse-
quent, more detailed discussion of several mechanisms and a ssociated recommendations. Speciﬁcally ,
we provide an overview of some existing institutional mecha nisms that have the following functions:
•Clarifying organizational goals and values;
•Increasing transparency regarding AI development process es;
•Creating incentives for developers to act in ways that are re sponsible; and
•Fostering exchange of information among developers.
Institutional mechanisms can help clarify an organization’s goals and values , which in turn can pro-
vide a basis for evaluating their claims. These statements o f goals and values–which can also be viewed
as (high level) claims in the framework discussed here–can h elp to contextualize the actions an orga-
nization takes and lay the foundation for others (sharehold ers, employees, civil society organizations,
governments, etc.) to monitor and evaluate behavior. Over 8 0 AI organizations [5], including technol-
ogy companies such as Google [22], OpenAI[23], and Microsoft[24]have publicly stated the principles
they will follow in developing AI. Codes of ethics or conduct are far from sufﬁcient, since they are typ-
ically abstracted away from particular cases and are not rel iably enforced, but they can be valuable by
establishing criteria that a developer concedes are approp riate for evaluating its behavior.
The creation and public announcement of a code of ethics proclaims an organization’s commitment to
ethical conduct both externally to the wider public, as well as internally to its employees, boards, and
shareholders. Codes of conduct differ from codes of ethics in that they contain a set of concr ete behavioral
standards.10
Institutional mechanisms can increase transparency regarding an organization’s AI d evelopment
processes in order to permit others to more easily verify compliance wi th appropriate norms, regula-
tions, or agreements. Improved transparency may reveal the extent to which actions taken by an AI
developer are consistent with their declared intentions an d goals. The more reliable, timely , and com-
plete the institutional measures to enhance transparency a re, the more assurance may be provided.
9Institutions may be formal and public institutions, such as: laws, courts , and regulatory agencies; private formal ar-
rangements between parties, such as contracts; interorgan izational structures such as industry associations, strat egic alliances,
partnerships, coalitions, joint ventures, and research co nsortia. Institutions may also be informal norms and practi ces that
prescribe behaviors in particular contexts; or third party organizations, such as professional bodies and academic in stitutions.
10Many organizations use the terms synonymously. The speciﬁc ity of codes of ethics can vary, and more speciﬁc (i.e., actio n-
guiding) codes of ethics (i.e. those equivalent to codes of c onduct) can be better for earning trust because they are more
falsiﬁable. Additionally, the form and content of these mec hanisms can evolve over time–consider, e.g., Google’s AI Pr inciples,
which have been incrementally supplemented with more concr ete guidance in particular areas.
8Transparency measures could be undertaken on a voluntary ba sis or as part of an agreed framework
involving relevant parties (such as a consortium of AI devel opers, interested non-proﬁts, or policymak-
ers). For example, algorithmic impact assessments are intended to support affected communities and
stakeholders in assessing AI and other automated decision s ystems[2]. The Canadian government, for
example, has centered AIAs in its Directive on Automated Dec ision-Making[25][26]. Another path
toward greater transparency around AI development involve s increasing the extent and quality of docu-
mentation for AI systems. Such documentation can help foster informed and safe use of AI systems by
providing information about AI systems’ biases and other at tributes[27][28][29].
Institutional mechanisms can create incentives for organizations to act in ways that are r esponsible .
Incentives can be created within an organization or externa lly , and they can operate at an organizational
or an individual level. The incentives facing an actor can pr ovide evidence regarding how that actor will
behave in the future, potentially bolstering the credibili ty of related claims. To modify incentives at an
organizational level, organizations can choose to adopt different organizational structures (such as beneﬁt
corporations) or take on legally binding intra-organizational commitments . For example, organizations
could credibly commit to distributing the beneﬁts of AI broa dly through a legal commitment that shifts
ﬁduciary duties.11
Institutional commitments to such steps could make a partic ular organization’s ﬁnancial incentives more
clearly aligned with the public interest. To the extent that commitments to responsible AI development
and distribution of beneﬁts are widely implemented, AI deve lopers would stand to beneﬁt from each
others’ success, potentially12reducing incentives to race against one another [1]. And critically , gov-
ernment regulations such as the General Data Protection Reg ulation (GDPR) enacted by the European
Union shift developer incentives by imposing penalties on d evelopers that do not adequately protect
privacy or provide recourse for algorithmic decision-maki ng.
Finally , institutional mechanisms can foster exchange of information between developers . To avoid
"races to the bottom" in AI development, AI developers can ex change lessons learned and demonstrate
their compliance with relevant norms to one another. Multil ateral fora (in addition to bilateral conversa-
tions between organizations) provide opportunities for di scussion and repeated interaction, increasing
transparency and interpersonal understanding. Voluntary membership organizations with stricter rules
and norms have been implemented in other industries and migh t also be a useful model for AI developers
[31].13
Steps in the direction of robust information exchange betwe en AI developers include the creation of
consensus around important priorities such as safety , secu rity , privacy , and fairness;14participation in
multi-stakeholder fora such as the Partnership on Artiﬁcia l Intelligence to Beneﬁt People and Society
(PAI), the Association for Computing Machinery (ACM), the I nstitute of Electrical and Electronics En-
gineers (IEEE), the International Telecommunications Uni on (ITU), and the International Standards
Organization (ISO); and clear identiﬁcation of roles or ofﬁ ces within organizations who are responsible
11The Windfall Clause [30]is one proposal along these lines, and involves an ex ante com mitment by AI ﬁrms to donate a
signiﬁcant amount of any eventual extremely large proﬁts.
12The global nature of AI development, and the national nature of much relevant regulation, is a key complicating factor.
13See for example the norms set and enforced by the European Tel ecommunications Standards Institute (ETSI). These norms
have real "teeth," such as the obligation for designated hol ders of Standard Essential Patents to license on Fair, Reaso nable and
Non-discriminatory (FRAND) terms. Breach of FRAND could gi ve rise to a breach of contract claim as well as constitute a
breach of antitrust law [32]. Voluntary standards for consumer products, such as those a ssociated with Fairtrade and Organic
labels, are also potentially relevant precedents [33].
14An example of such an effort is the Asilomar AI Principles [34].
9for maintaining and deepening interorganizational commun ication[10].15
It is also important to examine the incentives (and disincen tives) for free ﬂow of information within
an organization. Employees within organizations developi ng AI systems can play an important role in
identifying unethical or unsafe practices. For this to succ eed, employees must be well-informed about
the scope of AI development efforts within their organizati on and be comfortable raising their concerns,
and such concerns need to be taken seriously by management.16Policies (whether governmental or
organizational) that help ensure safe channels for express ing concerns are thus key foundations for
verifying claims about AI development being conducted resp onsibly .
The subsections below each introduce and explore a mechanis m with the potential for improving the
veriﬁability of claims in AI development: third party auditing ,red team exercises ,bias and safety
bounties , and sharing of AI incidents . In each case, the subsections below begin by discussing a
problem which motivates exploration of that mechanism, fol lowed by a recommendation for improving
or applying that mechanism.
15Though note competitors sharing commercially sensitive, n on-public information (such as strategic plans or R&D plans )
could raise antitrust concerns. It is therefore important t o have the right antitrust governance structures and proced ures in
place (i.e., setting out exactly what can and cannot be share d). See Appendix V .
16Recent revelations regarding the culture of engineering an d management at Boeing highlight the urgency of this issue
[35].
102.1 Third Party Auditing
Problem:
The process of AI development is often opaque to those outsid e a given organization, and
various barriers make it challenging for third parties to ve rify the claims being made by a
developer. As a result, claims about system attributes may n ot be easily veriﬁed.
AI developers have justiﬁable concerns about being transpa rent with information concerning commercial
secrets, personal information, or AI systems that could be m isused; however, problems arise when these
concerns incentivize them to evade scrutiny . Third party au ditors can be given privileged and secured
access to this private information, and they can be tasked wi th assessing whether safety , security , privacy ,
and fairness-related claims made by the AI developer are acc urate.
Auditing is a structured process by which an organization’s present or past behavior is assessed for
consistency with relevant principles, regulations, or nor ms. Auditing has promoted consistency and
accountability in industries outside of AI such as ﬁnance an d air travel. In each case, auditing is tailored
to the evolving nature of the industry in question.17Recently , auditing has gained traction as a potential
paradigm for assessing whether AI development was conducte d in a manner consistent with the stated
principles of an organization, with valuable work focused o n designing internal auditing processes (i.e.
those in which the auditors are also employed by the organiza tion being audited) [36].
Third party auditing is a form of auditing conducted by an ext ernal and independent auditor, rather
than the organization being audited, and can help address co ncerns about the incentives for accuracy
in self-reporting. Provided that they have sufﬁcient infor mation about the activities of an AI system, in-
dependent auditors with strong reputational and professio nal incentives for truthfulness can help verify
claims about AI development.
Auditing could take at least four quite different forms, and likely further variations are possible: auditing
by an independent body with government-backed policing and sanctioning power; auditing that occurs
entirely within the context of a government, though with mul tiple agencies involved [37]; auditing by
a private expert organization or some ensemble of such organ izations; and internal auditing followed
by public disclosure of (some subset of) the results.18As commonly occurs in other contexts, the re-
sults produced by independent auditors might be made public ly available, to increase conﬁdence in the
propriety of the auditing process.19
Techniques and best practices have not yet been established for auditing AI systems. Outside of AI,
however, there are well-developed frameworks on which to bu ild. Outcomes- or claim-based "assurance
frameworks" such as the Claims-Arguments-Evidence framew ork (CAE) and Goal Structuring Notation
(GSN) are already in wide use in safety-critical auditing co ntexts.20By allowing different types of ar-
guments and evidence to be used appropriately by auditors, t hese frameworks provide considerable
ﬂexibility in how high-level claims are substantiated, a ne eded feature given the wide ranging and fast-
17See Raji and Smart et al. [36]for a discussion of some lessons for AI from auditing in other industries.
18Model cards for model reporting [28]and data sheets for datasets [29]reveal information about AI systems publicly, and
future work in third party auditing could build on such tools , as advocated by Raji and Smart et al. [36].
19Consumer Reports, originally founded as the Consumers Unio n in 1936, is one model for an independent, third party
organization that performs similar functions for products that can affect the health, well-being, and safety of the peo ple using
those products. (https: //www.consumerreports.org /cro/about-us/what-we-do/research-and-testing /index.htm).
20SeeAppendix III for further discussion of claim-based frameworks for audit ing.
11evolving societal challenges posed by AI.
Possible aspects of AI systems that could be independently a udited include the level of privacy protection
guaranteed, the extent to (and methods by) which the AI syste ms were tested for safety , security or
ethical concerns, and the sources of data, labor, and other r esources used. Third party auditing could
be applicable to a wide range of AI applications, as well. Saf ety-critical AI systems such as autonomous
vehicles and medical AI systems, for example, could be audit ed for safety and security . Such audits
could conﬁrm or refute the accuracy of previous claims made b y developers, or compare their efforts
against an independent set of standards for safety and secur ity . As another example, search engines and
recommendation systems could be independently audited for harmful biases.
Third party auditors should be held accountable by governme nt, civil society , and other stakeholders
to ensure that strong incentives exist to act accurately and fairly . Reputational considerations help to
ensure auditing integrity in the case of ﬁnancial accountin g, where ﬁrms prefer to engage with credible
auditors[38]. Alternatively , a licensing system could be implemented in which auditors undergo a
standard training process in order to become a licensed AI sy stem auditor. However, given the variety
of methods and applications in the ﬁeld of AI, it is not obviou s whether auditor licensing is a feasible
option for the industry: perhaps a narrower form of licensin g would be helpful (e.g., a subset of AI such
as adversarial machine learning).
Auditing imposes costs (ﬁnancial and otherwise) that must b e weighed against its value. Even if auditing
is broadly societally beneﬁcial and non-ﬁnancial costs (e. g., to intellectual property) are managed, the
ﬁnancial costs will need to be borne by someone (auditees, la rge actors in the industry , taxpayers, etc.),
raising the question of how to initiate a self-sustaining pr ocess by which third party auditing could mature
and scale. However, if done well, third party auditing could strengthen the ability of stakeholders in the
AI ecosystem to make and assess veriﬁable claims. And notabl y , the insights gained from third party
auditing could be shared widely , potentially beneﬁting sta keholders even in countries with different
regulatory approaches for AI.
Recommendation: A coalition of stakeholders should create a task force to research options for
conducting and funding third party auditing of AI systems.
AI developers and other stakeholders (such as civil society organizations and policymakers) should col-
laboratively explore the challenges associated with third party auditing. A task force focused on this
issue could explore appropriate initial domains /applications to audit, devise approaches for handling
sensitive intellectual property , and balance the need for s tandardization with the need for ﬂexibility as
AI technology evolves.21Collaborative research into this domain seems especially p romising given that
the same auditing process could be used across labs and count ries. As research in these areas evolves, so
too will auditing processes–one might thus think of auditin g as a "meta-mechanism" which could involve
assessing the quality of other efforts discussed in this rep ort such as red teaming.
One way that third party auditing could connect to governmen t policies, and be funded, is via a "regu-
latory market"[42]. In a regulatory market for AI, a government would establish high-level outcomes
to be achieved from regulation of AI (e.g., achievement of a c ertain level of safety in an industry) and
then create or support private sector entities or other orga nizations that compete in order to design and
implement the precise technical oversight required to achi eve those outcomes.22Regardless of whether
such an approach is pursued, third party auditing by private actors should be viewed as a complement
21This list is not exhaustive - see, e.g., [39],[40], and[41]for related discussions.
22Examples of such entities include EXIDA, the UK Ofﬁce of Nucl ear Regulation, and the private company Adelard.
12to, rather than a substitute, for governmental regulation. And regardless of the entity conducting over-
sight of AI developers, in any case there will be a need to grap ple with difﬁcult challenges such as the
treatment of proprietary data.
132.2 Red Team Exercises
Problem:
It is difﬁcult for AI developers to address the "unknown unkn owns" associated with AI systems,
including limitations and risks that might be exploited by m alicious actors. Further, existing
red teaming approaches are insufﬁcient for addressing thes e concerns in the AI context.
In order for AI developers to make veriﬁable claims about the ir AI systems being safe or secure, they need
processes for surfacing and addressing potential safety an d security risks. Practices such as red teaming
exercises help organizations to discover their own limitat ions and vulnerabilities as well as those of the
AI systems they develop, and to approach them holistically , in a way that takes into account the larger
environment in which they are operating.23
A red team exercise is a structured effort to ﬁnd ﬂaws and vuln erabilities in a plan, organization, or
technical system, often performed by dedicated "red teams" that seek to adopt an attacker’s mindset
and methods. In domains such as computer security , red teams are routinely tasked with emulating
attackers in order to ﬁnd ﬂaws and vulnerabilities in organi zations and their systems. Discoveries made
by red teams allow organizations to improve security and sys tem integrity before and during deployment.
Knowledge that a lab has a red team can potentially improve th e trustworthiness of an organization with
respect to their safety and security claims, at least to the e xtent that effective red teaming practices exist
and are demonstrably employed.
As indicated by the number of cases in which AI systems cause o r threaten to cause harm, developers of an
AI system often fail to anticipate the potential risks assoc iated with technical systems they develop. These
risks include both inadvertent failures and deliberate mis use. Those not involved in the development
of a particular system may be able to more easily adopt and pra ctice an attacker’s skillset. A growing
number of industry labs have dedicated red teams, although b est practices for such efforts are generally
in their early stages.24There is a need for experimentation both within and across or ganizations in order
to move red teaming in AI forward, especially since few AI dev elopers have expertise in relevant areas
such as threat modeling and adversarial machine learning [44].
AI systems and infrastructure vary substantially in terms o f their properties and risks, making in-house
red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be
beneﬁcial to experiment with the formation of a community of AI red teaming professionals that draws
together individuals from different organizations and bac kgrounds, speciﬁcally focused on some subset
of AI (versus AI in general) that is relatively well-deﬁned a nd relevant across multiple organizations.25
A community of red teaming professionals could take actions such as publish best practices, collectively
analyze particular case studies, organize workshops on eme rging issues, or advocate for policies that
would enable red teaming to be more effective.
Doing red teaming in a more collaborative fashion, as a commu nity of focused professionals across
23Red teaming could be aimed at assessing various properties o f AI systems, though we focus on safety and security in this
subsection given the expertise of the authors who contribut ed to it.
24For an example of early efforts related to this, see Marshall et al., "Threat Modeling AI /ML Systems and Dependencies"
[43]
25In the context of language models, for example, 2019 saw a deg ree of communication and coordination across AI developers
to assess the relative risks of different language understa nding and generation systems [10]. Adversarial machine learning,
too, is an area with substantial sharing of lessons across or ganizations, though it is not obvious whether a shared red te am
focused on this would be too broad.
14organizations, has several potential beneﬁts:
•Participants in such a community would gain useful, broad kn owledge about the AI ecosystem,
allowing them to identify common attack vectors and make per iodic ecosystem-wide recommen-
dations to organizations that are not directly participati ng in the core community;
•Collaborative red teaming distributes the costs for such a t eam across AI developers, allowing those
who otherwise may not have utilized a red team of similarly hi gh quality or one at all to access its
beneﬁts (e.g., smaller organizations with less resources) ;
•Greater collaboration could facilitate sharing of informa tion about security-related AI incidents.26
Recommendation: Organizations developing AI should run re d teaming exercises to explore risks
associated with systems they develop, and should share best practices and tools for doing so.
Two critical questions that would need to be answered in the c ontext of forming a more cohesive AI
red teaming community are: what is the appropriate scope of s uch a group, and how will proprietary
information be handled?27The two questions are related. Particularly competitive co ntexts (e.g., au-
tonomous vehicles) might be simultaneously very appealing and challenging: multiple parties stand to
gain from pooling of insights, but collaborative red teamin g in such contexts is also challenging because
of intellectual property and security concerns.
As an alternative to or supplement to explicitly collaborat ive red teaming, organizations building AI
technologies should establish shared resources and outlet s for sharing relevant non-proprietary infor-
mation. The subsection on sharing of AI incidents also discusses some potential innovations that could
alleviate concerns around sharing proprietary informatio n.
26This has a precedent from cybersecurity; MITRE’s ATT&CK is a globally accessible knowledge base of adversary tactics an d
techniques based on real-world observations, which serves as a foundation for development of more speciﬁc threat model s and
methodologies to improve cybersecurity (https: //attack.mitre.org/).
27These practical questions are not exhaustive, and even addr essing them effectively might not sufﬁce to ensure that col-
laborative red teaming is beneﬁcial. For example, one poten tial failure mode is if collaborative red teaming fostered e xcessive
homogeneity in the red teaming approaches used, contributi ng to a false sense of security in cases where that approach is
insufﬁcient.
152.3 Bias and Safety Bounties
Problem:
There is too little incentive, and no formal process, for ind ividuals unafﬁliated with a particular
AI developer to seek out and report problems of AI bias and saf ety. As a result, broad-based
scrutiny of AI systems for these properties is relatively ra re.
"Bug bounty" programs have been popularized in the informat ion security industry as a way to compen-
sate individuals for recognizing and reporting bugs, espec ially those related to exploits and vulnerabil-
ities[45]. Bug bounties provide a legal and compelling way to report bu gs directly to the institutions
affected, rather than exposing the bugs publicly or selling the bugs to others. Typically , bug bounties
involve an articulation of the scale and severity of the bugs in order to determine appropriate compen-
sation.
While efforts such as red teaming are focused on bringing int ernal resources to bear on identifying risks
associated with AI systems, bounty programs give outside in dividuals a method for raising concerns
about speciﬁc AI systems in a formalized way . Bounties provi de one way to increase the amount of
scrutiny applied to AI systems, increasing the likelihood o f claims about those systems being veriﬁed or
refuted.
Bias28and safety bounties would extend the bug bounty concept to AI , and could complement existing
efforts to better document datasets and models for their per formance limitations and other properties.29
We focus here on bounties for discovering bias and safety iss ues in AI systems as a starting point for
analysis and experimentation, but note that bounties for ot her properties (such as security , privacy pro-
tection, or interpretability) could also be explored.30
While some instances of bias are easier to identify , others c an only be uncovered with signiﬁcant analysis
and resources. For example, Ziad Obermeyer et al. uncovered racial bias in a widely used algorithm
affecting millions of patients [47]. There have also been several instances of consumers with no direct
access to AI institutions using social media and the press to draw attention to problems with AI [48]. To
date, investigative journalists and civil society organiz ations have played key roles in surfacing different
biases in deployed AI systems. If companies were more open ea rlier in the development process about
possible faults, and if users were able to raise (and be compe nsated for raising) concerns about AI to
institutions, users might report them directly instead of s eeking recourse in the court of public opinion.31
In addition to bias, bounties could also add value in the cont ext of claims about AI safety . Algorithms or
models that are purported to have favorable safety properti es, such as enabling safe exploration or ro-
bustness to distributional shifts [49], could be scrutinized via bounty programs. To date, more att ention
has been paid to documentation of models for bias properties than safety properties,32though in both
28For an earlier exploration of bias bounties by one of the repo rt authors, see Rubinovitz [46].
29For example, model cards for model reporting [28]and datasheets for datasets [29]are recently developed means of
documenting AI releases, and such documentation could be ex tended with publicly listed incentives for ﬁnding new forms of
problematic behavior not captured in that documentation.
30Bounties for ﬁnding issues with datasets used for training A I systems could also be considered, though we focus on traine d
AI systems and code as starting points.
31We note that many millions of dollars have been paid to date vi a bug bounty programs in the computer security domain,
providing some evidence for this hypothesis. However, bug b ounties are not a panacea and recourse to the public is also
appropriate in some cases.
32We also note that the challenge of avoiding harmful biases is sometimes framed as a subset of safety, though for the
16cases, benchmarks remain in an early state. Improved safety metrics could increase the comparability
of bounty programs and the overall robustness of the bounty e cosystem; however, there should also be
means of reporting issues that are not well captured by exist ing metrics.
Note that bounties are not sufﬁcient for ensuring that a syst em is safe, secure, or fair, and it is important
to avoid creating perverse incentives (e.g., encouraging w ork on poorly-speciﬁed bounties and thereby
negatively affecting talent pipelines) [50]. Some system properties can be difﬁcult to discover even wit h
bounties, and the bounty hunting community might be too smal l to create strong assurances. However,
relative to the status quo, bounties might increase the amou nt of scrutiny applied to AI systems.
Recommendation: AI developers should pilot bias and safety bounties for AI systems to strengthen
incentives and processes for broad-based scrutiny of AI sys tems.
Issues to be addressed in setting up such a bounty program inc lude[46]:
•Setting compensation rates for different scales /severities of issues discovered;
•Determining processes for soliciting and evaluating bount y submissions;
•Developing processes for disclosing issues discovered via such bounties in a timely fashion;33
•Designing appropriate interfaces for reporting of bias and safety problems in the context of de-
ployed AI systems;
•Deﬁning processes for handling reported bugs and deploying ﬁxes;
•Avoiding creation of perverse incentives.
There is not a perfect analogy between discovering and addre ssing traditional computer security vul-
nerabilities, on the one hand, and identifying and addressi ng limitations in AI systems, on the other.
Work is thus needed to explore the factors listed above in ord er to adapt the bug bounty concept to
the context of AI development. The computer security commun ity has developed norms (though not a
consensus) regarding how to address "zero day" vulnerabili ties,34but no comparable norms yet exist in
the AI community .
There may be a need for distinct approaches to different type s of vulnerabilities and associated bounties,
depending on factors such as the potential for remediation o f the issue and the stakes associated with the
AI system. Bias might be treated differently from safety iss ues such as unsafe exploration, as these have
distinct causes, risks, and remediation steps. In some cont exts, a bounty might be paid for information
even if there is no ready ﬁx to the identiﬁed issue, because pr oviding accurate documentation to system
users is valuable in and of itself and there is often no preten se of AI systems being fully robust. In other
purposes of this discussion, little hinges on this terminol ogical issue. We distinguish the two in the title of this sect ion in order
to call attention to the unique properties of different type s of bounties.
33Note that we speciﬁcally consider public bounty programs he re, though instances of private bounty programs also exist
in the computer security community. Even in the event of a pub licly advertised bounty, however, submissions may be priva te,
and as such there is a need for explicit policies for handling submissions in a timely and legitimate fashion–otherwise s uch
programs will provide little assurance.
34A zero-day vulnerability is a security vulnerability that i s unknown to the developers of the system and other affected
parties, giving them "zero days" to mitigate the issue if the vulnerability were to immediately become widely known. The
computer security community features a range of views on app ropriate responses to zero-days, with a common approach bei ng
to provide a ﬁnite period for the vendor to respond to notiﬁca tion of the vulnerability before the discoverer goes public .
17cases, more care will be needed in responding to the identiﬁe d issue, such as when a model is widely
used in deployed products and services.
182.4 Sharing of AI Incidents
Problem:
Claims about AI systems can be scrutinized more effectively if there is common knowledge of
the potential risks of such systems. However, cases of desir ed or unexpected behavior by AI
systems are infrequently shared since it is costly to do unil aterally.
Organizations can share AI "incidents," or cases of undesir ed or unexpected behavior by an AI system
that causes or could cause harm, by publishing case studies a bout these incidents from which others can
learn. This can be accompanied by information about how they have worked to prevent future incidents
based on their own and others’ experiences.
By default, organizations developing AI have an incentive t o primarily or exclusively report positive
outcomes associated with their work rather than incidents. As a result, a skewed image is given to the
public, regulators, and users about the potential risks ass ociated with AI development.
The sharing of AI incidents can improve the veriﬁability of c laims in AI development by highlighting
risks that might not have otherwise been considered by certa in actors. Knowledge of these risks, in turn,
can then be used to inform questions posed to AI developers, i ncreasing the effectiveness of external
scrutiny . Incident sharing can also (over time, if used regu larly) provide evidence that incidents are
found and acknowledged by particular organizations, thoug h additional mechanisms would be needed
to demonstrate the completeness of such sharing.
AI incidents can include those that are publicly known and tr ansparent, publicly known and anonymized,
privately known and anonymized, or privately known and tran sparent. The Partnership on AI has begun
building an AI incident-sharing database, called the AI Inc ident Database.35The pilot was built using
publicly available information through a set of volunteers and contractors manually collecting known AI
incidents where AI caused harm in the real world.
Improving the ability and incentive of AI developers to repo rt incidents requires building additional
infrastructure, analogous to the infrastructure that exis ts for reporting incidents in other domains such
as cybersecurity . Infrastructure to support incident shar ing that involves non-public information would
require the following resources:
•Transparent and robust processes to protect organizations from undue reputational harm brought
about by the publication of previously unshared incidents. This could be achieved by anonymizing
incident information to protect the identity of the organiz ation sharing it. Other information-
sharing methods should be explored that would mitigate repu tational risk to organizations, while
preserving the usefulness of information shared;
•A trusted neutral third party that works with each organizat ion under a non-disclosure agreement
to collect and anonymize private information;
35See Partnership on AI’s AI Incident Registry as an example (h ttp://aiid.partnershiponai.org /). A related resource is a list
called Awful AI, which is intended to raise awareness of misu ses of AI and to spur discussion around contestational resea rch
and tech projects [51]. A separate list summarizes various cases in which AI system s "gamed" their speciﬁcations in unexpected
ways[52]. Additionally, AI developers have in some cases provided re trospective analyses of particular AI incidents, such as
with Microsoft’s "Tay" chatbot [53].
19•An organization that maintains and administers an online pl atform where users can easily access
the incident database, including strong encryption and pas sword protection for private incidents
as well as a way to submit new information. This organization would not have to be the same as
the third party that collects and anonymizes private incide nt data;
•Resources and channels to publicize the existence of this da tabase as a centralized resource, to ac-
celerate both contributions to the database and positive us es of the knowledge from the database;
and
•Dedicated researchers who monitor incidents in the databas e in order to identify patterns and
shareable lessons.
The costs of incident sharing (e.g., public relations risks ) are concentrated on the sharing organiza-
tion, although the beneﬁts are shared broadly by those who ga in valuable information about AI inci-
dents. Thus, a cooperative approach needs to be taken for inc ident sharing that addresses the potential
downsides. A more robust infrastructure for incident shari ng (as outlined above), including options
for anonymized reporting, would help ensure that fear of neg ative repercussions from sharing does not
prevent the beneﬁts of such sharing from being realized.36
Recommendation: AI developers should share more informati on about AI incidents, including
through collaborative channels.
Developers should seek to share AI incidents with a broad aud ience so as to maximize their usefulness,
and take advantage of collaborative channels such as centra lized incident databases as that infrastructure
matures. In addition, they should move towards publicizing their commitment to (and procedures for)
doing such sharing in a routine way rather than in an ad-hoc fa shion, in order to strengthen these
practices as norms within the AI development community .
Incident sharing is closely related to but distinct from res ponsible publication practices in AI and coor-
dinated disclosure of cybersecurity vulnerabilities [55]. Beyond implementation of progressively more
robust platforms for incident sharing and contributions to such platforms, future work could also explore
connections between AI and other domains in more detail, and identify key lessons from other domains
in which incident sharing is more mature (such as the nuclear and cybersecurity industries).
Over the longer term, lessons learned from experimentation and research could crystallize into a mature
body of knowledge on different types of AI incidents, report ing processes, and the costs associated with
incident sharing. This, in turn, can inform any eventual gov ernment efforts to require or incentivize
certain forms of incident reporting.
36We do not mean to claim that building and using such infrastru cture would be sufﬁcient to ensure that AI incidents are
addressed effectively. Sharing is only one part of the puzzl e for effectively managing incidents. For example, attenti on should
also be paid to ways in which organizations developing AI, an d particularly safety-critical AI, can become "high reliab ility
organizations" (see, e.g., [54]).
203 Software Mechanisms and Recommendations
Software mechanisms involve shaping and revealing the func tionality of existing AI systems. They can
support veriﬁcation of new types of claims or verify existin g claims with higher conﬁdence. This section
begins with an overview of the landscape of software mechani sms relevant to verifying claims, and then
highlights several key problems, mechanisms, and associat ed recommendations.
Software mechanisms, like software itself, must be underst ood in context (with an appreciation for the
role of the people involved). Expertise about many software mechanisms is not widespread, which
can create challenges for building trust through such mecha nisms. For example, an AI developer that
wants to provide evidence for the claim that "user data is kep t private" can help build trust in the lab’s
compliance with a a formal framework such as differential pr ivacy , but non-experts may have in mind
a different deﬁnition of privacy .37It is thus critical to consider not only which claims can and c an’t be
substantiated with existing mechanisms in theory , but also who is well-positioned to scrutinize these
mechanisms in practice.38
Keeping their limitations in mind, software mechanisms can substantiate claims associated with AI de-
velopment in various ways that are complementary to institu tional and hardware mechanisms. They
can allow researchers, auditors, and others to understand t heinternal workings of any given system.
They can also help characterize the behavioral proﬁle of a system over a domain of expected usage .
Software mechanisms could support claims such as:
•This system is robust to ’natural’ distributional shifts [49][56];
•This system is robust even to adversarial examples [57][58];
•This system has a well-characterized error surface and user s have been informed of contexts in
which the system would be unsafe to use;
•This system’s decisions exhibit statistical parity with re spect to sensitive demographic attributes39;
and
•This system provides repeatable or reproducible results.
Below, we summarize several clusters of mechanisms which he lp to substantiate some of the claims
above.
Reproducibility of technical results in AI is a key way of enabling veriﬁcatio n of claims about system
37For example, consider a desideratum for privacy: access to a dataset should not enable an adversary to learn anything
about an individual that could not be learned without access to the database. Differential privacy as originally concei ved does
not guarantee this–rather, it guarantees (to an extent dete rmined by a privacy budget) that one cannot learn whether tha t
individual was in the database in question.
38InSection 3.3 , we discuss the role that computing power–in addition to exp ertise–can play in inﬂuencing who can verify
which claims.
39Conceptions of, and measures for, fairness in machine learn ing, philosophy, law, and beyond vary widely. See, e.g., Xia ng
and Raji[59]and Binns[60].
21properties, and a number of ongoing initiatives are aimed at improving reproducibility in AI.4041Publi-
cation of results, models, and code increase the ability of o utside parties (especially technical experts)
to verify claims made about AI systems. Careful experimenta l design and the use of (and contribution
to) standard software libraries can also improve reproduci bility of particular results.42
Formal veriﬁcation establishes whether a system satisﬁes some requirements us ing the formal methods
of mathematics. Formal veriﬁcation is often a compulsory te chnique deployed in various safety-critical
domains to provide guarantees regarding the functional beh aviors of a system. These are typically guar-
antees that testing cannot provide. Until recently , AI syst ems utilizing machine learning (ML)43have
not generally been subjected to such rigor, but the increasi ng use of ML in safety-critical domains, such
as automated transport and robotics, necessitates the crea tion of novel formal analysis techniques ad-
dressing ML models and their accompanying non-ML component s. Techniques for formally verifying ML
models are still in their infancy and face numerous challeng es,44which we discuss in Appendix VI(A) .
The empirical veriﬁcation and validation of machine learni ng by machine learning has been pro-
posed as an alternative paradigm to formal veriﬁcation. Not ably , it can be more practical than formal
veriﬁcation, but since it operates empirically , the method cannot as fully guarantee its claims. Machine
learning could be used to search for common error patterns in another system’s code, or be used to
create simulation environments to adversarially ﬁnd fault s in an AI system’s behavior.
For example, adaptive stress testing (AST) of an AI system al lows users to ﬁnd the most likely failure of a
system for a given scenario using reinforcement learning [61], and is being used by to validate the next
generation of aircraft collision avoidance software [62]. Techniques requiring further research include
using machine learning to evaluate another machine learnin g system (either by directly inspecting its
policy or by creating environments to test the model) and usi ng ML to evaluate the input of another
machine learning model. In the future, data from model failu res, especially pooled across multiple labs
and stakeholders, could potentially be used to create class iﬁers that detect suspicious or anomalous AI
behavior.
Practical veriﬁcation is the use of scientiﬁc protocols to characterize a model’s d ata, assumptions, and
performance. Training data can be rigorously evaluated for representativeness [63][64]; assumptions
can be characterized by evaluating modular components of an AI model and by clearly communicating
output uncertainties; and performance can be characterize d by measuring generalization, fairness, and
performance heterogeneity across population subsets. Cau ses of differences in performance between
40We note the distinction between narrow senses of reproducib ility that focus on discrete technical results being reprod ucible
given the same initial conditions, sometimes referred to as repeatability, and broader senses of reproducibility that involve
reported performance gains carrying over to different cont exts and implementations.
41One way to promote robustness is through incentivizing repr oducibility of reported results. There are increas-
ing effort to award systems the recognition that they are rob ust, e.g., through ACM’s artifact evaluation badges
https://www.acm.org/publications/policies/artifact-review-badging. Conferences are also introduci ng artifact evaluation,
e.g., in the intersection between computer systems researc h and ML. See, e.g., https: //reproindex.com/event/repro-
sml2020 and http: //cknowledge.org/request.html The Reproducibility Challenge is another not able effort in this area:
https://reproducibility-challenge.github.io /neurips2019/
42In the following section on hardware mechanisms, we also dis cuss how reproducibility can be advanced in part by leveling
the playing ﬁeld between industry and other sectors with res pect to computing power.
43Machine learning is a subﬁeld of AI focused on the design of so ftware that improves in response to data, with that data
taking the form of unlabeled data, labeled data, or experien ce. While other forms of AI that do not involve machine learni ng
can still raise privacy concerns, we focus on machine learni ng here given the recent growth in associated privacy techni ques
as well as the widespread deployment of machine learning.
44Research into perception-based properties such as pointwi se robustness, for example, are not sufﬁciently comprehens ive
to be applied to real-time critical AI systems such as autono mous vehicles.
22models could be robustly attributed via randomized control led trials.
A developer may wish to make claims about a system’s adversarial robustness .45Currently , the security
balance is tilted in favor of attacks rather than defenses, w ith only adversarial training [65]having
stood the test of multiple years of attack research. Certiﬁcates of robustness , based on formal proofs, are
typically approximate and give meaningful bounds of the inc rease in error for only a limited range of
inputs, and often only around the data available for certiﬁc ation (i.e. not generalizing well to unseen
data[66][67][68]). Without approximation, certiﬁcates are computationall y prohibitive for all but the
smallest real world tasks [69]. Further, research is needed on scaling formal certiﬁcatio n methods to
larger model sizes.
The subsections below discuss software mechanisms that we c onsider especially important to advance
further. In particular, we discuss audit trails ,interpretability , and privacy-preserving machine learn-
ing.
45Adversarial robustness refers to an AI system’s ability to p erform well in the context of (i.e. to be robust against) "adv er-
sarial" inputs, or inputs designed speciﬁcally to degrade t he system’s performance.
233.1 Audit Trails
Problem:
AI systems lack traceable logs of steps taken in problem-deﬁ nition, design, development, and
operation, leading to a lack of accountability for subseque nt claims about those systems’ prop-
erties and impacts.
Audit trails can improve the veriﬁability of claims about en gineered systems, although they are not yet a
mature mechanism in the context of AI. An audit trail is a trac eable log of steps in system operation, and
potentially also in design and testing. We expect that audit trails will grow in importance as AI is applied
to more safety-critical contexts. They will be crucial in su pporting many institutional trust-building
mechanisms, such as third-party auditors, government regu latory bodies,46and voluntary disclosure of
safety-relevant information by companies.
Audit trails could cover all steps of the AI development proc ess, from the institutional work of problem
and purpose deﬁnition leading up to the initial creation of a system, to the training and development of
that system, all the way to retrospective accident analysis .
There is already strong precedence for audit trails in numer ous industries, in particular for safety-critical
systems. Commercial aircraft, for example, are equipped wi th ﬂight data recorders that record and cap-
ture multiple types of data each second [70]. In safety-critical domains, the compliance of such eviden ce
is usually assessed within a larger "assurance case" utilis ing the CAE or Goal-Structuring-Notation (GSN)
frameworks.47Tools such as the Assurance and Safety Case Environment (ACS E) exist to help both the
auditor and the auditee manage compliance claims and corres ponding evidence. Version control tools
such as GitHub or GitLab can be utilized to demonstrate indiv idual document traceability . Proposed
projects like Veriﬁable Data Audit [71]could establish conﬁdence in logs of data interactions and u sage.
Recommendation: Standards setting bodies should work with academia and industry to develop
audit trail requirements for safety-critical application s of AI systems.
Organizations involved in setting technical standards–in cluding governments and private actors–should
establish clear guidance regarding how to make safety-crit ical AI systems fully auditable.48Although
application dependent, software audit trails often requir e a base set of traceability49trails to be demon-
strated for qualiﬁcation;50the decision to choose a certain set of trails requires consi dering trade-offs
about efﬁciency , completeness, tamperprooﬁng, and other d esign considerations. There is ﬂexibility in
the type of documents or evidence the auditee presents to sat isfy these general traceability requirements
46Such as the National Transportation Safety Board with regar ds to autonomous vehicle trafﬁc accidents.
47SeeAppendix III for discussion of assurance cases and related frameworks.
48Others have argued for the importance of audit trails for AI e lsewhere, sometimes under the banner of "logging." See, e.g .,
[72].
49Traceability in this context refers to "the ability to verif y the history, location, or application of an item by means of doc-
umented recorded identiﬁcation," https: //en.wikipedia.org /wiki/Traceability, where the item in question is digital in natur e,
and might relate to various aspects of an AI system’s develop ment and deployment process.
50This includes traceability: between the system safety requ irements and the software safety requirements, between the
software safety requirements speciﬁcation and software ar chitecture, between the software safety requirements spec iﬁcation
and software design, between the software design speciﬁcat ion and the module and integration test speciﬁcations, betw een
the system and software design requirements for hardware /software integration and the hardware /software integration test
speciﬁcations, between the software safety requirements s peciﬁcation and the software safety validation plan, and be tween
the software design speciﬁcation and the software veriﬁcat ion (including data veriﬁcation) plan.
24(e.g., between test logs and requirement documents, veriﬁc ation and validation activities, etc.).51
Existing standards often deﬁne in detail the required audit trails for speciﬁc applications. For example,
IEC 61508 is a basic functional safety standard required by m any industries, including nuclear power.
Such standards are not yet established for AI systems. A wide array of audit trails related to an AI
development process can already be produced, such as code ch anges, logs of training runs, all outputs
of a model, etc. Inspiration might be taken from recent work o n internal algorithmic auditing [36]and
ongoing work on the documentation of AI systems more general ly , such as the ABOUT ML project [27].
Importantly , we recommend that in order to have maximal impa ct, any standards for AI audit trails
should be published freely , rather than requiring payment a s is often the case.
51SeeAppendix III .
253.2 Interpretability
Problem:
It’s difﬁcult to verify claims about "black-box" AI systems that make predictions without ex-
planations or visibility into their inner workings. This pr oblem is compounded by a lack of
consensus on what interpretability means.
Despite remarkable performance on a variety of problems, AI systems are frequently termed "black
boxes" due to the perceived difﬁculty of understanding and a nticipating their behavior. This lack of
interpretability in AI systems has raised concerns about using AI models in hig h stakes decision-making
contexts where human welfare may be compromised [73]. Having a better understanding of how the in-
ternal processes within these systems work can help proacti vely anticipate points of failure, audit model
behavior, and inspire approaches for new systems.
Research in model interpretability is aimed at helping to un derstand how and why a particular model
works. A precise, technical deﬁnition for interpretabilit y is elusive; by nature, the deﬁnition is subject
to the inquirer. Characterizing desiderata for interpreta ble models is a helpful way to formalize inter-
pretability[74][75]. Useful interpretability tools for building trust are also highly dependent on the
target user and the downstream task. For example, a model dev eloper or regulator may be more inter-
ested in understanding model behavior over the entire input distribution whereas a novice layperson
may wish to understand why the model made a particular predic tion for their individual case.52
Crucially , an "interpretable" model may not be necessary fo r all situations. The weight we place upon a
model being interpretable may depend upon a few different fa ctors, for example:
•More emphasis in sensitive domains (e.g., autonomous drivi ng or healthcare,53where an incor-
rect prediction adversely impacts human welfare) or when it is important for end-users to have
actionable recourse (e.g., bank loans) [77];
•Less emphasis given historical performance data (e.g., a mo del with sufﬁcient historical perfor-
mance may be used even if it’s not interpretable); and
•Less emphasis if improving interpretability incurs other c osts (e.g., compromising privacy).
In the longer term, for sensitive domains where human rights and/or welfare can be harmed, we antic-
ipate that interpretability will be a key component of AI sys tem audits, and that certain applications of
AI will be gated on the success of providing adequate intuiti on to auditors about the model behavior.
This is already the case in regulated domains such as ﬁnance [78].54
An ascendent topic of research is how to compare the relative merits of different interpretability methods
in a sensible way . Two criteria appear to be crucial: a. The method should provide sufﬁcient insight for
52While deﬁnitions in this area are contested, some would dist inguish between "interpretability" and "explainability" as
categories for these two directions, respectively.
53See, e.g., Sendak et. al. [76]which focuses on building trust in a hospital context, and co ntextualizes the role of inter-
pretability in this process.
54In New York, an investigation is ongoing into apparent gende r discrimination associated with the Apple Card’s credit li ne
allowances. This case illustrates the interplay of (a lack o f) interpretability and the potential harms associated wit h automated
decision-making systems [79].
26the end-user to understand how the model is making its predic tions (e.g., to assess if it aligns with
human judgment), and b. the interpretable explanation should be faithful to the mod el, i.e., accurately
reﬂect its underlying behavior.
Work on evaluating a. , while limited in treatment, has primarily centered on comp aring methods using
human surveys[80]. More work at the intersection of human-computer interacti on, cognitive science,
and interpretability research–e.g., studying the efﬁcacy of interpretability tools or exploring possible
interfaces–would be welcome, as would further exploration of how practitioners currently use such
tools[81][82][83][78][84].
Evaluating b. , the reliability of existing methods is an active area of res earch[85][86][87][88][89]
[90][91][92][93]. This effort is complicated by the lack of ground truth on sys tem behavior (if we
could reliably anticipate model behavior under all circums tances, we would not need an interpretability
method). The wide use of interpretable tools in sensitive do mains underscores the continued need to
develop benchmarks that assess the reliability of produced model explanations.
It is important that techniques developed under the umbrell a of interpretability not be used to provide
clear explanations when such clarity is not feasible. Witho ut sufﬁcient rigor, interpretability could be
used in service of unjustiﬁed trust by providing misleading explanations for system behavior. In identi-
fying, carrying out, and /or funding research on interpretability , particular atten tion should be paid to
whether and how such research might eventually aid in verify ing claims about AI systems with high
degrees of conﬁdence to support risk assessment and auditin g.
Recommendation: Organizations developing AI and funding b odies should support research into
the interpretability of AI systems, with a focus on supporti ng risk assessment and auditing.
Some areas of interpretability research are more developed than others. For example, attribution meth-
ods for explaining individual predictions of computer visi on models are arguably one of the most well-
developed research areas. As such, we suggest that the follo wing under-explored directions would be
useful for the development of interpretability tools that c ould support veriﬁable claims about system
properties:
•Developing and establishing consensus on the criteria, obj ectives, and frameworks for interpretabil-
ity research;
•Studying the provenance of a learned model (e.g., as a functi on of the distribution of training data,
choice of particular model families, or optimization) inst ead of treating models as ﬁxed; and
•Constraining models to be interpretable by default, in cont rast to the standard setting of trying to
interpret a model post-hoc.
This list is not intended to be exhaustive, and we recognize t hat there is uncertainty about which research
directions will ultimately bear fruit. We discuss the lands cape of interpretability research further in
Appendix VI(C) .
273.3 Privacy-Preserving Machine Learning
Problem:
A range of methods can potentially be used to veriﬁably safeg uard the data and models involved
in AI development. However, standards are lacking for evalu ating new privacy-preserving ma-
chine learning techniques, and the ability to implement the m currently lies outside a typical AI
developer’s skill set.
Training datasets for AI often include sensitive informati on about people, raising risks of privacy viola-
tion. These risks include unacceptable access to raw data (e .g., in the case of an untrusted employee or a
data breach), unacceptable inference from a trained model ( e.g., when sensitive private information can
be extracted from a model), or unacceptable access to a model itself (e.g., when the model represents
personalized preferences of an individual or is protected b y intellectual property).
For individuals to trust claims about an ML system sufﬁcient ly so as to participate in its training, they
need evidence about data access (who will have access to what kinds of data under what circumstances),
data usage, and data protection. The AI development communi ty , and other relevant communities, have
developed a range of methods and mechanisms to address these concerns, under the general heading of
"privacy-preserving machine learning" (PPML) [94].
Privacy-preserving machine learning aims to protect the pr ivacy of data or models used in machine
learning, at training or evaluation time and during deploym ent. PPML has beneﬁts for model users, and
for those who produce the data that models are trained on.
PPML is heavily inspired by research from the cryptography a nd privacy communities and is performed
in practice using a combination of techniques, each with its own limitations and costs. These techniques
are a powerful tool for supporting trust between data owners and model users, by ensuring privacy of
key information. However, they must be used judiciously , wi th informed trade-offs among (1) privacy
beneﬁts, (2) model quality , (3) AI developer experience and productivity , and (4) overhead costs such as
computation, communication, or energy consumption. They a re also not useful in all contexts; therefore,
a combination of techniques may be required in some contexts to protect data and models from the
actions of well-resourced malicious actors.
Before turning to our recommendation, we provide brief summ aries of several PPML techniques that
could support veriﬁable claims.
Federated learning is a machine learning technique where many clients (e.g., mo bile devices or whole
organizations) collaboratively train a model under the orc hestration of a central server (e.g., service
provider), while keeping the training data decentralized [95]. Each client’s raw data is stored locally and
not exchanged or transferred [95]. Federated learning addresses privacy concerns around the centralized
collection of raw data, by keeping the data where it is genera ted (e.g., on the user’s device or in a local
silo) and only allowing model updates to leave the client.
Federated learning does not, however, fully guarantee the p rivacy of sensitive data on its own, as some
aspects of raw data could be memorized in the training proces s and extracted from the trained model if
measures are not taken to address this threat. These measure s include quantifying the degree to which
models memorize training data [96], and incorporating differential privacy techniques to lim it the con-
tribution of individual clients in the federated setting [97]. Even when used by itself, federated learning
28addresses the threats that are endemic to centralized data c ollection and access, such as unauthorized
access, data hacks, and leaks, and the inability of data owne rs to control their data lifecycle.
Differential privacy [98]is a system for publicly sharing information derived from a d ataset by de-
scribing the patterns of groups within the dataset, while wi thholding information about individuals in
the dataset; it allows for precise measurements of privacy r isks for current and potential data owners,
and can address the raw-data-extraction threat described a bove. Differential privacy works through the
addition of a controlled amount of statistical noise to obsc ure the data contributions from records or
individuals in the dataset.55Differential privacy is already used in various private and public AI settings,
and researchers are exploring its role in compliance with ne w privacy regulations [100][99].
Differential privacy and federated learning complement ea ch other in protecting the privacy of raw data:
federated learning keeps the raw data on the personal device , so it is never seen by the model trainer,
while differential privacy ensures the model sufﬁciently p revents the memorization of raw data, so that
it cannot be extracted from the model by its users.56These techniques do not, however, protect the
model itself from theft [101].
Encrypted computation addresses this risk by allowing the model to train and run on e ncrypted data
while in an encrypted state, at the cost of overhead in terms o f computation and communication. As a re-
sult, those training the model will not be able to see, leak, o r otherwise abuse the data in its unencrypted
form. The most well known methods for encrypted computation are homomorphic encryption, secure
multi-party computation, and functional encryption [102]. For example, one of OpenMined’s upcoming
projects is Encrypted Machine Learning as a Service, which a llows a model owner and data owner to
use their model and data to make a prediction, without the mod el owner disclosing their model, and
without the data owner disclosing their data.57
These software mechanisms can guarantee tighter bounds on A I model usage than the legal agreements
that developers currently employ , and tighter bounds on use r data usage than institutional mechanisms
such as user privacy agreements. Encrypted computation cou ld also potentially improve the veriﬁability
of claims by allowing sensitive models to be shared for audit ing in a more secure fashion. A hardware-
based method to protect models from theft (and help protect r aw data from adversaries) is the use of
secure enclaves, as discussed in Section 4.1 below.
In the future, it may be possible to rely on a platform that ena blesveriﬁable data policies which address
some of the security and privacy vulnerabilities in existin g IT systems. One proposal for such a platform
is Google’s Project Oak,58which leverages open source secure enclaves (see Section 4.1 ) and formal
veriﬁcation to technically enforce and assure policies aro und data storage, manipulation, and exchange.
As suggested by this brief overview of PPML techniques, ther e are many opportunities for improving
the privacy and security protections associated with ML sys tems. However, greater standardization of
55To illustrate how statistical noise can be helpful in protec ting privacy, consider the difference between a survey that solicits
and retains "raw" answers from individuals, on the one hand, and another survey in which the respondents are asked to ﬂip
a coin in order to determine whether they will either provide the honest answer right away or ﬂip the coin again in order
to determine which answer to provide. The latter approach wo uld enable individual survey respondents to have plausible
deniability regarding their true answers, but those conduc ting the survey could still learn useful information from th e responses,
since the noise would largely cancel out at scale. For an acce ssible discussion of the ideas behind differential privacy and its
applications, from which this short summary was adapted, se e[99].
56For an example of the combination of federated learning and d ifferential privacy, see McMahan et. al. [97].
57Seehttps://www.openmined.com .
58See the Appendix VI(B) for further discussion of this project.
29of PPML techniques–and in particular, the use of open source PPML frameworks that are benchmarked
against common performance measures–may be needed in order for this to translate into a major impact
on the veriﬁability of claims about AI development. First, r obust open source frameworks are needed in
order to reduce the skill requirement for implementing PPML techniques, which to date have primarily
been adopted by large technology companies with in-house ex pertise in both ML and cryptography .
Second, common standards for evaluating new PPML technique s could increase the comparability of new
results, potentially accelerating research progress. Fin ally , standardization could improve the ability of
external parties (including users, auditors, and policyma kers) to verify claims about PPML performance.
Recommendation: AI developers should develop, share, and u se suites of tools for privacy-
preserving machine learning that include measures of perfo rmance against common standards.
Where possible, AI developers should contribute to, use, an d otherwise support the work of open-source
communities working on PPML, such as OpenMined, Microsoft S EAL, tf-encrypted, tf-federated, and
nGraph-HE. These communities have opened up the ability to u se security and privacy tools in the ML
setting, and further maturation of the software libraries b uilt by these communities could yield still
further beneﬁts.
Open-source communities projects or projects backed by a pa rticular company can sometimes suffer from
a lack of stable funding support59or independence as organizational priorities shift, sugge sting a need for
an AI community-wide approach to supporting PPML’s growth. Notwithstanding some challenges asso-
ciated with open source projects, they are uniquely amenabl e to broad-based scrutiny and iteration, and
have yielded beneﬁts already . Notably , integrated librari es for multiple techniques in privacy-preserving
ML have started being developed for major deep learning fram eworks such as TensorFlow and PyTorch.
Benchmarks for PPML could help unify goals and measure progr ess across different groups.60A central-
ized repository of real-world implementation cases, a comp ilation of implementation guides, and work
on standardization /interoperability would all also aid in supporting adoption and scrutiny of privacy-
preserving methods.61
59Novel approaches to funding open source work should also be c onsidered in this context, such as GitHub’s "sponsors"
initiative.https://help.github.com/en/github/supporting-the-op en-source-community-wi
th-github-sponsors/about-github-sponsors
60The use of standard tools, guides, and benchmarks can also po tentially advance research in other areas, but we focus on
privacy-preserving ML in particular here given the backgro unds of the authors who contributed to this subsection. Addi tionally,
we note that some benchmarks have been proposed in the PPML li terature for speciﬁc subsets of techniques, such as DPComp
for differential privacy, but we expect that further explor ation of benchmarks across the full spectra of PPML techniqu es would
be valuable.
61On the other hand, we note that benchmarks also have potentia l disadvantages, as they incentivize developers to perform
well on the speciﬁc benchmark, rather than focusing on the sp eciﬁcs of the intended use case of their product or service, w hich
may signiﬁcantly diverge from the benchmark setting; the de sign of benchmarks, and more diverse and adaptive evaluatio n
and comparison methods, is its own technical challenge, as w ell as an institutional challenge to incentivize appropria te curation
and use of benchmarks to establish a common understanding of what is achievable.
304 Hardware Mechanisms and Recommendations
Computing hardware enables the training, testing, and use o f AI systems. Hardware relevant to AI
development ranges from sensors, networking, and memory , t o, perhaps most crucially , processing power
[103].62Concerns about the security and other properties of computi ng hardware, as well as methods
to address those concerns in a veriﬁable manner, long preced e the current growth in adoption of AI.
However, because of the increasing capabilities and impact s of AI systems and the particular hardware
demands of the ﬁeld, there is a need for novel approaches to as suring the veriﬁability of claims about
the hardware used in AI development.
Hardware mechanisms involve physical computing resources (e.g., CPUs and GPUs), including their
distribution across actors, the ways they are accessed and m onitored, and their properties (e.g., how they
are designed, manufactured, or tested). Hardware can suppo rt veriﬁable claims in various ways. Secure
hardware can play a key role in private and secure machine lea rning by translating privacy constraints
and security guarantees into scrutable hardware designs or by leveraging hardware components in a
software mechanism. Hardware mechanisms can also be used to demonstrate the ways in which an
organization is using its general-purpose computing capab ilities.
At a higher level, the distribution of computing power acros s actors can potentially inﬂuence who is
in a position to verify certain claims about AI development. This is true on the assumption that, all
things being equal, more computing power will enable more po werful AI systems to be built, and that
a technical capability to verify claims may itself require n on-negligible computing resources.63The
use of standardized, publicly available hardware (sometim es called "commodity hardware") across AI
systems also aids in the independent reproducibility of tec hnical results, which in turn could play a role
in technical auditing and other forms of accountability . Fi nally , hardware mechanisms can be deployed
to enforce and verify policies relating to the security of th e hardware itself (which, like software, might
be compromised through error or malice).
Existing mechanisms performing one or more of these functio ns are discussed below.
Formal veriﬁcation , discussed above in the software mechanisms section, is the process of establishing
whether a software or hardware system satisﬁes some require ments or properties, using formal methods
to generate mathematical proofs. Practical tools, such as G PUVerify for GPU kernels,65exist to formally
verify components of the AI hardware base, but veriﬁcation o f the complete hardware base is currently
an ambitious goal. Because only parts of the AI hardware ecos ystem are veriﬁed, it is important to map
which properties are being veriﬁed for different AI acceler ators and under what assumptions, who has
access to evidence of such veriﬁcation processes (which may be part of a third party audit), and what
properties we should invest more research effort into verif ying (or which assumption would be a priority
to drop).
62Experts disagree on the extent to which large amounts of comp uting power are key to progress in AI development. See,
e.g., Sutton[104]and Brooks[105]for different opinions about the importance of computing po wer relative to other factors.
63Since training AI systems is more compute-intensive64than running them, it is not clear that equivalent computati onal
resources will always be required on the part of those verify ing claims about an AI system. However, AI systems are also
beginning to require non-trivial computing resources to ru n, sometimes requiring the model to be split over multiple ma chines.
Additionally, one might need to run an AI system many times in order to verify claims about its characteristics, even if ea ch
run is inexpensive. We thus make the conservative assumptio n that more computing resources would be (all things being
equal) helpful to the scrutiny of claims about large-scale A I systems, as discussed below in the context of academic acce ss to
computing resources, while recognizing that this may not al ways be true in particular cases.
65http://multicore.doc.ic.ac.uk /tools/GPUVerify/
31Remote attestation leverages a "root of trust" (provided in hardware or in softw are, e.g., a secret key
stored in isolated memory) to cryptographically sign a meas urement or property of the system, thus
providing a remote party proof of the authenticity of the mea surement or property . Remote attestation
is often used to attest that a certain version of software is c urrently running, or that a computation took
a certain amount of time (which can then be compared to a refer ence by the remote party to detect
tampering)[106].
Cloud computing: Hardware is also at the heart of the relationship between clo ud providers and cloud
users (as hardware resources are being rented). Associated veriﬁcation mechanisms can help ensure
that computations are being performed as promised, without the client having direct physical access
to the hardware. For example, one could have assurances that a cloud-based AI service is not skimp-
ing on computations by running a less powerful model than adv ertised, operating on private data in a
disallowed fashion, or compromised by malware [107].
Cloud providers are a promising intervention point for trus t-building mechanisms; a single cloud provider
services, and therefore has inﬂuence over, many customers. Even large AI labs rely predominantly on
cloud computing for some or all of their AI development. Clou d providers already employ a variety of
mechanisms to minimize risks of misuse on their platforms, i ncluding "Know Your Customer" services
and Acceptable Use Policies. These mechanisms could be exte nded to cover AI misuse [108]. Additional
mechanisms could be developed such as a forum where cloud pro viders can share best-practices about
detecting and responding to misuse and abuse of AI through th eir services.66
We now turn to more detailed discussions of three hardware me chanisms that could improve the veriﬁa-
bility of claims: we highlight the importance of secure hardware for machine learning ,high-precision
compute measurement , and computing power support for academia .
66These conversations could take place in existing industry f ora, such as the Cloud Security Alliance
(https://cloudsecurityalliance.org), or through the establishmen t of new fora dedicated to AI cloud providers.
324.1 Secure Hardware for Machine Learning
Problem:
Hardware security features can provide strong assurances a gainst theft of data and models,
but secure enclaves (also known as Trusted Execution Enviro nments) are only available on
commodity (non-specialized) hardware. Machine learning t asks are increasingly executed on
specialized hardware accelerators, for which the developm ent of secure enclaves faces signiﬁ-
cant up-front costs and may not be the most appropriate hardw are-based solution.
Since AI systems always involve physical infrastructure, t he security of that infrastructure can play a
key role in claims about a system or its components being secu re and private. Secure enclaves have
emerged in recent years as a way to demonstrate strong claims about privacy and security that cannot
be achieved through software alone. iPhones equipped with f acial recognition for screen unlocking, for
example, store face-related data on a physically distinct p art of the computer known as a secure enclave
in order to provide more robust privacy protection. Increas ing the range of scenarios in which secure
enclaves can be applied in AI, as discussed in this subsectio n, would enable higher degrees of security
and privacy protection to be demonstrated and demanded.
A secure enclave is a set of software and hardware features th at together provide an isolated execution
environment that enables a set of strong guarantees regardi ng security for applications running inside the
enclave[109]. Secure enclaves reduce the ability of malicious actors to a ccess sensitive data or interfere
with a program, even if they have managed to gain access to the system outside the enclave. Secure
enclaves provide these guarantees by linking high-level de sired properties (e.g., isolation of a process
from the rest of the system) to low-level design of the chip la yout and low-level software interacting
with the chip.
The connection between physical design and low-level softw are and high-level security claims relies
on a set of underlying assumptions. Despite the fact that res earchers have been able to ﬁnd ways to
invalidate these underlying assumptions in some cases, and thus invalidate the high-level security claims
[110][111], these mechanisms help to focus defensive efforts and assur e users that relatively extreme
measures would be required to invalidate the claims guarant eed by the design of the enclave.
While use of secure enclaves has become relatively commonpl ace in the commodity computing indus-
tries, their use in machine learning is less mature. Executi on of machine learning on secure enclaves has
been demonstrated, but comes with a performance overhead [112].67Demonstrations to date have been
carried out on commodity hardware (CPUs [113][114]and GPUs[115]) or have secure and veriﬁable
outsourcing of parts of the computation to less secure hardw are[116][117], rather than on hardware
directly optimized for machine learning (such as TPUs).
For most machine learning applications, the cost of using co mmodity hardware not specialized for ma-
chine learning is fairly low because the hardware already ex ists, and their computational demands can
be met on such commodity hardware. However, cutting edge mac hine learning models often use sig-
niﬁcantly more computational resources [118], driving the use of more specialized hardware for both
training and inference. If used with specialized AI hardwar e, the use of secure enclaves would require
renewed investment for every new design, which can end up bei ng very costly if generations are short
67For example, training ResNet-32 using Myelin (which utilis es a CPU secure enclave) requires 12.9 mins /epoch and results
in a ﬁnal accuracy of 90.8%, whereas the same training on a non -private CPU requires 12.3 mins /epoch and results in a ﬁnal
accuracy of 92.4% [112].
33and of limited batch sizes (as the cost is amortized across al l chips that use the design). Some specialized
AI hardware layouts may require entirely novel hardware sec urity features – as the secure enclave model
may not be applicable – involving additional costs.
One particularly promising guarantee that might be provide d by ML-speciﬁc hardware security features,
coupled with some form of remote attestation, is a guarantee that a model will never leave a particular
chip, which could be a key building block of more complex priv acy and security policies.
Recommendation: Industry and academia should work togethe r to develop hardware security
features for AI accelerators68or otherwise establish best practices for the use of secure h ardware
(including secure enclaves on commodity hardware) in machi ne learning contexts.
A focused and ongoing effort to integrate hardware security features into ML-specialized hardware could
add value, though it will require collaboration across the s ector.
Recent efforts to open source secure enclave designs could h elp accelerate the process of comprehen-
sively analyzing the security claims made about certain sys tems[119]. As more workloads move to
specialized hardware, it will be important to either develo p secure enclaves for such hardware (or al-
ternative hardware security solutions), or otherwise deﬁn e best practices for outsourcing computation
to "untrusted" accelerators while maintaining privacy and security . Similarly , as many machine learn-
ing applications are run on GPUs, it will be important to impr ove the practicality of secure enclaves or
equivalent privacy protections on these processors.
The addition of dedicated security features to ML accelerat ors at the hardware level may need to take
a different form than a secure enclave. This is in part due to d ifferent architectures and different use
of space on the chip; in part due to different weighting of sec urity concerns (e.g., it may be especially
important to prevent unauthorized access to user data); and in part due to a difference in economies
of scale relative to commodity chips, with many developers o f ML accelerators being smaller, less-well-
resourced actors relative to established chip design compa nies like Intel or NVIDIA.
68An AI accelerator is a form of computing hardware that is spec ialized to perform an AI-related computation efﬁciently,
rather than to perform general purpose computation.
344.2 High-Precision Compute Measurement
Problem:
The absence of standards for measuring the use of computatio nal resources reduces the value
of voluntary reporting and makes it harder to verify claims a bout the resources used in the AI
development process.
Although we cannot know for certain due to limited transpare ncy , it is reasonable to assume that a
signiﬁcant majority of contemporary computing hardware us ed for AI training and inference is installed
in data centers (which could be corporate, governmental, or academic), with smaller fractions in server
rooms or attached to individual PCs.69
Many tools and systems already exist to monitor installed ha rdware and compute usage internally (e.g.,
across a cloud provider’s data center or across an academic c luster’s user base). A current example of
AI developers reporting on their compute usage is the inclus ion of training-related details in published
research papers and pre-prints, which often share the amoun t of compute used to train or run a model.70
These are done for the purposes of comparison and replicatio n, though often extra work is required to
make direct comparisons as there is no standard method for me asuring and reporting compute usage.71
This ambiguity poses a challenge to trustworthy AI developm ent, since even AI developers who want to
make veriﬁable claims about their hardware use are not able t o provide such information in a standard
form that is comparable across organizations and contexts.
Even in the context of a particular research project, issues such as mixed precision training,72use of
heterogeneous computing resources, and use of pretrained m odels all complicate accurate reporting
that is comparable across organizations.73The lack of a common standard or accepted practice on how
to report the compute resources used in the context of a parti cular project has led to several efforts to
extract or infer the computational requirements of various advances and compare them using a common
framework[118].
The challenge of providing accurate and useful information about the computational requirements of a
system or research project is not unique to AI – computer syst ems research has struggled with this prob-
lem for some time.74Both ﬁelds have seen an increasing challenge in comparing an d reproducing results
now that organizations with exceptionally large compute re sources (also referred to as "hyperscalers")
69For reference, the Cisco Global Cloud Index forecasts that t he ratio of data center trafﬁc to non-data center trafﬁc by 20 21
will be 103:1. When looking just at data centers, they foreca st that by 2021, 94% of workloads and compute instances will b e
processed by cloud data centers, with the remaining 6% proce ssed by traditional data centers. Note, however, that these are
for general workloads, not AI speciﬁc [120].
70Schwartz and Dodge et al. have recommended that researchers always publish ﬁnancial and computational costs alongside
performance increases [121].
71There are, however, emerging efforts at standardization in particular contexts. The Transaction Processing Performa nce
Council has a related working group, and efforts like MLPerf are contributing to standardization of some inference-rel ated
calculations, though accounting for training remains espe cially problematic.
72Mixed precision refers to the growing use of different binar y representations of ﬂoating point numbers with varying lev els
of precision (e.g., 8 bit, 16 bit, 32 bit or 64 bit) at differen t points of a computation, often trading-off lower precisio n for higher
throughput or performance in ML contexts relative to non-ML contexts. Since an 8 bit ﬂoating point operation, say, diffe rs in
hardware requirements from a 64 bit ﬂoating point operation , traditional measures in terms of Floating Point Operation s Per
Second (FLOPS) fail to capture this heterogeneity.
73For an illustrative discussion of the challenges associate d with reporting compute usage for a large-scale AI project, see,
e.g., OpenAI’s Dota 2 project [122].
74See, e.g., Vitek & Kalibera [123]and Hoeﬂer & Belli [124].
35play an ever-increasing role in research in those ﬁelds. We b elieve there is value in further engaging
with the computer systems research community to explore cha llenges of reproducibility , benchmarking,
and reporting, though we also see value in developing AI-spe ciﬁc standards for compute reporting.
Increasing the precision and standardization of compute re porting could enable easier comparison of
research results across organizations. Improved methods c ould also serve as building blocks of credible
third party oversight of AI projects: an auditor might note, for example, that an organization has more
computing power available to it than was reportedly used on a n audited project, and thereby surface
unreported activities relevant to that project. And employ ees of an organization are better able to ensure
that their organization is acting responsibly to the extent that they are aware of how computing power,
data, and personnel are being allocated internally for diff erent purposes.
Recommendation: One or more AI labs should estimate the comp uting power involved in a single
project in great detail, and report on the potential for wide r adoption of such methods.
We see value in one or more AI labs conducting a "comprehensiv e" compute accounting effort, as a means
of assessing the feasibility of standardizing such account ing. "Comprehensive" here refers to accounting
for as much compute usage pertinent to the project as is feasi ble, and increasing the precision of reported
results relative to existing work.
It is not clear how viable standardization is, given the afor ementioned challenges, though there is likely
room for at least incremental progress: just in the past few y ears, a number of approaches to calculating
and reporting compute usage have been tried, and in some case s have propagated across organizations.
AI researchers interested in conducting such a pilot should work with computer systems researchers who
have worked on related challenges in other contexts, includ ing the automating of logging and reporting.
Notably , accounting of this sort has costs associated with i t, and the metrics of success are unclear. Some
accounting efforts could be useful for experts but inaccess ible to non-experts, for example, or could only
be workable in a particular context (e.g., with a relatively simple training and inference pipeline and
limited use of pretrained models). As such, we do not advocat e for requiring uniformly comprehensive
compute reporting.
Depending on the results of early pilots, new tools might hel p automate or simplify such reporting,
though this is uncertain. One reason for optimism about the d evelopment of a standardized approach
is that a growing fraction of computing power usage occurs in the cloud at "hyperscale" data centers,
so a relatively small number of actors could potentially imp lement best practices that apply to a large
fraction of AI development [120].
It is also at present unclear who should have access to report s about compute accounting. While we
applaud the current norm in AI research to voluntarily share compute requirements publicly , we expect
for-proﬁt entities would have to balance openness with comm ercial secrecy , and government labs may
need to balance openness with security considerations. Thi s may be another instance in which audi-
tors or independent veriﬁers could play a role. Standardiza tion of compute accounting is one path to
formalizing the auditing practice in this space, potential ly as a building block to more holistic auditing
regimes. However, as with other mechanisms discussed here, it is insufﬁcient on its own.
364.3 Compute Support for Academia
Problem:
The gap in compute resources between industry and academia l imits the ability of those outside
of industry to scrutinize technical claims made by AI develo pers, particularly those related to
compute-intensive systems.
In recent years, a large number of academic AI researchers ha ve transitioned into industry AI labs. One
reason for this shift is the greater availability of computi ng resources in industry compared to academia.
This talent shift has resulted in a range of widely useful sof tware frameworks and algorithmic insights,
but has also raised concerns about the growing disparity bet ween the computational resources available
to academia and industry [125].
The disparity between industry and academia is clear overal l, even though some academic labs are
generously supported by government75or industry76sponsors, and some government agencies are on
the cutting edge of building and providing access to superco mputers.77
Here we focus on a speciﬁc beneﬁt of governments78taking action to level the playing ﬁeld of computing
power: namely , improving the ability of ﬁnancially disinte rested parties such as academics to verify
the claims made by AI developers in industry , especially in t he context of compute-intensive systems.
Example use cases include:
•Providing open-source alternatives to commercial AI syste ms: given the current norm in AI
development of largely-open publication of research, a lim iting factor in providing open source al-
ternatives to commercially trained AI models is often the co mputing resources required. As models
become more compute-intensive, government support may be r equired to maintain a thriving open
source AI ecosystem and the various beneﬁts that accrue from it.
•Increasing scrutiny of commercial models : as outlined in the institutional mechanisms section
(see the subsections on red team exercises andbias and safety bounties ), there is considerable
value in independent third parties stress-testing the mode ls developed by others. While "black box"
testing can take place without access to signiﬁcant compute resources (e.g., by remote access to an
instance of the system), local replication for the purpose o f testing could make testing easier, and
could uncover further issues than those surfaced via remote testing alone. Additional computing
resources may be especially needed for local testing of AI sy stems that are too large to run on a
single computer (such as some recent language models).
•Leveraging AI to test AI : as AI systems become more complex, it may be useful or even ne cessary
to deploy adaptive, automated tests to explore potential fa ilure modes or hidden biases, and such
testing may become increasingly compute-intensive.
75https://epsrc.ukri.org/research/facilities/hpc/
76See, e.g., https: //www.tensorﬂow.org /tfrc, https://aws.amazon.com /grants/and https://www.microsoft.com /en-
us/research/academic-program /microsoft-azure-for-research /for examples of industry support for academic computing.
77For example, the US Department of Energy’s supercomputing d ivision currently hosts the fastest supercomputer worldwi de.
78While industry actors can and do provide computing power sup port to non-industry actors in beneﬁcial ways, the scale and
other properties of such programs are likely to be affected b y the rises and falls of particular companies’ commercial fo rtunes,
and thus are not a reliable long-term solution to the issues d iscussed here.
37•Verifying claims about compute requirements : as described above, accounting for the compute
inputs of model training is currently an open challenge in AI development. In tandem with stan-
dardization of compute accounting, compute support to non- industry actors would enable repli-
cation efforts, which would verify or undermine claims made by AI developers about the resource
requirements of the systems they develop.
Recommendation: Government funding bodies should substan tially increase funding of comput-
ing power resources for researchers in academia, in order to improve the ability of those re-
searchers to verify claims made by industry.
While computing power is not a panacea for addressing the gap in resources available for research in
academia and industry , funding bodies such as those in gover nments could level the playing ﬁeld between
sectors by more generously providing computing credits to r esearchers in academia.79Such compute
provision could be made more affordable by governments leve raging their purchasing power in negoti-
ations over bulk compute purchases. Governments could also build their own compute infrastructures
for this purpose. The particular amounts of compute in quest ion, securing the beneﬁts of scale while
avoiding excessive dependence on a particular compute prov ider, and ways of establishing appropriate
terms for the use of such compute are all exciting areas for fu ture research.80
79As advocated by various authors, e.g., Sastry et al. [118], Rasser & Lambert et. al. [126], and Etchemendy and Li [127].
80This may require signiﬁcant levels of funding, and so the ben eﬁts should be balanced against the opportunity cost of publ ic
spending.
385 Conclusion
Artiﬁcial intelligence has the potential to transform soci ety in ways both beneﬁcial and harmful. Ben-
eﬁcial applications are more likely to be realized, and risk s more likely to be avoided, if AI developers
earn rather than assume the trust of society and of one anothe r. This report has ﬂeshed out one way
of earning such trust, namely the making and assessment of ve riﬁable claims about AI development
through a variety of mechanisms. A richer toolbox of mechani sms for this purpose can inform develop-
ers’ efforts to earn trust, the demands made of AI developers by activists and civil society organizations,
and regulators’ efforts to ensure that AI is developed respo nsibly .
If the widespread articulation of ethical principles can be seen as a ﬁrst step toward ensuring responsible
AI development, insofar as it helped to establish a standard against which behavior can be judged, then
the adoption of mechanisms to make veriﬁable claims represe nts a second. The authors of this report
are eager to see further steps forward and hope that the frami ng of these mechanisms inspires the AI
community to begin a meaningful dialogue around approachin g veriﬁability in a collaborative fashion
across organizations. We are keen to discover, study , and fo reground additional institutional, software,
and hardware mechanisms that could help enable trustworthy AI development. We encourage readers
interested in collaborating in these or other areas to conta ct the corresponding authors of the report.81
As suggested by the title of the report (which references sup porting veriﬁable claims rather than ensur-
ing them), we see the mechanisms discussed here as enabling i ncremental improvements rather than
providing a decisive solution to the challenge of verifying claims in the AI ecosystem. And despite the
beneﬁts associated with veriﬁable claims, they are also ins ufﬁcient to ensure that AI developers will
behave responsibly . There are at least three reasons for thi s.
First, there is a tension between veriﬁability of claims and the generality of such claims. This tension
arises because the narrow properties of a system are easier t o verify than the general ones, which tend
to be of greater social interest. Safety writ large, for exam ple, is inherently harder to verify than perfor-
mance on a particular metric for safety . Additionally , broa d claims about the beneﬁcial societal impacts
of a system or organization are harder to verify than more cir cumscribed claims about impacts in speciﬁc
contexts.
Second, the veriﬁability of claims does not ensure that they will be veriﬁed in practice. The mere ex-
istence of mechanisms for supporting veriﬁable claims does not ensure that they will be demanded by
consumers, citizens, and policymakers (and even if they are , the burden ought not to be on them to do
so). For example, consumers often use technologies in ways t hat are inconsistent with their stated values
(e.g., a concern for personal privacy) because other factor s such as convenience and brand loyalty also
play a role in inﬂuencing their behavior [128].
Third, even if a claim about AI development is shown to be fals e, asymmetries of power may prevent
corrective steps from being taken. Members of marginalized communities, who often bear the brunt of
harms associated with AI [2], often lack the political power to resist technologies that they deem detri-
mental to their interests. Regulation will be required to en sure that AI developers provide evidence that
bears on important claims they make, to limit applications o f AI where there is insufﬁcient technical and
social infrastructure for ensuring responsible developme nt, or to increase the variety of viable options
81The landing page for this report, www.towardtrustworthyai .com, will also be used to share relevant updates after the
report’s publication.
39available to consumers that are consistent with their state d values.
These limitations notwithstanding, veriﬁable claims repr esent a step toward a more trustworthy AI de-
velopment ecosystem. Without a collaborative effort betwe en AI developers and other stakeholders to
improve the veriﬁability of claims, society’s concerns abo ut AI development are likely to grow: AI is
being applied to an increasing range of high-stakes tasks, a nd with this wide deployment comes a grow-
ing range of risks. With a concerted effort to enable veriﬁab le claims about AI development, there is
a greater opportunity to positively shape AI’s impact and in crease the likelihood of widespread societal
beneﬁts.
40Acknowledgements
We are extremely grateful to participants in the April 2019 w orkshop that catalyzed this report, as well
as the following individuals who provided valuable input on earlier versions of this document: David
Lansky , Tonii Leach, Shin Shin Hua, Chris Olah, Alexa Hagert y , Madeleine Clare Elish, Larissa Schiavo,
Heather Roff, Rumman Chowdhury , Ludwig Schubert, Joshua Ac hiam, Chip Huyen, Xiaowei Huang,
Rohin Shah, Genevieve Fried, Paul Christiano, Sean McGrego r, Tom Arnold, Jess Whittlestone, Irene
Solaiman, Ashley Pilipiszyn, Catherine Olsson, Bharath Ra msundar, Brandon Perry , Justin Wang, Max
Daniel, Ryan Lowe, Rebecca Crootof, Umang Bhatt, Ben Garﬁnk el, Claire Leibowicz, Ryan Khurana,
Connor Leahy , Chris Berner, Daniela Amodei, Erol Can Akbaba , William Isaac, Iason Gabriel, Laura
Weidinger, Thomas Dietterich, Olexa Bilaniuk, and attende es of a seminar talk given by author Miles
Brundage on this topic at the Center for Human-Compatible AI (CHAI). None of these people necessarily
endorses the content of the report.
41References
[1]Amanda Askell, Miles Brundage, and Gillian Hadﬁeld. “The Ro le of Cooperation in Responsible
AI Development”. In: arXiv (July 2019). arXiv: 1907.04534 .URL:http://arxiv.org/
abs/1907.04534 .
[2]Meredith Whittaker et al. AI Now Report 2018 . Tech. rep. New York, NY, USA: AI Now Institute,
2018, p. 68. URL:https://ainowinstitute.org/AI%7B%5C_%7DNow%7B%5C_
%7D2018%7B%5C_%7DReport.pdf .
[3]Kate Crawford et al. AI Now 2019 Report . Tech. rep. New York, NY, USA: AI Now Institute, 2019,
p. 100. URL:https://ainowinstitute.org/AI%7B%5C_%7DNow%7B%5C_%7 D2
019%7B%5C_%7DReport.pdf .
[4]Elizabeth Gibney. “The battle for ethical AI at the world’s b iggest machine-learning conference”.
In:Nature (2020). ISSN: 0028-0836. DOI:10.1038/d41586-020-00160-y .URL:http
://www.nature.com/articles/d41586-020-00160-y .
[5]Brent Mittelstadt. “AI Ethics - Too Principled to Fail?” In: Nature Machine Intelligence (June
2019). DOI:10.2139/ssrn.3391293 . arXiv:1906.06668 .URL:https://arxi
v.org/ftp/arxiv/papers/1906/1906.06668.pdf .
[6]Karen Hao. This is how AI bias really happens-and why it’s so hard to ﬁx - MIT Tec hnology Re-
view. 2019. URL:https://www.technologyreview.com/s/612876/this-is-
how-ai-bias-really-happensand-why-its-so-hard-to-fi x/(visited on
02/01/2020).
[7]University of California - Berkeley . Artiﬁcial intelligence advances threaten privacy of health data:
Study ﬁnds current laws and regulations do not safeguard individu als’ conﬁdential health informa-
tion – ScienceDaily . 2019. URL:https://www.sciencedaily.com/releases/201
9/01/190103152906.htm (visited on 02/01/2020).
[8]Adam Alter. Irresistible : the rise of addictive technology and the business o f keeping us hooked . The
Penguin Group, 2017, p. 354. ISBN: 9781594206641. URL:http://adamalterauthor.
com/irresistible .
[9]Julia Angwin et al. Machine Bias . 2016. URL:https://www.propublica.org/articl
e/machine-bias-risk-assessments-in-criminal-sentenc ing (visited on
02/01/2020).
[10]Irene Solaiman et al. “Release Strategies and the Social Imp acts of Language Models”. In: arXiv
(Aug. 2019). arXiv: 1908.09203 .URL:http://arxiv.org/abs/1908.09203 .
[11]Mary L. Gray and Siddharth Suri. Ghost work : how to stop Silicon Valley from building a new
global underclass . Eamon Dolan/Houghton Mifﬂin Harcourt, 2019, p. 254. ISBN: 1328566242.
URL:https://www.hmhco.com/shop/books/Ghost-Work/9781328 566249 .
[12]Carl Benedikt Frey and Michael A Osborne. The Future of Employment: How Susceptible Are Jobs
to Computerisation? Tech. rep. The Oxfort Marting Programme on Technology and Em ployment,
2013, p. 72. URL:https://www.oxfordmartin.ox.ac.uk/downloads/acade
mic/The%7B%5C_%7DFuture%7B%5C_%7Dof%7B%5C_%7DEmploy ment.pdf .
42[13]Jess Whittlestone et al. “The role and limits of principles i n AI ethics: Towards a focus on ten-
sions”. In: AIES 2019 - Proceedings of the 2019 AAAI /ACM Conference on AI, Ethics, and Society .
Association for Computing Machinery , Inc, Jan. 2019, pp. 19 5–200. ISBN: 9781450363242. DOI:
10.1145/3306618.3314289 .URL:https://www.researchgate.net/publi
cation/334378492%7B%5C_%7DThe%7B%5C_%7DRole%7B%5C_% 7Dand%7B%5C
_%7DLimits%7B%5C_%7Dof%7B%5C_%7DPrinciples%7B%5C_%7 Din%7B%5C_%
7DAI%7B%5C_%7DEthics%7B%5C_%7DTowards%7B%5C_%7Da%7B %5C_%7DFocu
s%7B%5C_%7Don%7B%5C_%7DTensions .
[14]Anna Jobin, Marcello Ienca, and Effy Vayena. “The global lan dscape of AI ethics guidelines”. In:
Nature Machine Intelligence 1.9 (Sept. 2019), pp. 389–399. DOI:10.1038/s42256-019-0088-2 .
URL:https://www.nature.com/articles/s42256-019-0088-2 .
[15]Beijing AI Principles Blog. Beijing AI Principles . 2019. URL:https://www.baai.ac.cn/
blog/beijing-ai-principles (visited on 02/01/2020).
[16]Yi Zeng, Enmeng Lu, and Cunqing Huangfu. “Linking Artiﬁcial Intelligence Principles”. In: arXiv
(Dec. 2018). arXiv: 1812.04814 .URL:http://arxiv.org/abs/1812.04814 .
[17]Edelman. 2019 Edelman Trust Barometer | Edelman . 2019. URL:https://www.edelman.
com/research/2019-edelman-trust-barometer (visited on 02/01/2020).
[18]Baobao Zhang and Allan Dafoe. Artiﬁcial Intelligence: American Attitudes and Trends . Tech. rep.
Future of Humanity Institute, 2019. URL:https://governanceai.github.io/US-
Public-Opinion-Report-Jan-2019/ .
[19]Gregory Travis. How the Boeing 737 Max Disaster Looks to a Software Developer . 2019. URL:http
s://spectrum.ieee.org/aerospace/aviation/how-the-bo eing-737-m
ax-disaster-looks-to-a-software-developer (visited on 02/08/2020).
[20]Jenny et al. Gesley. “Regulation of Artiﬁcial Intelligence in Selected Jurisdictions”. In: (Jan.
2019). URL:https://www.loc.gov/law/help/artificial-intelligenc e/
regulation-artificial-intelligence.pdf .
[21]E. L. Trist. The evolution of socio-technical systems : a conceptual framework and an action research
program . Ontario Quality of Working Life Centre, 1981, p. 67. ISBN: 9780774362863. URL:http
s://www.semanticscholar.org/paper/The-Evolution-of- Socio-Tech
nical-Systems%7B%5C%%7D3A-A-and-an-Trist/0ca460a31c %2093e5d7d
1789be14e3cc30338f4ad4c .
[22]Sundar Pichai. AI at Google: our principles . 2018. URL:https://www.blog.google/te
chnology/ai/ai-principles/ (visited on 02/01/2020).
[23]OpenAI. OpenAI Charter . 2018. URL:https://openai.com/charter/ (visited on 02/01/2020).
[24]Microsoft. AI Principles & Approach from Microsoft .URL:https://www.microsoft.com
/en-us/ai/our-approach-to-ai (visited on 02/01/2020).
[25]Government of Canada. Algorithmic Impact Assessment (AIA) - Canada.ca . 2019. URL:https:/
/www.canada.ca/en/government/system/digital-governm ent/modern-
emerging-technologies/responsible-use-ai/algorithmi c-impact-a
ssessment.html .
[26]Treasury Board of Canada Secretariat. Directive on Automated Decision-Making . Tech. rep. Trea-
sury Board of Canada Secreteriat, 2019. URL:https://www.tbs-sct.gc.ca/pol/d
oc-eng.aspx?id=32592 .
43[27]Inioluwa Deborah Raji and Jingying Yang. “ABOUT ML: Annotat ion and Benchmarking on Un-
derstanding and Transparency of Machine Learning Lifecycl es”. In: arXiv (Dec. 2019). arXiv:
1912.06166 .URL:http://arxiv.org/abs/1912.06166 .
[28]Margaret Mitchell et al. “Model Cards for Model Reporting”. In:FAT ’19: Proceedings of the Confer-
ence on Fairness, Accountability, and Transparency . Oct. 2019. DOI:10.1145/3287560.3287596 .
arXiv:1810.03993 .URL:http://arxiv.org/abs/1810.03993%20http://dx
.doi.org/10.1145/3287560.3287596 .
[29]Timnit Gebru et al. “Datasheets for Datasets”. In: arXiv (Mar. 2018). arXiv: 1803.09010 .URL:
http://arxiv.org/abs/1803.09010 .
[30]Cullen O’Keefe et al. “The Windfall Clause: Distributing th e Beneﬁts of AI for the Common Good”.
In:arXiv (Dec. 2019). arXiv: 1912.11595 .URL:http://arxiv.org/abs/1912.11
595.
[31]James M. Buchanan. “An Economic Theory of Clubs”. In: Economica 32.125 (Feb. 1965), p. 1.
ISSN: 00130427. DOI:10.2307/2552442 .URL:https://www.jstor.org/stabl
e/2552442?seq=1 .
[32]ETSI. Intellectual Property Rights .URL:https://www.etsi.org/intellectual-pr
operty-rights (visited on 02/01/2020).
[33]International Trade Forum. “Voluntary standards in develo ping Countries: The potential of vol-
untary standards and their role in international trade”. In : (Mar. 2010). URL:http://www.t
radeforum.org/Voluntary-Standards-in-Developing-Cou ntries-The
-Potential-of-Voluntary-Standards-and-their-Role-in -Inter%20n
ational-Trade/ .
[34]Future of Life Institute. AI Principles . 2017. URL:https://futureoflife.org/ai-pr
inciples/?cn-reloaded=1 (visited on 02/01/2020).
[35]Andy Pasztor and Alison Sider. Internal Boeing Documents Show Cavalier Attitude to Safety . 2020.
URL:https://www.wsj.com/articles/internal-boeing-docume nts-sho
w-cavalier-attitude-to-safety-11578627206 (visited on 02/06/2020).
[36]Inioluwa Deborah Raji et al. “Closing the AI Accountability Gap: Deﬁning an End-to-End Frame-
work for Internal Algorithmic Auditing”. In: arXiv (Jan. 2020). arXiv: 2001.00973 .URL:ht
tp://arxiv.org/abs/2001.00973 .
[37]David Engstrom and Daniel Ho. “Algorithmic Accountability in the Administrative State”. In:
CSAS Working Papers (Nov. 2019). URL:https://administrativestate.gmu.edu
/wp-content/uploads/sites/29/2019/11/Engstrom-Ho-Al gorithmic-
Accountability-in-the-Administrative-State.pdf .
[38]Jan Barton. “Who Cares about Auditor Reputation?” In: Contemporary Accounting Research 22.3
(Oct. 2005), pp. 549–586. ISSN: 0823-9150. DOI:10.1506/C27U-23K8-E1VL-20R0 .
URL:http://doi.wiley.com/10.1506/C27U-23K8-E1VL-20R0 .
[39]Ryan Carrier. “Governance and Oversight Coming to AI and Aut omation: Independent Audit of AI
Systems”. In: ACM blog (Feb. 2019). URL:https://cacm.acm.org/blogs/blog-cac
m/234723-governance-and-oversight-coming-to-ai-and- automation
-independent-audit-of-ai-systems/fulltext .
[40]Jesse Hempel. “Want to Prove Your Business is Fair? Audit You r Algorithm”. In: (May 2018). URL:
https://www.wired.com/story/want-to-prove-your-busi ness-is-fa
ir-audit-your-algorithm .
44[41]Oren Etzioni and Michael Li. “Want to Prove Your Business is F air? Audit Your Algorithm”. In:
(July 2019). URL:https://www.wired.com/story/ai-needs-to-be-audite
d.
[42]Jack Clark and Gillian K. Hadﬁeld. “Regulatory Markets for A I Safety”. In: arXiv (Dec. 2019).
arXiv:2001.00078 .URL:http://arxiv.org/abs/2001.00078 .
[43]Andrew Marshall et al. “Threat Modeling AI /ML Systems and Dependencies”. In: (Nov. 2019).
URL:https://docs.microsoft.com/en-us/security/threat-mo deling-
aiml .
[44]Ariel Herbert-Voss. Dont Red Team AI Like a Chump . 2019. URL:https://www.youtube.
com/watch?v=ivlc1yPw76A (visited on 02/01/2020).
[45]Hackerone. The Hacker-Powered Security Report 2017 . Tech. rep. Hackerone, 2017. URL:https
://www.hackerone.com/sites/default/files/2017-06/Th e%20Hacker-
Powered%20Security%20Report.pdf .
[46]J. B. Rubinovitz. Bias Bounty Programs as a Method of Combatting Bias in AI . 2018. URL:https:
//rubinovitz.com/post/187669528132/bias-bounty-prog rams-as-a-
method-of-combatting (visited on 02/01/2020).
[47]Ziad Obermeyer et al. “Dissecting racial bias in an algorith m used to manage the health of
populations”. In: Science 366.6464 (2019), pp. 447–453. ISSN: 0036-8075. URL:https://s
cience.sciencemag.org/content/366/6464/447 .
[48]Taylor Telford. Apple Card algorithm sparks gender bias allegations against Goldman Sachs . Nov.
2019. URL:https://www.washingtonpost.com/business/2019/11/11/ app
le-card-algorithm-sparks-gender-bias-allegations-ag ainst-gold
man-sachs/ (visited on 02/06/2020).
[49]Dario Amodei et al. “Concrete Problems in AI Safety”. In: arXiv (June 2016). arXiv: 1606.06565 .
URL:http://arxiv.org/abs/1606.06565 .
[50]Warwick Ashford. Bug bounties not a silver bullet, Katie Moussouris warns . 2018. URL:https:/
/www.computerweekly.com/news/252450337/Bug-bounties -not-a-sil
ver-bullet-Katie-Moussouris-warns (visited on 02/08/2020).
[51]David Dao. Awful AI .URL:https://github.com/daviddao/awful-ai (visited on
02/01/2020).
[52]Victoria Krakovna. “Speciﬁcation gaming examples in AI”. I n: (Apr. 2018). URL:https://vkr
akovna.wordpress.com/2018/04/02/specification-gamin g-examples
-in-ai/ .
[53]Peter Lee. “Learning from Tay’s introduction”. In: (Mar. 20 16). URL:https://blogs.mic
rosoft.com/blog/2016/03/25/learning-tays-introducti on/.
[54]Thomas Dietterich. “Robust Artiﬁcial Intelligence and Rob ust Human Organizations”. In: arXiv
(Mar. 2018). URL:https://arxiv.org/abs/1811.10840 .
[55]Aviv Ovadya and Jess Whittlestone. “Reducing malicious use of synthetic media research: Con-
siderations and potential release practices for machine le arning”. In: arXiv (July 2019). arXiv:
1907.11274 .URL:http://arxiv.org/abs/1907.11274 .
[56]Jan Leike et al. “AI Safety Gridworlds”. In: arXiv (Nov. 2017). arXiv: 1711.09883 .URL:htt
p://arxiv.org/abs/1711.09883 .
45[57]Christian Szegedy et al. “Intriguing properties of neural n etworks”. In: 2nd International Confer-
ence on Learning Representations, ICLR 2014 - Conference Track Proc eedings . International Con-
ference on Learning Representations, ICLR, 2014. arXiv: 1312.6199 .URL:https://arx
iv.org/abs/1312.6199 .
[58]Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing adver-
sarial examples”. In: 3rd International Conference on Learning Representations, ICLR 20 15 - Con-
ference Track Proceedings . International Conference on Learning Representations, I CLR, 2015.
arXiv:1412.6572 .URL:https://arxiv.org/abs/1412.6572 .
[59]Alice Xiang and Inioluwa Deborah Raji. “On the Legal Compati bility of Fairness Deﬁnitions”. In:
arXiv (Nov. 2019). arXiv: 1912.00761 .URL:http://arxiv.org/abs/1912.0076
1.
[60]Reuben Binns. “Fairness in Machine Learning: Lessons from P olitical Philosophy”. In: arXiv (Dec.
2017). arXiv: 1712.03586 .URL:http://arxiv.org/abs/1712.03586 .
[61]Mark Koren, Anthony Corso, and Mykel Kochenderfer. “The Ada ptive Stress Testing Formula-
tion”. In: RSS 2019: Workshop on Safe Autonomy . Freiburg, 2019. URL:https://openrev
iew.net/pdf?id=rJgoNK-oaE .
[62]Ritchie Lee et al. “Adaptive stress testing of airborne coll ision avoidance systems”. In: AIAA/IEEE
Digital Avionics Systems Conference - Proceedings . Institute of Electrical and Electronics Engineers
Inc., Oct. 2015. ISBN: 9781479989409. DOI:10.1109/DASC.2015.7311613 .URL:htt
ps://ieeexplore.ieee.org/document/7311613/versions .
[63]N. V . Chawla et al. “SMOTE: Synthetic Minority Over-samplin g Technique”. In: Journal of Artiﬁ-
cial Intelligence Research (June 2002). DOI:10.1613/jair.953 . arXiv:1106.1813 .URL:
http://arxiv.org/abs/1106.1813%20http://dx.doi.org/ 10.1613/jai
r.953 .
[64]Guillaume Lemaître, Fernando Nogueira, and Christos K. Ari das. “Imbalanced-learn: A Python
Toolbox to Tackle the Curse of Imbalanced Datasets in Machin e Learning”. In: Journal of Machine
Learning Research 7 (2016), pp. 1–5. arXiv: 1609.06570v1 .URL:http://www.jmlr.o
rg/papers/volume18/16-365/16-365.pdf .
[65]Aleksander Madry et al. “Towards Deep Learning Models Resis tant to Adversarial Attacks”. In:
arXiv (June 2017). arXiv: 1706.06083 .URL:http://arxiv.org/abs/1706.0608
3.
[66]Matthias Hein and Maksym Andriushchenko. “Formal Guarante es on the Robustness of a Clas-
siﬁer against Adversarial Manipulation”. In: arXiv (May 2017). arXiv: 1705.08475 .URL:ht
tp://arxiv.org/abs/1705.08475 .
[67]Mathias Lecuyer et al. “Certiﬁed Robustness to Adversarial Examples with Differential Privacy”.
In:arXiv (Feb. 2018). arXiv: 1802.03471 .URL:http://arxiv.org/abs/1802.03
471.
[68]Jeremy M Cohen, Elan Rosenfeld, and J. Zico Kolter. “Certiﬁe d Adversarial Robustness via Ran-
domized Smoothing”. In: arXiv (Feb. 2019). arXiv: 1902.02918 .URL:http://arxiv.o
rg/abs/1902.02918 .
[69]Guy Katz et al. “Reluplex: An efﬁcient smt solver for verifyi ng deep neural networks”. In: Lec-
ture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and
Lecture Notes in Bioinformatics) . Vol. 10426 LNCS. Springer Verlag, 2017, pp. 97–117. ISBN:
9783319633862. DOI:10.1007/978-3-319-63387-9_5 . arXiv:1702.01135 .URL:
https://arxiv.org/abs/1702.01135 .
46[70]Kevin Bonsor and Nathan Chandler. How Black Boxes Work .URL:https://science.h
owstuffworks.com/transport/flight/modern/black-box. htm (visited on
02/02/2020).
[71]Mustafa Suleyman and Ben Laurie. Trust, conﬁdence and Veriﬁable Data Audit . 2017. URL:https
://deepmind.com/blog/article/trust-confidence-verif iable-data
-audit (visited on 02/02/2020).
[72]Joanna Bryson. “AI & Global Governance: No One Should Trust A I”. In: (Nov. 2018). URL:http
s://cpr.unu.edu/ai-global-governance-no-one-should- trust-ai.h
tml.
[73]Cynthia Rudin. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions
and Use Interpretable Models Instead”. In: arXiv (Nov. 2018). arXiv: 1811.10154 .URL:htt
p://arxiv.org/abs/1811.10154 .
[74]Zachary C. Lipton. “The Mythos of Model Interpretability”. In:arXiv (June 2016). arXiv: 1606.03490 .
URL:http://arxiv.org/abs/1606.03490 .
[75]Mukund Sundararajan, Ankur Taly, and Qiqi Yan. “Axiomatic A ttribution for Deep Networks”.
In:arXiv (Mar. 2017). arXiv: 1703.01365 .URL:http://arxiv.org/abs/1703.01
365.
[76]Mark Sendak et al. “"The Human Body is a Black Box": Supportin g Clinical Decision-Making with
Deep Learning”. In: arXiv (Nov. 2019). arXiv: 1911.08089 .URL:http://arxiv.org/
abs/1911.08089 .
[77]Berk Ustun, Alexander Spangher, and Yang Liu. “Actionable R ecourse in Linear Classiﬁcation”.
In:FAT ’19: Proceedings of the Conference on Fairness, Accountabilit y, and Transparency . Sept.
2019. DOI:10.1145/3287560.3287566 . arXiv:1809.06514 .URL:http://arxiv
.org/abs/1809.06514%20http://dx.doi.org/10.1145/328 7560.328756
6.
[78]Forough Poursabzi-Sangdeh et al. “Manipulating and Measur ing Model Interpretability”. In:
arXiv (Feb. 2018). arXiv: 1802.07810 .URL:http://arxiv.org/abs/1802.0781
0.
[79]Neil Vigdor. Apple Card Investigated After Gender Discrimination Complaints - T he New York Times .
2019. URL:https://www.nytimes.com/2019/11/10/business/Apple-c red
it-card-investigation.html (visited on 02/02/2020).
[80]David Bau et al. “Semantic photo manipulation with a generat ive image prior”. In: ACM Transac-
tions on Graphics 38.4 (2019). ISSN: 15577368. DOI:10.1145/3306346.3323023 .URL:h
ttp://ganpaint.io/Bau%7B%5C_%7Det%7B%5C_%7Dal%7B%5C _%7DSemanti
c%7B%5C_%7DPhoto%7B%5C_%7DManipulation%7B%5C_%7Dpre %20print.pd
f.
[81]Fred Hohman et al. “Gamut: A design probe to understand how da ta scientists understand ma-
chine learning models”. In: Conference on Human Factors in Computing Systems - Proceedings . As-
sociation for Computing Machinery , May 2019. ISBN: 9781450359702. URL:https://www.m
icrosoft.com/en-us/research/uploads/prod/2019/01/19 %7B%5C_%7Dg
amut%7B%5C_%7Dchi.pdf .
[82]Isaac Lage et al. “An Evaluation of the Human-Interpretabil ity of Explanation”. In: arXiv (Jan.
2019). arXiv: 1902.00006 .URL:http://arxiv.org/abs/1902.00006 .
47[83]Berkeley J. Dietvorst, Joseph P . Simmons, and Cade Massey. “ Algorithm aversion: People erro-
neously avoid algorithms after seeing them err”. In: Journal of Experimental Psychology: General
144.1 (2015), pp. 114–126. ISSN: 00963445. DOI:10.1037/xge0000033 .URL:https:/
/repository.upenn.edu/cgi/viewcontent.cgi?article=1 392%7B%5C&%
7Dcontext=fnce%7B%5C_%7Dpapers .
[84]Umang Bhatt et al. “Explainable Machine Learning in Deploym ent”. In: arXiv (Sept. 2019). URL:
https://arxiv.org/abs/1909.06342 .
[85]Julius Adebayo et al. “Sanity Checks for Saliency Maps”. In: arXiv (Oct. 2018). arXiv: 1810.03292 .
URL:http://arxiv.org/abs/1810.03292 .
[86]Aravindh Mahendran and Andrea Vedaldi. “Salient deconvolu tional networks”. In: Lecture Notes
in Computer Science (including subseries Lecture Notes in Artiﬁcia l Intelligence and Lecture Notes
in Bioinformatics) . Vol. 9910 LNCS. Springer Verlag, 2016, pp. 120–135. ISBN: 9783319464657.
DOI:10.1007/978-3-319-46466-4_8 .URL:https://www.robots.ox.ac.u
k/%7B~%7Dvedaldi/assets/pubs/mahendran16salient.pdf .
[87]Pieter-Jan Kindermans et al. “The (Un)reliability of salie ncy methods”. In: arXiv (Nov. 2017).
arXiv:1711.00867 .URL:http://arxiv.org/abs/1711.00867 .
[88]Sara Hooker et al. “A Benchmark for Interpretability Method s in Deep Neural Networks”. In:
arXiv (June 2018). arXiv: 1806.10758 .URL:http://arxiv.org/abs/1806.1075
8.
[89]Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. “BadN ets: Identifying Vulnerabilities in
the Machine Learning Model Supply Chain”. In: arXiv (Aug. 2017). arXiv: 1708.06733 .URL:
http://arxiv.org/abs/1708.06733 .
[90]Juyeon Heo, Sunghwan Joo, and Taesup Moon. “Fooling Neural N etwork Interpretations via
Adversarial Model Manipulation”. In: arXiv (Feb. 2019). arXiv: 1902.02041 .URL:http:/
/arxiv.org/abs/1902.02041 .
[91]Dylan Slack et al. “How can we fool LIME and SHAP? Adversarial Attacks on Post hoc Explanation
Methods”. In: arXiv (Nov. 2019). arXiv: 1911.02508 .URL:http://arxiv.org/abs/
1911.02508 .
[92]Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “"W hy should i trust you?" Explaining
the predictions of any classiﬁer”. In: Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining . Vol. 13-17-Augu. Association for Computing Machinery,
Aug. 2016, pp. 1135–1144. ISBN: 9781450342322. DOI:10.1145/2939672.2939778 .
arXiv:1602.04938 .URL:https://arxiv.org/abs/1602.04938 .
[93]B Dimanov et al. “You shouldn’t trust me: Learning models whi ch conceal unfairness from mul-
tiple explanation methods”. In: European Conference on Artiﬁcial Intelligence . 2020. URL:http
://ecai2020.eu/accepted-papers/ .
[94]Kamalika Chaudhuri and Claire Monteleoni. “Privacy-prese rving logistic regression”. In: Neural
Information Processing Systems Conference . 2018. URL:https://papers.nips.cc/pap
er/3486-privacy-preserving-logistic-regression .
[95]Peter Kairouz et al. “Advances and Open Problems in Federate d Learning”. In: arXiv (Dec. 2019).
arXiv:1912.04977 .URL:http://arxiv.org/abs/1912.04977 .
[96]Nicholas Carlini et al. “The Secret Sharer: Evaluating and T esting Unintended Memorization in
Neural Networks”. In: arXiv (Feb. 2018). arXiv: 1802.08232 .URL:http://arxiv.org
/abs/1802.08232 .
48[97]H. Brendan McMahan et al. “Learning Differentially Private Recurrent Language Models”. In:
arXiv (Oct. 2017). arXiv: 1710.06963 .URL:http://arxiv.org/abs/1710.0696
3.
[98]Cynthia Dwork et al. “Calibrating noise to sensitivity in pr ivate data analysis”. In: Lecture Notes
in Computer Science (including subseries Lecture Notes in Artiﬁcia l Intelligence and Lecture Notes
in Bioinformatics) . Vol. 3876 LNCS. 2006, pp. 265–284. ISBN: 3540327312. URL:https://p
eople.csail.mit.edu/asmith/PS/sensitiv%20ity-tcc-fi nal.pdf .
[99]Aaron Roth and Michael Kearns. The Ethical Algorithm: The Science of Socially Aware Algorithm
Design . Oxford University Press, 2019.
[100]Rachel Cummings and Deven Desai. “The Role of Differential P rivacy in GDPR Compliance”. In:
FAT ’18: Proceedings of the Conference on Fairness, Accountabilit y, and Transparency . 2018. URL:h
ttps://piret.gitlab.io/fatrec2018/program/fatrec201 8-cummings.
pdf.
[101]Mihailo Isakov et al. “Survey of Attacks and Defenses on Edge -Deployed Neural Networks”. In:
arXiv (Nov. 2019). arXiv: 1911.11932 .URL:http://arxiv.org/abs/1911.1193
2.
[102]Nathan Dowlin et al. “CryptoNets: Applying Neural Networks to Encrypted Data with High
Throughput and Accuracy”. In: Proceedings of the 33rd International Conference on Machine
Learning . 2016. URL:http://sealcrypto.codeplex.com .
[103]Tim Hwang. “Computational Power and the Social Impact of Art iﬁcial Intelligence”. In: SSRN
Electronic Journal (Apr. 2018). DOI:10.2139/ssrn.3147971 . arXiv:1803.08971 .URL:
https://arxiv.org/abs/1803.08971 .
[104]Rich Sutton. The Bitter Lesson . 2019. URL:http://www.incompleteideas.net/Inc
Ideas/BitterLesson.html (visited on 02/02/2020).
[105]Rodney Brooks. A Better Lesson . 2019. URL:https://rodneybrooks.com/a-better
-lesson/ (visited on 02/02/2020).
[106]Huili Chen et al. “DeepAttest: An end-to-end attestation fr amework for deep neural networks”.
In:Proceedings of the International Symposium on Computer Architec ture. Institute of Electrical
and Electronics Engineers Inc., June 2019, pp. 487–498. ISBN: 9781450366694. URL:https:/
/www.microsoft.com/en-us/research/uploads/prod/2019 /05/DeepAtt
est.pdf .
[107]Zahra Ghodsi, Tianyu Gu, and Siddharth Garg. “SafetyNets: V eriﬁable Execution of Deep Neu-
ral Networks on an Untrusted Cloud”. In: Conference on Neural Information Processing Systems .
2017. URL:https://papers.nips.cc/paper/7053-safetynets-verifi abl
e-execution-of-deep-neural-networks-on-an-untrusted -cloud.pdf .
[108]Seyyed Ahmad Javadi et al. “Monitoring Misuse for Accountab le ’Artiﬁcial Intelligence as a Ser-
vice’”. In: arXiv (Jan. 2020). arXiv: 2001.09723 .URL:http://arxiv.org/abs/200
1.09723 .
[109]Frank Mckeen et al. “Innovative Instructions and Software M odel for Isolated Execution”. In:
HASP ’13: Proceedings of the 2nd International Workshop on Hard ware and Architectural Support
for Security and Privacy . 2013. DOI:10.1145/2487726.2488368 .URL:https://www.
eit.lth.se/fileadmin/eit/courses/eitn50/Literature/ hasp-2013-
innovative-instructions-and-software-model-for-isol ated-execu
tion.pdf .
49[110]Jo Van Bulck et al. “Foreshadow: Extracting the Keys to the In tel{SGX}Kingdom with Tran-
sient Out-of-Order Execution”. In: 27th{USENIX}Security Symposium ( {USENIX}Security 18) .
Baltimore, MD:{USENIX}Association, Aug. 2018, 991 {\textendash}1008. ISBN: 978-1-939133-
04-5. URL:https://www.usenix.org/conference/usenixsecurity18/ pre
sentation/bulck .
[111]Mark Seaborn and Thomas Dullien. Exploiting the DRAM rowhammer bug to gain kernel privileges
How to cause and exploit single bit errors . 2015. URL:https://www.blackhat.com/doc
s/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM -Rowhammer
-Bug-To-Gain-Kernel-Privileges.pdf (visited on 02/02/2020).
[112]Nick Hynes, Raymond Cheng, and Dawn Song. “Efﬁcient Deep Lea rning on Multi-Source Private
Data”. In: arXiv (July 2018). arXiv: 1807.06689 .URL:http://arxiv.org/abs/180
7.06689 .
[113]Christian Priebe et al. “SGX-LKL: Securing the Host OS Inter face for Trusted Execution”. In: arXiv
(Aug. 2019). arXiv: 1908.11143 .URL:http://arxiv.org/abs/1908.11143 .
[114]Ehsan Hesamifard et al. “Privacy-preserving Machine Learn ing as a Service”. In: Proceedings on
Privacy Enhancing Technologies . 2018, pp. 123–142. arXiv: 1803.05961 .URL:https://pe
tsymposium.org/2018/files/papers/issue3/popets-2018 -0024.pdf .
[115]Stavros Volos, Kapil Vaswani, and Rodrigo Bruno. “Graviton : Trusted Execution Environments on
GPUs”. In: Proceedings of the 12th USENIX Conference on Operating Systems Desi gn and Implemen-
tation . OSDI’18. USA: USENIX Association, 2018, pp. 681–696. ISBN: 9781931971478. URL:ht
tps://www.microsoft.com/en-us/research/uploads/prod /2018/09/Gr
aviton-Trusted-Execution-Environments-on-GPUs.pdf .
[116]Florian Tramèr and Dan Boneh. “Slalom: Fast, Veriﬁable and P rivate Execution of Neural Net-
works in Trusted Hardware”. In: arXiv (June 2018). arXiv: 1806.03287 .URL:http://ar
xiv.org/abs/1806.03287 .
[117]Zhongshu Gu et al. “YerbaBuena: Securing Deep Learning Infe rence Data via Enclave-based
Ternary Model Partitioning”. In: arXiv (July 2018). arXiv: 1807.00969 .URL:http://arx
iv.org/abs/1807.00969 .
[118]Dario Amodei et al. AI and Compute . 2018. URL:https://openai.com/blog/ai-and
-compute/ (visited on 02/02/2020).
[119]Lily Hay Newman. Google Is Helping Design an Open Source, Ultra-Secure Chip | WIRED . 2019.
URL:https://www.wired.com/story/open-titan-open-source- secure-
enclave/ (visited on 02/02/2020).
[120]Cisco. Cisco Global Cloud Index: Forecast and Methodology, 2016-2021 Whi te Paper . Tech. rep.
Cisco, 2018. URL:https://www.cisco.com/c/en/us/solutions/collatera
l/service-provider/global-cloud-index-gci/white-pap er-c11-738
085.html .
[121]Roy Schwartz et al. “Green AI”. In: arXiv (July 2019). arXiv: 1907.10597 .URL:http://a
rxiv.org/abs/1907.10597 .
[122]Christopher Berner et al. “Dota 2 with Large Scale Deep Reinf orcement Learning”. In: arXiv (Dec.
2019). arXiv: 1912.06680 .URL:http://arxiv.org/abs/1912.06680 .
50[123]J Vitek and T Kalibera. “Repeatability , reproducibility an d rigor in systems research”. In: 2011
Proceedings of the Ninth ACM International Conference on Embedded S oftware (EMSOFT) . Oct.
2011, pp. 33–38. DOI:10.1145/2038642.2038650 .URL:https://ieeexplore.
ieee.org/document/6064509 .
[124]T Hoeﬂer and R Belli. “Scientiﬁc benchmarking of parallel co mputing systems: twelve ways to
tell the masses when reporting performance results”. In: SC ’15: Proceedings of the International
Conference for High Performance Computing, Networking, Storag e and Analysis . Nov. 2015, pp. 1–
12. DOI:10.1145/2807591.2807644 .URL:https://htor.inf.ethz.ch/pub
lications/img/hoefler-scientific-benchmarking.pdf .
[125]Steve Lohr. At Tech’s Leading Edge, Worry About a Concentration of Power . 2019. URL:https://
www.nytimes.com/2019/09/26/technology/ai-computer-e xpense.htm
l(visited on 02/02/2020).
[126]Martijn Rasser et al. The American AI Century: A Blueprint for Action . Tech. rep. Center for a New
American Security, Dec. 2019. URL:https://www.cnas.org/publications/repo
rts/the-american-ai-century-a-blueprint-for-action .
[127]John Etchemendy and Fei-Fei Li. “National Research Cloud: E nsuring the Continuation of Amer-
ican Innovation”. In: (Mar. 2020). URL:https://hai.stanford.edu/news/natio
nal-research-cloud-ensuring-continuation-american-i nnovation .
[128]Susanne Barth and Menno D.T . de Jong. The privacy paradox - Investigating discrepancies between
expressed privacy concerns and actual online behavior - A systemati c literature review . Nov. 2017.
DOI:10.1016/j.tele.2017.04.013 .URL:https://www.sciencedirect.c
om/science/article/pii/S0736585317302022 .
[129]Ehsan Toreini et al. “The relationship between trust in AI an d trustworthy machine learning
technologies”. In: arXiv (Nov. 2019). arXiv: 1912.00782 .URL:http://arxiv.org/ab
s/1912.00782 .
[130]Mike Ananny and Kate Crawford. “Seeing without knowing: Lim itations of the transparency
ideal and its application to algorithmic accountability”. In:New Media & Society 20.3 (Mar.
2018), pp. 973–989. ISSN: 1461-4448. DOI:10.1177/1461444816676645 .URL:htt
p://journals.sagepub.com/doi/10.1177/14614448166766 45.
[131]Joshua A. Kroll et al. “Accountable Algorithms”. In: University of Pennsylvania Law Review 165
(2017). URL:https://papers.ssrn.com/sol3/papers.cfm?abstract%7B %
5C_%7Did=2765268%7B%5C#%7D%7B%5C#%7D .
[132]Partnership on AI. “Human-AI Collaboration Trust Literatu re Review - Key Insights and Bibliogra-
phy”. In: (Sept. 2019). URL:https://www.partnershiponai.org/human-ai-co
llaboration-trust-literature-review-key-insights-an d-bibliogr
aphy/ .
[133]High-Level Expert Group on AI. Ethics guidelines for trustworthy AI . Tech. rep. High-Level Expert
Group on AI, 2019. URL:https://ec.europa.eu/digital-single-market/en
/news/ethics-guidelines-trustworthy-ai .
[134]Google PAIR. Explainability+Trust .URL:https://pair.withgoogle.com/chapte
r/explainability-trust/ (visited on 02/05/2020).
[135]Matthew Arnold et al. “FactSheets: Increasing Trust in AI Se rvices through Supplier’s Declara-
tions of Conformity”. In: arXiv (Aug. 2018). arXiv: 1808.07261 .URL:http://arxiv.o
rg/abs/1808.07261 .
51[136]Roger C Mayer, James H Davis, and F David Schoorman. “An Integ rative Model of Organizational
Trust”. In: The Academy of Management Review 20.3 (1995), pp. 709–734. ISSN: 03637425. URL:
http://www.jstor.org/stable/258792 .
[137]D Gambetta. Trust: Making and Breaking Cooperative Relations . B. Blackwell, 1988. ISBN: 978063
1155065. URL:https://books.google.com.tr/books?id=97VmQgAACAAJ .
[138]ANDREW J COE and JANE VAYNMAN. “Why Arms Control Is So Rare”. I n:American Political Sci-
ence Review (2019), pp. 1–14. DOI:10.1017/S000305541900073X .URL:https://ww
w.cambridge.org/core/journals/american-political-sc ience-revi
ew/article/why-arms-control-is-so-rare/BAC79354627F 72CDDD%20B
102FE82889B8A .
[139]Suzanne Massie and Nicholas (ed.) Daniloff. “Suzanne Massi e, Trust but Verify: Reagan, Russia,
and Me. Rockland, ME: Maine Authors Publishing, 2013. 380 pp .” In: Journal of Cold War Studies
18.4 (2016), pp. 225–228. DOI:10.1162/JCWS_r_00693 .URL:https://doi.org/
10.1162/JCWS%7B%5C_%7Dr%7B%5C_%7D00693 .
[140]Robin Bloomﬁeld et al. “Disruptive Innovations and Disrupt ive Assurance: Assuring Machine
Learning and Autonomy”. In: Computer 52.9 (Sept. 2019), pp. 82–89. ISSN: 15580814. DOI:
10.1109/MC.2019.2914775 .URL:https://openaccess.city.ac.uk/id/e
print/23295/1/bloomfield%7B%5C_%7DpreIEEE%7B%5C_%7D assuring%7B
%5C_%7Dautonomy%7B%5C_%7Dv01i.pdf .
[141]Ofﬁce for Nuclear Regulation (ONR). ONR Guide . Tech. rep. Ofﬁce for Nuclear Regulation, 2019.
URL:http://www.onr.org.uk/operational/tech%7B%5C_%7Dass t%7B%5C
_%7Dguides/ns-tast-gd-051.pdf .
[142]Uber. Uber ATG Safety Case . 2019. URL:https://uberatg.com/safetycase/ (visited
on 02/01/2020).
[143]Xingyu Zhao et al. “A Safety Framework for Critical Systems U tilising Deep Neural Networks”.
In:arXiv (Mar. 2020). URL:https://arxiv.org/abs/2003.05311 .
[144]Stephen Edelston Toulmin, joint author Rieke Richard D., an d joint author Janik Allan. An in-
troduction to reasoning . New York Macmillan, 1979. ISBN: 0024210307. URL:http://open
library.org/books/OL4723061M .
[145]Donald G. (ed) Brennan. Arms Control, Disarmament, and National Security . G. Braziller, 1961.
URL:https://www.jstor.org/stable/pdf/20025353.pdf?seq=1 .
[146]Bernard Brodie. “HEDLEY BULL. The Control of the Arms Race: D isarmament and Arms Control
in the Missile Age. (Studies in Inter national Security , II. ) Pp. 215. New York: Frederick A. Praeger
for the Institute for Strategic Studies, 1961. $3.95”. In: The ANNALS of the American Academy of
Political and Social Science 341.1 (1962), pp. 115–116. DOI:10.1177/000271626234100116 .
URL:https://doi.org/10.1177/000271626234100116 .
[147]Richard Dean Burns. Encyclopedia of arms control and disarmament . Encyclopedia of Arms Con-
trol and Disarmament v. 1-3. Scribner’s, 1993. ISBN: 9780684192819. URL:https://book
s.google.com.tr/books?id=f42yAAAAIAAJ .
[148]Richard Dean Burns. The Evolution of Arms Control: From Antiquity to the Nuclear Age . Rowman &
Littleﬁeld, 2013, p. 264. ISBN: 1442223790. URL:https://rowman.com/ISBN/97814
42223790/The-Evolution-of-Arms-Control-From-Antiqui ty-to-the-
Nuclear-Age .
52[149]Jozef Goldblat. Agreements for Arms Control: A Critical Survey . Taylor & Francis, 1982, p. 387.
ISBN: 0-85066-229-X. URL:https://www.sipri.org/publications/1982/agr
eements-arms-control-critical-survey .
[150]Tim Caughley . Nuclear Disarmament Veriﬁcation: Survey of Veriﬁcation Mechanism s. Tech. rep.
United Nations Institute for Disarmament Research, 2016, p . 48. URL:https://unidir.or
g/publication/nuclear-disarmament-verification-surv ey-verif%2
0ication-mechanisms .
[151]Nancy W . Gallagher. The politics of veriﬁcation . Johns Hopkins University Press, 2003, p. 311.
ISBN: 9780801877391. URL:https://www.tandfonline.com/doi/abs/10.108
0/13523269708404165 .
[152]John H. Herz. “Political Realism and Political Idealism: A S tudy in Theories and Realities”.
In:American Political Science Review 46.2 (June 1952), pp. 550–552. ISSN: 0003-0554. DOI:
10.2307/1950848 .URL:https://www.cambridge.org/core/journals/am
erican-political-science-review/article/political-r ealism-and
-political-idealism-a-study-in-theories-and-realiti es-by-john
-h-herz-chicago-university-of-chicago-press-1951-pp -xii-275-3
75/8DA0D7344AD6FC86873EA5A4935 .
[153]Robert Jervis. “Cooperation Under the Security Dilemma”. I n:World Politics 30.2 (1978), pp. 167–
214. ISSN: 00438871, 10863338. URL:http://www.jstor.org/stable/2009958 .
[154]Robert Jervis. “Arms Control, Stability , and Causes of War” . In:Political Science Quarterly 108.2
(1993), pp. 239–253. ISSN: 00323195. URL:http://www.jstor.org/stable/2152
010.
[155]Organisation for the Prohibition of Chemical Weapons. Chemical Weapons Convention . Tech. rep.
Organisation for the Prohibition of Chemical Weapons, 1993 .URL:https://www.opcw.o
rg/chemical-weapons-convention .
[156]Frederic Joseph Brown. Chemical Warfare: A Study in Restraints . Princeton University Press,
1968, p. 376. ISBN: 9780691069012. URL:http://www.jstor.org/stable/j.ctt
183ph9d .
[157]Carol Barner-Barry. “The Diplomacy of Biological Disarmam ent: Vicissitudes of a Treaty in Force”.
In:Politics and the Life Sciences 9.1 (1990), pp. 149–149. DOI:10.1017/S0730938400010352 .
URL:https://www.cambridge.org/core/journals/politics-an d-the-l
ife-sciences/article/diplomacy-of-biological-disarm ament-vici
ssitudes-of-a-treaty-in-force-nicholas-a-sims-new-y ork-st-mar
tins-press1988/58994CAE4E3EF4EEF484F5313FCAD83E .
[158]Kenneth R Rutherford. “The Evolving Arms Control Agenda: Im plications of the Role of NGOs in
Banning Antipersonnel Landmines”. In: World Politics 53.1 (2000), pp. 74–114. ISSN: 00438871,
10863338. URL:http://www.jstor.org/stable/25054137 .
[159]United Nations. Convention on the Prohibition of the Use, Stockpiling, Product ion and Transfer of
Anti-Personnel Mines and on their Destruction . Oslo, 1997. URL:https://treaties.un.o
rg/Pages/ViewDetails.aspx?src=IND%7B%5C&%7Dmtdsg%7B %5C_%7Dno=X
XVI-5%7B%5C&%7Dchapter=26%7B%5C&%7Dclang=%7B%5C_%7D en.
[160]United Nations. The Convention on Cluster Munitions . 2008. URL:https://www.cluster
c%20onvention.org/ .
53[161]Heather Roff and Richard Moyes. “Meaningful Human Control, Artiﬁcial Intelligence and Au-
tonomous Weapons”. In: Informal Meeting of Experts on Lethal Autonomous Weapons Systems,
UN Convention on Certain Conventional Weapons . 2016. URL:http://www.article36.o
rg/wp-content/uploads/2016/04/MHC-AI-and-AWS-FINAL. pdf.
[162]Atomic Heritage Foundation. Non-Proliferation, Limitation, and Reduction . 2017. URL:https:/
/www.atomicheritage.org/history/non-proliferation-l imitation-
and-reduction (visited on 02/02/2020).
[163]United States and USSR. Treaty On The Limitation of Anti-Ballistic Missile Systems . 1972. URL:ht
tps://www.nti.org/learn/treaties-and-regimes/treaty -limitat%2
0ion-anti-ballistic-missile-systems-abm-treaty/ .
[164]Rebecca Crootof. “The Killer Robots Are Here: Legal and Poli cy Implications”. In: Cardozo law
review 36 (2015), p. 1837. URL:https://papers.ssrn.com/sol3/papers.cfm?
abstract%7B%5C_%7Did=2534567 .
[165]Sean Watts. “Autonomous Weapons: Regulation Tolerant or Re gulation Resistant?” In: SSRN
Electronic Journal (Nov. 2015). DOI:10.2139/ssrn.2681283 .URL:https://paper
s.ssrn.com/sol3/papers.cfm?abstract%7B%5C_%7Did=268 1283 .
[166]Paul Scharre. Army of None . W . W . Norton & Company, 2019, p. 448. URL:https://wwnor
ton.com/books/Army-of-None/ .
[167]Kenneth Payne. “Artiﬁcial Intelligence: A Revolution in St rategic Affairs?” In: Survival 60.5
(2018), pp. 7–32. DOI:10.1080/00396338.2018.1518374 .URL:https://doi
.org/10.1080/00396338.2018.1518374 .
[168]Miles Brundage et al. “The Malicious Use of Artiﬁcial Intell igence: Forecasting, Prevention, and
Mitigation”. In: arXiv (Feb. 2018). arXiv: 1802.07228 .URL:http://arxiv.org/abs
/1802.07228 .
[169]International Committe of the Red Cross. Report of the ICRC Expert Meeting on ’Autonomous
weapon systems: technical, military, legal and humanitarian aspect s’. Tech. rep. The Red Cross,
2014. URL:https://www.icrc.org/en/doc/assets/files/2014/exper t-m
eeting-autonomous-weapons-icrc-report-2014-05-09.pd f.
[170]Nehal Bhuta et al., eds. Autonomous weapons systems : law, ethics, policy . Cambridge University
Press, 2016, p. 410. ISBN: 9781316607657. URL:https://www.cambridge.org/tr/a
cademic/subjects/law/humanitarian-law/autonomous-we apons-syst
ems-law-ethics-policy?format=PB .
[171]Greg Allen and Taniel Chan. Artiﬁcial Intelligence and National Security . Tech. rep. Belfer Center
for Science and International Affairs, Harvard Kennedy Sch ool, 2017. URL:https://www.be
lfercenter.org/publication/artificial-intelligence- and-nati%2
0onal-security .
[172]Andrew Imbrie and Elsa B. Kania. AI Safety, Security, and Stability Among Great Powers . Tech. rep.
Center for Security and Emerging Technology , 2019, p. 25. URL:https://cset.georget
own.edu/wp-content/uploads/AI-Safety-Security-and-S tability-A
mong-the-Great-Powers.pdf .
[173]Richard Moyes. “Key elements of meaningful human control”. In:Informal Meeting of Experts
on Lethal Autonomous Weapons Systems, UN Convention on Certain Co nventional Weapons . 2016.
URL:http://www.article36.org/wp-content/uploads/2016/04 /MHC-20
16-FINAL.pdf .
54[174]Heather Roff. “Autonomous weapon systems: Evaluating the c apacity for ’meaningful human
control’ in weapon review processes”. In: Convention on Certain Conventional Weapons Group of
Governmental Experts meeting on Lethal Autonomous Weapons Systems . 2017. URL:http://ww
w.article36.org/wp-content/uploads/2013/06/Evaluati ng-human-c
ontrol-1.pdf .
[175]Haydn Belﬁeld. “Activism by the AI Community: Analysing Rec ent Achievements and Future
Prospects”. In: arXiv (Jan. 2020). arXiv: 2001.06528 .URL:http://arxiv.org/abs/
2001.06528 .
[176]Charli Carpenter. "Lost" Causes: Agenda Vetting in Global Issue Networks and the Sha ping of Human
Security . Cornell University Press, 2014. ISBN: 9780801448850. URL:http://www.jstor
.org/stable/10.7591/j.ctt5hh0r5 .
[177]Serif Onur Bahcecik. “Civil Society Responds to the AWS: Gro wing Activist Networks and Shifting
Frames”. In: Global Policy 10.3 (2019), pp. 365–369. DOI:10.1111/1758-5899.12671 .
URL:https://onlinelibrary.wiley.com/doi/abs/10.1111/175 8-5899.
12671 .
[178]Maaike Verbruggen. “The Role of Civilian Innovation in the D evelopment of Lethal Autonomous
Weapon Systems”. In: Global Policy 10.3 (2019), pp. 338–342. URL:https://onlinelib
rary.wiley.com/doi/abs/10.1111/1758-5899.12663 .
[179]Kerstin Vignard. Manifestos and open letters: Back to the future? 2018. URL:https://thebul
l%20etin.org/2018/04/manifestos-and-open-letters-ba ck-to-the-
future/ (visited on 02/03/2020).
[180]Kara Frederick. The civilian private sector: part of a new arms control regime? 2019. URL:https:
//www.orfonline.org/expert-speak/the-civilian-priva te-sector-
part-of-a-new-arms-control-regime-57345/ (visited on 02/03/2020).
[181]Future of Life Institute. Open Letter on Autonomous Weapons . Tech. rep. Future of Life Insti-
tute, 2015. URL:https://futureoflife.org/open-letter-autonomous-we
apons/ .
[182]Emanuel Adler. “The emergence of cooperation: national epi stemic communities and the in-
ternational evolution of the idea of nuclear arms control”. In:International Organization 46.1
(1992), pp. 101–145. DOI:10.1017/S0020818300001466 .URL:https://www.cam
bridge.org/core/journals/international-organization /article/em
ergence-of-cooperation-national-epistemic-communiti es-and-the
-international-evolution-of-the-idea-of-nuclear-arm s-control/
AD5AB338380EC8691C621B351BC11CE3 .
[183]Peter M. Haas. Introduction: Epistemic Communities and International Policy Coordination . 1992.
DOI:10.2307/2706951 .URL:https://www.jstor.org/stable/2706951 .
[184]Standard Oil Co. of New Jersey v. United States :: 221 U.S. 1 . May 1911. URL:https://supr
eme.justia.com/cases/federal/us/221/1/ .
[185]William E. Kovacic and Carl Shapiro. “Antitrust Policy: A Ce ntury of Economic and Legal Think-
ing”. In: Journal of Economic Perspectives 14.1 (2000), pp. 43–60. URL:https://perma.c
c/N6EF-NGD3 .
[186]B. Y. Orbach. “THE ANTITRUST CONSUMER WELFARE PARADOX”. In: Journal of Competition
Law and Economics 7.1 (Mar. 2011), pp. 133–164. ISSN: 1744-6414. URL:https://academ
ic.oup.com/jcle/article-lookup/doi/10.1093/joclec/n hq019 .
55[187]Lina M Khan. “Amazon’s Antitrust Paradox”. In: The Yale Law Journal 126 (2017), pp. 710–
805. URL:http://www.internetretailer.com/2016/ol/28/amazon-s ale
s-climb-22 .
[188]Astead W . Herndon. Elizabeth Warren Proposes Breaking Up Tech Giants Like Amazon and F ace-
book. 2019. URL:https://www.nytimes.com/2019/03/08/us/politics/eli
zabeth-warren-amazon.html (visited on 02/03/2020).
[189]Arizona v. Maricopa County Med. Soc’y :: 457 U.S. 332 . June 1982. URL:https://supreme
.justia.com/cases/federal/us/457/332/ .
[190]Practical Law Antitrust. US Antitrust Laws: Overview .URL:https://uk.practicallaw.
thomsonreuters.com/9-204-0472 .
[191]State Oil Co. v. Khan :: 522 U.S. 3 . Nov. 1997. URL:https://supreme.justia.com/c
ases/federal/us/522/3/ .
[192]FTC v. Indiana Fed’n of Dentists :: 476 U.S. 447 . June 1986. URL:https://supreme.just
ia.com/cases/federal/us/476/447/ .
[193]National Soc’y of Prof. Engineers v. United States :: 435 U.S. 679 . Apr. 1978. URL:https://s
upreme.justia.com/cases/federal/us/435/679/ .
[194]Federal Trade Commision and U.S. Department of Justice. Antitrust Guidelines for Collaborations
Among Competitors . Tech. rep. Federal Trade Commision, Apr. 2000. URL:https://www.ft
c.gov/sites/default/files/documents/public%7B%5C_%7 Devents/joi
nt-venture-hearings-antitrust-guidelines-collaborat ion-among-
competitors/ftcdojguidelines-2.pdf .
[195]Osbert Bastani, Carolyn Kim, and Hamsa Bastani. “Interpret ing Blackbox Models via Model Ex-
traction”. In: arXiv (May 2017). arXiv: 1705.08504 .URL:http://arxiv.org/abs/1
705.08504 .
[196]Luca Pulina and Armando Tacchella. “An Abstraction-Reﬁnem ent Approach to Veriﬁcation of
Artiﬁcial Neural Networks”. In: CEUR Workshop Proceedings . 2010, pp. 243–257. URL:http:/
/link.springer.com/10.1007/978-3-642-14295-6%7B%5C_ %7D24 .
[197]Changliu Liu et al. “Algorithms for Verifying Deep Neural Ne tworks”. In: arXiv (Mar. 2019).
arXiv:1903.06758 .URL:http://arxiv.org/abs/1903.06758 .
[198]Daniel Selsam, Percy Liang, and David L. Dill. “Developing B ug-Free Machine Learning Systems
With Formal Mathematics”. In: arXiv (June 2017). arXiv: 1706.08605 .URL:http://arx
iv.org/abs/1706.08605 .
[199]C Hutchison et al. “Robustness Testing of Autonomy Software ”. In: 2018 IEEE/ACM 40th Inter-
national Conference on Software Engineering: Software Engineeri ng in Practice Track (ICSE-SEIP) .
May 2018, pp. 276–285. URL:https://users.ece.cmu.edu/%7B~%7Dkoopman/p
ubs/hutchison18%7B%5C_%7Dicse%7B%5C_%7Drobustness%7 B%5C_%7Dtes
ting%7B%5C_%7Dautonomy%7B%5C_%7Dsoftware.pdf .
[200]Joe Gibbs Politz et al. “Python: The Full Monty”. In: Proceedings of the 2013 ACM SIGPLAN In-
ternational Conference on Object Oriented Programming Systems Lan guages & Applications . OOP-
SLA ’13. New York, NY, USA: Association for Computing Machin ery , 2013, pp. 217–232. ISBN:
9781450323741. DOI:10.1145/2509136.2509536 .URL:https://doi.org/10.
1145/2509136.2509536 .
56[201]Philippa Anne Gardner, Sergio Maffeis, and Gareth David Smi th. “Towards a Program Logic for
JavaScript”. In: SIGPLAN Not. 47.1 (Jan. 2012), pp. 31–44. ISSN: 0362-1340. URL:https://
doi.org/10.1145/2103621.2103663 .
[202]Lindsey Kuper et al. “Toward Scalable Veriﬁcation for Safet y-Critical Deep Networks”. In: arXiv
(Jan. 2018). arXiv: 1801.05950 .URL:http://arxiv.org/abs/1801.05950 .
[203]Pang Wei Koh and Percy Liang. “Understanding Black-box Pred ictions via Inﬂuence Functions”.
In:arXiv (Mar. 2017). arXiv: 1703.04730 .URL:http://arxiv.org/abs/1703.04
730.
[204]Rajiv Khanna et al. “Interpreting Black Box Predictions usi ng Fisher Kernels”. In: arXiv (Oct.
2018). arXiv: 1810.10118 .URL:http://arxiv.org/abs/1810.10118 .
[205]Boris Sharchilev et al. “Finding Inﬂuential Training Sampl es for Gradient Boosted Decision
Trees”. In: arXiv (Feb. 2018). arXiv: 1802.06640 .URL:http://arxiv.org/abs/1
802.06640 .
[206]Chih-Kuan Yeh et al. “Representer Point Selection for Expla ining Deep Neural Networks”. In: Con-
ference on Neural Information Processing Systems . 2018. URL:https://papers.nips.cc/
paper/8141-representer-point-selection-for-explaini ng-deep-ne
ural-networks.pdf .
[207]Quanshi Zhang et al. “Interpreting CNNs via Decision Trees” . In: IEEE Conference on Computer
Vision and Pattern Recognition . 2019. URL:https://arxiv.org/abs/1802.00121 .
[208]Himabindu Lakkaraju, Stephen H. Bach, and Jure Leskovec. “I nterpretable decision sets: A joint
framework for description and prediction”. In: Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining . Vol. 13-17-Augu. Association for Computing
Machinery, Aug. 2016, pp. 1675–1684. ISBN: 9781450342322. URL:https://dl.acm.or
g/doi/10.1145/2939672.2939874 .
[209]Sarah Tan et al. “Learning Global Additive Explanations for Neural Nets Using Model Distilla-
tion”. In: arXiv (Jan. 2018). arXiv: 1801.08640 .URL:http://arxiv.org/abs/180
1.08640 .
[210]Quanshi Zhang et al. “Interpreting CNN Knowledge via an Expl anatory Graph”. In: arXiv (Aug.
2017). arXiv: 1708.01785 .URL:http://arxiv.org/abs/1708.01785 .
[211]Wieland Brendel and Matthias Bethge. “Approximating CNNs w ith Bag-of-local-Features models
works surprisingly well on ImageNet”. In: arXiv (Mar. 2019). arXiv: 1904.00760 .URL:htt
p://arxiv.org/abs/1904.00760 .
[212]Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. “Interpreta ble Convolutional Neural Net-
works”. In: arXiv (Oct. 2017). arXiv: 1710.00935 .URL:http://arxiv.org/abs/17
10.00935 .
[213]Xi Chen et al. “InfoGAN: Interpretable Representation Lear ning by Information Maximizing Gen-
erative Adversarial Nets”. In: arXiv (June 2016). arXiv: 1606.03657 .URL:http://arxi
v.org/abs/1606.03657 .
[214]Irina Higgins et al. “ β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VAR I-
ATIONAL FRAMEWORK”. In: International Conference on Learning Representations . 2017. URL:
https://openreview.net/references/pdf?id=Sy2fzU9gl .
[215]Abhinav Verma et al. “Programmatically Interpretable Rein forcement Learning”. In: arXiv (Apr.
2018). arXiv: 1804.02477 .URL:http://arxiv.org/abs/1804.02477 .
57[216]Andrew Slavin Ross and Finale Doshi-Velez. “Improving the A dversarial Robustness and Inter-
pretability of Deep Neural Networks by Regularizing their I nput Gradients”. In: arXiv (Nov.
2017). arXiv: 1711.09404 .URL:http://arxiv.org/abs/1711.09404 .
[217]Saleema Amershi et al. “Modeltracker: Redesigning perform ance analysis tools for machine
learning”. In: Conference on Human Factors in Computing Systems - Proceedings . Vol. 2015-April.
Association for Computing Machinery , Apr. 2015, pp. 337–34 6.ISBN: 9781450331456. DOI:
10.1145/2702123.2702509 .URL:https://www.microsoft.com/en-us/re
search/publication/modeltracker-redesigning-perform ance-ana%2
0lysis-tools-for-machine-learning/ .
[218]Tongshuang Wu et al. “Errudite: Scalable, Reproducible, an d Testable Error Analysis”. In: Pro-
ceedings of the 57th Annual Meeting of the Association for Comput ational Linguistics . Stroudsburg,
PA, USA: Association for Computational Linguistics, 2019, pp. 747–763. URL:https://www
.aclweb.org/anthology/P19-1073 .
[219]Jesse Vig. “A Multiscale Visualization of Attention in the T ransformer Model”. In: arXiv (June
2019). arXiv: 1906.05714 .URL:http://arxiv.org/abs/1906.05714 .
[220]David Bau et al. “GaN dissection: Visualizing and understan ding generative adversarial net-
works”. In: 7th International Conference on Learning Representations, ICLR 20 19. International
Conference on Learning Representations, ICLR, 2019. arXiv :1811.10597 .URL:https://
arxiv.org/abs/1811.10597 .
[221]Shan Carter et al. “Activation Atlas”. In: Distill 4.3 (Mar. 2019). ISSN: 2476-0757. URL:https
://distill.pub/2019/activation-atlas .
[222]Jost Tobias Springenberg et al. “Striving for Simplicity: T he All Convolutional Net”. In: arXiv
(Dec. 2014). arXiv: 1412.6806 .URL:http://arxiv.org/abs/1412.6806 .
[223]Matthew D. Zeiler and Rob Fergus. “Visualizing and understa nding convolutional networks”. In:
Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and
Lecture Notes in Bioinformatics) . Springer Verlag, 2014, pp. 818–833. ISBN: 9783319105895.
DOI:10.1007/978-3-319-10590-153 . arXiv:1311.2901 .URL:https://cs.n
yu.edu/%7B~%7Dfergus/papers/zeilerECCV2014.pdf .
[224]Ruth Fong and Andrea Vedaldi. “Interpretable Explanations of Black Boxes by Meaningful Pertur-
bation”. In: IEEE International Conference on Computer Vision . Apr. 2017. arXiv: 1704.03296 .
URL:http://arxiv.org/abs/1704.03296%20http://dx.doi.org /10.110
9/ICCV.2017.371 .
[225]Ruth Fong, Mandela Patrick, and Andrea Vedaldi. “Understan ding Deep Networks via Extremal
Perturbations and Smooth Masks”. In: IEEE International Conference on Computer Vision . 2019.
URL:https://arxiv.org/abs/1910.08485 .
[226]Piotr Dabkowski and Yarin Gal. “Real Time Image Saliency for Black Box Classiﬁers”. In: Advances
in Neural Information Processing Systems 30 . Ed. by I Guyon et al. Curran Associates, Inc., 2017,
pp. 6967–6976. URL:http://papers.nips.cc/paper/7272-real-time-imag
e-saliency-for-black-box-classifiers.pdf .
[227]Vitali Petsiuk, Abir Das, and Kate Saenko. “RISE: Randomize d Input Sampling for Explanation
of Black-box Models”. In: arXiv (June 2018). arXiv: 1806.07421 .URL:http://arxiv.
org/abs/1806.07421 .
[228]Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. “Dee p Inside Convolutional Net-
works: Visualising Image Classiﬁcation Models and Salienc y Maps”. In: arXiv (Dec. 2013). arXiv:
1312.6034 .URL:http://arxiv.org/abs/1312.6034 .
58[229]Himabindu Lakkaraju et al. “Interpretable & Explorable App roximations of Black Box Models”.
In:Fairness, Accountability, and Transparency in Machine Learnin g. 2017. arXiv: 1707.01154v1 .
URL:https://arxiv.org/abs/1707.01154 .
[230]Anh Nguyen et al. “Synthesizing the preferred inputs for neu rons in neural networks via deep
generator networks”. In: Advances in Neural Information Processing Systems 29 . Ed. by D D Lee et
al. Curran Associates, Inc., 2016, pp. 3387–3395. URL:http://papers.nips.cc/pape
r/6519-synthesizing-the-preferred-inputs-for-neuron s-in-neura
l-networks-via-deep-generator-networks.pdf .
[231]David Bau et al. “Network Dissection: Quantifying Interpre tability of Deep Visual Representa-
tions”. In: arXiv (Apr. 2017). arXiv: 1704.05796 .URL:http://arxiv.org/abs/170
4.05796 .
[232]Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. “F eature Visualization”. In: Distill
2.11 (Nov. 2017). ISSN: 2476-0757. DOI:10.23915/distill.00007 .URL:https://
distill.pub/2017/feature-visualization .
[233]Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky . “Dee p Image Prior”. In: The IEEE Con-
ference on Computer Vision and Pattern Recognition . 2018. URL:https://arxiv.org/ab
s/1711.10925 .
[234]Chris Olah et al. “The Building Blocks of Interpretability” . In:Distill 3.3 (Mar. 2018). ISSN: 2476-
0757. DOI:10.23915/distill.00010 .URL:https://distill.pub/2018/bu
ilding-blocks .
[235]Ari S. Morcos et al. “On the importance of single directions f or generalization”. In: arXiv (Mar.
2018). arXiv: 1803.06959 .URL:http://arxiv.org/abs/1803.06959 .
[236]Karel Lenc and Andrea Vedaldi. “Understanding image repres entations by measuring their equiv-
ariance and equivalence”. In: arXiv (Nov. 2014). arXiv: 1411.5908 .URL:http://arxiv
.org/abs/1411.5908 .
[237]Laurens Van Der Maaten and Geoffrey Hinton. “Visualizing Da ta using t-SNE”. In: Journal of
Machine Learning Research 9 (2008), pp. 2579–2605. URL:http://www.jmlr.org/pap
ers/volume9/vandermaaten08a/vandermaaten08a.pdf .
[238]Carrie J. Cai et al. “Human-Centered Tools for Coping with Im perfect Algorithms during Medical
Decision-Making”. In: arXiv (Feb. 2019). arXiv: 1902.02960 .URL:http://arxiv.org
/abs/1902.02960 .
59Appendices
I Workshop and Report Writing Process
This report began as an effort to identify areas for producti ve work related to trust in AI development.
The project began in earnest with an interdisciplinary expe rt workshop in San Francisco in April of 2019,
which brought together participants from academia, indust ry labs, and civil society organizations.82As
discussed below, during the writing process, we shifted our focus to veriﬁable claims in particular, rather
than trust more broadly .
Workshop attendees are listed below in alphabetical order:
•Amanda Askell
•Andrew Critch
•Andrew Lohn
•Andrew Reddie
•Andrew Trask
•Ben Garﬁnkel
•Brian Tse
•Catherine Olsson
•Charina Chou
•Chris Olah
•David Luan
•Dawn Song
•Emily Oehlsen
•Eric Sigler
•Genevieve Fried
•Gillian Hadﬁeld
•Heidy Khlaaf
•Helen Toner•Ivan Vendrov
•Jack Clark
•Jeff Alstott
•Jeremy Nixon
•Jingying Yang
•Joshua Kroll
•Lisa Dyer
•Miles Brundage
•Molly Welch
•Paul Christiano
•Peter Eckersley
•Seán Ó hÉigeartaigh
•Shahar Avin
•Shixiong (Austin) Zhang
•Teddy Collins
•Tim Hwang
•William Isaac
82Our intent with this section of the report is to be transparen t with readers about our process and to acknowledge some of
the voices and methods missing from that process. We also hop e that providing information about our process could be help ful
for those considering similar multi-stakeholder research projects.
60Given our initial focus on synthesizing and extending exist ing work, we brought together experts in
dimensions of trust that were identiﬁed in a pre-workshop wh ite paper, several of which are discussed
in this report (such as secure enclaves, third party auditin g, and privacy-preserving machine learning).
However, a number of voices were missing from that conversat ion. The workshop could have bene-
ﬁted in particular from greater gender diversity (fewer tha n one third of participants were women, and
none were members of trans or non-binary communities); grea ter racial diversity (people of color and
especially women of color were under-represented, particu larly given the number of women of color
with relevant expertise on trust in AI development); greate r representation of low income communities;
greater representation of people with disabilities; and gr eater geographic, political, philosophical, and
religious diversity .
Following the workshop, the corresponding authors led a mul ti-stakeholder writing, editing, and feed-
back process. A subset of workshop attendees opted to engage in the writing process for the report.
After the ﬁrst round of writing and editing, we tried to incor porate new authors with complementary
expertise to those at the original workshop. Notwithstandi ng these efforts, not all dimensions of trust in
AI development (or even veriﬁable claims in AI development) were represented in the expertise of the
authors. As such, over time and especially in response to ext ernal reviewer feedback, we progressively
narrowed the scope of the report in order to avoid overreach a nd "stay in our lane" topic-wise. One such
shift was a move from discussing trust in AI development gene rally to veriﬁable claims speciﬁcally as
the focus of the report.
The report was written in a semi-modular fashion. Experts in particular areas drafted subsections on
mechanisms or research areas with which they are familiar. T hese subsections were revised substantially
over time in response to author and reviewer feedback, and to shifts in the framing of the report. External
reviewers provided feedback on speciﬁc portions of the repo rt for clarity and accuracy , although the
report as a whole was not formally peer reviewed.
Given the number of authors involved and the wide-ranging na ture of the report, it was difﬁcult to ensure
that all authors were fully supportive of all content throug hout the writing process. Where appropriate,
we have included footnotes to clarify process and attributi on.
61II Key Terms and Concepts
AI: we deﬁne artiﬁcial intelligence (AI) as any digital system capable of performing tasks commonly
thought to require intelligence, with these tasks often bei ng learned from data and /or experience.83
AI system : we deﬁne an AI system as a software process (with the characteristics of AI mentioned
above), running on physical hardware , under the direction of humans operating in some institutional
context. This framing of AI systems informs the discussion o f mechanisms in the report. The properties
of the software, hardware, and institutions at work in a give n AI system are all potentially relevant to
the veriﬁability of claims made by an AI developer. Focusing on any of these to the exclusion of others
could result in a ﬂawed understanding of the overall system.
AI development : we use the term AI development to refer to the process of rese arching, designing,
testing, deploying, or monitoring AI as deﬁned above.
AI developer : we use the term AI developer to refer to individuals or organ izations involved in AI de-
velopment as deﬁned broadly above, including research scie ntists, data engineers, and project managers
at companies building AI-based products and services as wel l as those in analogous roles in academia,
government, or civil society . Given the major role played by technology companies in contemporary
AI development, we pay particular attention to such compani es in the report, while recognizing that
different contexts will require different approaches to ea rning trust.
Responsible AI development : we follow Askell et al. in deﬁning responsible AI developme nt as follows
[1]:
"Responsible AI development involves taking steps to ensur e that AI systems have an ac-
ceptably low risk of harming their users or society and, idea lly , to increase their likelihood
of being socially beneﬁcial. This involves testing the safe ty and security of systems during
development, evaluating the potential social impact of the systems prior to release, being
willing to abandon research projects that fail to meet a high bar of safety , and being willing
to delay the release of a system until it has been established that it does not pose a risk to
consumers or the public."
Transparency : we deﬁne transparency as making information about the char acteristics of an AI devel-
oper’s operations or their AI systems available to actors bo th inside and outside the organization. In
recent years, transparency has emerged as a key theme in work on the societal implications of AI.84
Transparency can beneﬁt from the open publication of AI syst ems (including code, data, and models),
though privacy , safety , and competitive considerations pr event this from being appropriate in all cases.85
83Some distinctions are made between different phases of AI de velopment in the report, although the authors have also
found it helpful to take a broad view of look at AI development : in many cases, the same mechanisms (especially institutio nal
ones) are applicable to multiple phases of development, and AI development was found to be the most appropriate catch-all
term for such purposes. A recent and representative example of a more granular breakdown, from Toreini et al., distingui shes
data-related steps (data collection, data preparation, an d feature extraction) from model-related steps (training, testing, and
inference)[129]. In practice, these steps are not followed in a linear manner , since (e.g.) testing may inform changes to the
training process and data may be improved over time.
84See, for example, the Fairness, Accountability, and Transp arency in Machine Learning (FATML) workshop and community,
which was followed by the ACM Conference on Fairness, Accoun tability, and Transparency (ACM FAccT).
85The necessity and sufﬁciency of transparency as an ideal for technical systems has also been critiqued in recent years, s uch
as from Ananny and Crawford [130]and Kroll et. al. [131].
62Realizing transparency in AI development requires attenti on to institutional mechanisms and legal struc-
tures, particularly when scrutinizing developers’ incent ives and scope of activities.
Trust andtrustworthiness : these concepts have been extensively explored by research ers, though a
consensus account across domains remains elusive. Substan tial prior and ongoing work focuses on
how these concepts manifest in AI development. This include s academic work [129],86government-
associated efforts such as the European Union’s High-Level Expert Group on AI [133], and industry
efforts[134][135]. Our report focuses on a particular subset of what these conc epts entail in the practice
of AI development, namely the veriﬁability of claims about A I development, and more speciﬁcally , the
veriﬁability of claims about safety , security , privacy , an d fairness.
Several common frameworks for thinking about trust suggest a premium on the veriﬁability of claims
even when they do not reference these terms explicitly . For e xample, Mayer et al.’s widely cited work
[136]identiﬁes benevolence, integrity , and ability as three pil lars of trustworthiness. In the context of AI,
a developer might make claims that suggest their pursuit of a benevolent goal (e.g., by adopting a set of
ethical principles), but this needs to be accompanied by the skills and resources (ability) to achieve that
goal as well as sufﬁcient transparency and incentives to ens ure consistent follow-through (integrity).
Another prominent deﬁnition of trust which supports a focus on veriﬁable claims comes from Gambetta
(paraphrased below):87
"When we say we trust someone or that someone is trustworthy , we implicitly mean that
we assess that the probability [they]will take actions that are beneﬁcial (or at least not
detrimental) is high enough for us to consider engaging in so me form of cooperation with
[them]."
Here, too, the ability to scrutinize the claims and commitme nts made by an AI developer can provide
calibration regarding the extent to which trust is appropri ate in a given context.
The veriﬁability of claims is also a key theme in the study of t rust in the context of international relations
and arms control [138]. Ronald Reagan’s famous "trust but verify" (a proverb taugh t to him by advisor
on Russian affairs Suzanne Massie [139]) emphasized the value of generating and assessing evidence of
compliance with arms control agreements between the United States and Soviet Union. AI, veriﬁcation,
and arms control are discussed further in Appendix IV .
This report is not intended to make novel contributions to th e theory of trust or trustworthiness, but
rather to explore veriﬁable claims as a building block of tru stworthy AI development. When we use
terms such as "earn trust" or "calibrate trust" in the report , these are meant to refer to cases in which
evidence is provided to substantiate claims about an actor’ s behavior (including AI development).
86See[132]for a useful literature review.
87This is a modiﬁcation of Gambetta’s deﬁnition [137].
63III The Nature and Importance of Veriﬁable Claims
Veriﬁable88claims are statements for which evidence and arguments can be broug ht to bear on the
likelihood of those claims being true. Veriﬁable claims are sufﬁciently precise to be falsiﬁable, and the
degree of attainable certainty in such claims will vary acro ss contexts.
AI developers regularly make claims regarding the properti es of AI systems they develop as well as their
associated societal consequences. Claims related to AI dev elopment might include, e.g.:
•We will adhere to the data usage protocols we have speciﬁed;
•The cloud services on which our AI systems run are secure;
•We will evaluate risks and beneﬁts of publishing AI systems i n partnership with appropriately
qualiﬁed third parties;
•We will not create or sell AI systems that are intended to caus e harm;
•We will assess and report any harmful societal impacts of AI s ystems that we build; and
•Broadly , we will act in a way that aligns with society’s inter ests.
The veriﬁcation of claims about AI development is difﬁcult i n part due to the inherent complexity and
heterogeneity of AI and its supporting infrastructure. The highly dispersed ecosystem means there are
many actors and sets of incentives to keep track of and coordi nate. Further, the speed of development
also means there is less time for claims to be carefully expre ssed, defended, and evaluated. And, per-
haps most critically , claims about AI development are often too vague to be assessed with the limited
information publicly made available.
Notwithstanding these challenges, there are at least three distinct reasons why it is highly desirable for
claims made about AI development to be veriﬁable.
First, those potentially affected by AI development–as wel l as those seeking to represent those parties’
interests via government or civil society–deserve to be abl e to scrutinize the claims made by AI developers
in order to reduce risk of harm or foregone beneﬁt.
Second, to the extent that claims become veriﬁable, various actors such as civil society , policymakers,
and users can raise their standards for what constitutes res ponsible AI development. This, in turn, can
improve societal outcomes associated with the ﬁeld as a whol e.
Third, a lack of veriﬁable claims in AI development could fos ter or worsen a "race to the bottom" in
AI development, whereby developers seek to gain a competiti ve edge even when this trades off against
important societal values such as safety , security , privac y , or fairness[1]. In both commercial (e.g.,
88While the report does discuss the technical area of formal ve riﬁcation at several points, the sense in which we use "veriﬁ able"
is distinct from how the term is used in that context. Unless o therwise speciﬁed by the use of the adjective "formal" or oth er
context, this report uses the word veriﬁcation in a looser se nse. Formal veriﬁcation seeks mathematical proof that a cer tain
technical claim is true with certainty (subject to certain a ssumptions). In contrast, this report largely focuses on cl aims that
are unlikely to be demonstrated with absolute certainty, bu t which can be shown likely or unlikely to be true, i.e. trustw orthy,
or untrustworthy through relevant arguments and evidence.
64autonomous vehicles) and non-commercial (e.g., military) contexts, veriﬁable claims may be needed to
foster cooperation rather than race-like behavior.
Without the ability to verify AI-related claims, the decisi on of how and whether to interact with AI sys-
tems must be made without information that could bear on the d esirability of having that interaction.
Given the large (and growing) stakes of AI development, such an information deﬁcit is ethically unten-
able. An environment of largely unveriﬁable claims about AI could encourage extreme reactions to AI
in particular situations (i.e., blind trust or blind reject ion), resulting in both over-use and under-use of
AI. The world instead needs trust in AI development to be well -calibrated, i.e. it should be the case that
conﬁdence in certain claims or actors is proportional to the available evidence. The beneﬁts and risks of
AI are many , and need to be appraised with context and nuance.
In the past decade, claim-oriented approaches have been dev eloped in order to structure arguments
about the safety of engineered systems [140], and we draw inspiration from such approaches in this
report. One result of such work is the introduction and stand ardization of assurance cases in numerous
domains. An assurance case is a documented body of evidence t hat provides a convincing and valid
argument regarding a top-level claim (such as the safety of a nuclear power plant), and presents a
structured justiﬁcation in support of that claim to decide t he status of it. Assurance cases are often
required as part of a regulatory process (e.g., a certiﬁcate of safety being granted only when the regulator
is satisﬁed by the argument presented in a safety case).89
This work matured into the widely-used Claims, Arguments, a nd Evidence (CAE) framework.90CAE is
often the framework of choice in aviation, nuclear, and defe nse industries worldwide to reason about
safety , security , reliability and dependability , and rece nt work has begun applying CAE to the safety
analysis of AI systems.91
The CAE framework consists of three key elements. Claims are assertions put forward for general accep-
tance. They’re typically statements about a property of the system or some subsystem. Claims asserted
as true without justiﬁcation are assumptions, and claims su pporting an argument are subclaims. Argu-
ments link evidence to a claim, which can be deterministic, probab ilistic, or qualitative.92They consist
of "statements indicating the general ways of arguing being applied in a particular case and implicitly
relied on and whose trustworthiness is well established" [144], together with validation of any scien-
tiﬁc laws used. In an engineering context, arguments should be explicit. Evidence serves as the basis
for justiﬁcation of a claim. Sources of evidence can include the design, the development process, prior
experience, testing, or formal analysis.
For a sufﬁciently complex AI system or development process, a wide variety of mechanisms will likely
89Assurance cases are primarily concerned with demonstratin g the validity (or otherwise) of the resulting argument and
have two main roles: logical reasoning and communication. C ases are usually integrated within a regulatory process tha t
provides for independent challenge and review. There can be a number of stakeholders including public ofﬁcials, develo pers,
certiﬁers, regulators. Communication is thus essential to create a shared understanding between the different stakeh olders,
build conﬁdence and consensus
90See, e.g., this discussion of CAE in the nuclear safety conte xt[141]and Uber’s use of GSN [142]
91See Zhao et al.[143].
92Arguments are presented in a form of defeasible reasoning of top-level claims, supported by the available evidence; and
driven by practical concerns of achieving the required goal in the best possible way considering the existing uncertain ties,
point of views, concerns and perspectives of different stak eholders. Such argumentation is expected to be multidiscip linary,
and cover a wide range of mechanisms, which we aim to address. To support CAE, a graphical notation can be used to describe
the interrelationship of claims, arguments, and evidence. Claim justiﬁcations can be constructed using argument bloc ks–
concretion, substitution, decomposition, calculation, a nd evidence incorporationâ ˘AˇTas well as narrative analyses that describe
the claims, arguments, and evidence in detail.
65need to be brought to bear in order to adequately substantiat e a high-level claim such as "this system
was developed in accordance with our organization’s ethica l principles and relevant laws."
66IV AI, Veriﬁcation, and Arms Control
At an international level, arms control is a possible approa ch to addressing some of the risks of AI de-
velopment in a military context. Arms control involves simi lar issues to those discussed earlier (namely ,
the need for credible commitments and close attention to tra nsparency and incentives) in non-military
contexts. In this subsection, we provide an overview of the r elationship between veriﬁable claims and
arms control applied to AI.
Arms control is a special case of regulation, in which nation -states cooperate to self-regulate weapons
technologies under particularly challenging conditions [145][146][147][148][149]. Unlike in domes-
tic regulation, there is no external actor to force complian ce if states violate the terms of an arms control
agreement. Instead, states generally rely on reciprocity t o enforce arms control agreements. If states
violate an agreement, they can often expect others to follow suit and develop the weapon themselves.
Formal agreements such as treaties act as coordination mech anisms for states to reach agreement, but
do not directly enforce compliance. Some treaties include v eriﬁcation regimes to help increase visibility
among states as to whether or not others are complying with an agreement, as a means of facilitating
trust[150][151]. But it is on states themselves to take action if others are fo und violating an agreement,
whether through sanctions, reciprocal weapons developmen t, military action, or other tools of statecraft.
Arms control is inherently challenging not only because the re is no third-party enforcement mechanism,
but because states may be incentivized to violate agreement s if they believe that doing so may give them
an edge against competitors [152][153][154]. This tension is exacerbated if it is challenging to verify
other states’ behavior. States may assume others are cheati ng and developing a prohibited technology in
secret, incentivizing them to do so as well or risk falling be hind a competitor. Arms control agreements
can also be more challenging to hold together if a technology is more widely accessible to a larger number
of actors and if defection by one actor generates incentives for others to defect. There are many cases
in history in which nation-states genuinely desired mutual restraint for certain weapons, such as turn of
the century rules regulating submarines, air-delivered we apons, and poison gas, but states were unable
to achieve effective cooperation in wartime for a variety of reasons.
Despite these hurdles, there have been successful examples of arms control for a number of weapons, in-
cluding: chemical [155][156]and biological[157]weapons; land mines [158][159]; cluster munitions
[160]; blinding lasers [161]; exploding bullets; limits on the proliferation, quantity , and deployment of
nuclear weapons [162]; anti-ballistic missile systems [163]; weapons of mass destruction in space; and
weapons on the Moon or in Antarctica. There are also examples of mutual restraint with some weapons
despite the lack of formal agreements, including neutron bo mbs, kinetic (debris-causing) anti-satellite
weapons, and certain forms of bayonets.
Even these successes highlight the limitations of arms cont rol, however. Some treaties have collapsed
over time as more nations gained access to the underlying tec hnology and did not abide by the prohibi-
tion. And even the most successful prohibitions, such as tho se on chemical and biological weapons, have
failed to rein in rogue regimes or terrorists. Despite the wi despread global condemnation of chemical
weapons, Bashar al Assad has used them in Syria to murder civi lians, with minimal consequences from
the international community .
In general, arms control is more likely to succeed when:93(1) there are clear lines between which
93See, e.g., Crootof [164], Watts[165], and Scharre[166].
67weapons are prohibited and which are permitted; (2) the perc eived horribleness of a weapon outweighs
its military value; (3) states have the ability , either thro ugh formal veriﬁcation regimes or other mecha-
nisms, to ensure that others are complying with the regulati on; and (4) fewer states are needed for an
agreement to work. Regulation can occur at multiple points o f technology development, limiting or pro-
hibiting access to the underlying technology , weapons deve lopment, production, and /or use. Note also
that while some of the variables above are exogenous from the perspective of the AI community , others
are potentially amenable to inﬂuence (e.g., research could potentially improve the distinguishability of
offensive and defensive uses, or improve the general tracea bility and interpretability of AI systems).
AI will likely be a transformative technology in warfare [167]. Anticipating this transformation, many
in the scientiﬁc community have called for restrictions or b ans on military AI applications. Because AI
is a general-purpose enabling technology with many applica tions, broad bans on AI overall are unlikely
to succeed. However, prohibitions on speciﬁc military appl ications of AI could succeed, provided states
could agree to such limits (requiring that the terms be compa tible with the incentives of each party) and
that appropriate means of verifying compliance are develop ed and implemented.
AI technology has certain attributes that may make successf ul restraint challenging, however. These
include its widespread availability , dual use or "omni-use " nature[168], the difﬁculty in drawing clear
lines between acceptable and unacceptable AI applications , and the challenges of verifying compliance,
which are at least as difﬁcult as those found in non-military contexts and perhaps more challenging given
the more adversarial context.
One special case worth highlighting is the development of le thal autonomous weapon systems (LAWS).
An international LAWS ban has been the subject of discussion at the UN Convention on Certain Conven-
tional Weapons (CCW) since 2014. There are many arguments ma de in support of restrictions on LAWS.
Three relevant arguments are: (1) their use is immoral becau se AI systems will not in the foreseeable
future understand the moral, psychological, and social con text at the time of killing a person (unlike a
human, who could decide to not press the trigger) [169][170]; (2) the state of the technology today
would preclude their use under international law in anythin g but isolated cases, such as undersea where
civilians are not present; and (3) they might proliferate ea sily , enabling misuse [168][171]. Those
skeptical of a ban on lethal autonomous weapon systems often reference mutual distrust as a reason for
development: "if we don’t develop them, others will, puttin g us at a strategic disadvantage" is a refrain
echoed by several great powers.
Avoiding such an impasse requires grappling with the issue o f trust head-on, and closely attending to the
complexities of AI development in practice. Similar to how A I ethics principles need to be supplemented
with mechanisms that demonstrate the implementation of suc h principles, trust in military-relevant AI
systems must be supported by mechanisms based on a rigorous a nalysis of the dynamics of military AI
development. Lethal autonomous weapons are currently the f ocus of much related discussion, though
the use of AI in cyberwarfare and nuclear command and control have also been highlighted as chal-
lenging areas in recent years. Some early work in the directi on of coordination on AI among great
powers[172]has called attention to the need for early dialogue on AI safe ty and security . Other work
has ﬂeshed out the notion of meaningful human control as a cor nerstone of lethal autonomous weapon
system governance [173][161][174].
The AI community and advocates in other disciplines have pla yed a key role in bringing this issue to the
attention of the international community [175][176][177][178][179][180][181]. Similar efforts by
expert communities have improved prospects for arms contro l in prior contexts such as nuclear weapons
[182][183]. There remains more to be done to raise the proﬁle of the issue among policymakers, and
68to identify appropriate steps that individuals and organiz ations in the AI community can take to forestall
the development of potentially harmful systems.
AI researchers could contribute technical expertise that h elps identify potential governance mechanisms
in this context. For example, AI researchers, working with a rms control experts in other disciplines,
could scrutinize proposals such as defensively-oriented A I weapons systems that could target lethal au-
tonomous weapons (but not humans) and help think through dif ferent means of limiting proliferation
and ensuring human accountability . The AI community’s dist ributed expertise in the process of AI devel-
opment and the feasibility of different technical scenario s could thus be brought to bear to limit AI "arms
racing" and prevent a race to the bottom with respect to the sa fety , security , and human-accountability
of deployed military AI systems [166]. The feasibility of verifying, interpreting, and testing p otential
AI systems designed for various purposes, as well as the feas ibility of using different points of control
for governance of the supply chain (e.g., the computing and n on-computing hardware associated with
autonomous weapons vs. the underlying software), are all is sues to which AI expertise is relevant.
Of the various inputs into AI development (including hardwa re, software, data, and human effort), it’s
worth noting that hardware is uniquely governable, at least in principle. Computing chips, no matter
how fast, can perform only a ﬁnite and known number of operati ons per second, and each one has
to be produced using physical materials that are countable, trackable, and inspectable.94Similarly ,
physical robots rely on supply chains and materials that are in principle trackable. Computing power and
hardware platforms for robotics are thus potentially amena ble to some governance tools used in other
domains that revolve around tracking of physical goods (e.g ., export controls and on-site inspections).
While it is unclear what hardware-based veriﬁcation effort s might look like in the context of AI-related
arms control, and how feasible they would be, one might imagi ne, e.g., a bottleneck in the manufacturing
process for lethal autonomous weapons. In contrast to such a bottleneck, AI-related insights, data, code,
and models can be reproduced and distributed at negligible m arginal cost, making it inherently difﬁcult
to control their spread or to use them as a metric for gauging t he capabilities of an organization with
respect to developing lethal decision-making systems.95Given such considerations, it is incumbent upon
stakeholders to consider ways in which the distinctive prop erties of hardware might be leveraged in
service of verifying any future arms control agreements.
94We emphasize that this discussion is exploratory in nature, and that there would be major practical challenges involved
in acting on these high-level ideas. Our goal in highlightin g the unique affordances of hardware is to foster creative th inking
about these issues rather than to suggest that there is a read ily available solution to the weaponization of AI.
95Another potential approach would be to impose constraints o n the physical characteristics of AI-enabled military syst ems,
such as their range, payload, endurance, or other non-AI rel ated physical attributes.
69V Cooperation and Antitrust Laws
Collaborations between competing AI labs, even for beneﬁci al purposes such as enabling veriﬁable
claims, can raise antitrust issues. Antitrust law is also kn own as "competition law" or "anti-monopoly law"
outside the US. This section primarily addresses US antitru st law, but given the international nature of
AI development and markets, attention to the international legal implications of industry collaborations
is warranted.
US antitrust law seeks to prevent "unreasonable" restraint s on trade[184]. Unreasonableness, in turn,
is tested by economic analysis [185]–speciﬁcally , a "consumer welfare" test [186]. Although recent
academic[187]and popular[188]proposals challenge the wisdom and usefulness of this test, consumer
welfare remains the guiding principle for antitrust courts [186].
Antitrust law generally condemns per se particularly harmf ul restraints on trade,96such as direct re-
straints on price, output levels, competitive bidding, and market allocation [190]. Other practices
are analyzed by the Rule of Reason, "according to which the ﬁn der of fact must decide whether the
questioned practice imposes an unreasonable restraint on c ompetition, taking into account a variety of
factors, including speciﬁc information about the relevant business, its condition before and after the re-
straint was imposed, and the restraint’s history , nature, a nd[net]effect."[191]Importantly , courts in the
past have consistently rejected safety-based (and other public policy-based) defenses of a nticompetitive
behavior[192].
In a leading case on point, National Society Professional Engineers v. United States , the US Supreme
Court reasoned that by adopting antitrust laws, Congress ha d made a "basic policy" decision to protect
competition[193]. The Court therefore concluded that the defendant’s argume nt that price competi-
tion between engineers was unsafe " [wa]s nothing less than a frontal assault on the basic policy of th e
[antitrust laws].97" "In sum," the Court held, "the Rule of Reason does not suppor t a defense based on
the assumption that competition itself is unreasonable."98
None of this implies, however, that collaborations between competitors are always anticompetitive and
therefore violative of antitrust laws. American antitrust authorities have acknowledged that collabora-
tions between competitors can have important procompetiti ve beneﬁts, such as enabling new products to
be developed, sharing useful know-how, and capitalizing on economies of scale and scope [194]. These
beneﬁts need to be balanced against possible harms from coll aboration such as reduced competition on
pricing or output levels, reducing the pace of progress, or i ncreasing the uniformity of outputs.99
If the right antitrust governance procedures are in place, j oint activities between competitive AI labs can
both enhance consumer welfare and enhance intra-industry t rust. Nevertheless, it is important to not
allow the goal of supporting veriﬁable claims to provide cov er for practices that would harm consumer
welfare and therefore erode trust between society and AI lab s collectively .
96Practices are condemned per se " [o]nce experience with [that]particular kind of restraint enables the Court to predict wi th
conﬁdence that[antitrust analysis ]will condemn it..." [189]
97See id . at 695.
98Idat 696.
99See generally id.
70VI Supplemental Mechanism Analysis
A Formal Veriﬁcation
Formal veriﬁcation techniques for ML-based AI systems are s till in their infancy . Challenges include:
•Generating formal claims and corresponding proofs regardi ng the behavior of ML models, given
that their output behavior may not always be clear or expecte d relative to the inputs (e.g., an
ML model will not necessarily display the same behavior in th e ﬁeld that it exhibited under a
testing environment). As a consequence, traditional forma l properties must be reconceived and
redeveloped for ML models;
•The difﬁculty of correctly modeling certain ML systems as ma thematical objects, especially if their
building blocks cannot be formalised within mathematical d omains utilized by existing veriﬁcation
techniques; and
•The size of real-world ML models, which are usually larger th an existing veriﬁcation techniques
can work with.
Some preliminary research [195]has attempted to ﬁnd ways of specifying types of ML robustnes s that
would be amenable to formal veriﬁcation: for example, point wise robustness. Pointwise robustness is a
property that states that an ML model is robust against some m odel of adversarial attacks and perturba-
tions at a given point [58]. However, researchers [140]have observed that the maturity and applicability
of both the speciﬁcation and corresponding techniques fall short of justifying functionality , dependabil-
ity , and security claims. In general, most system dependabi lity properties have gone unspeciﬁed,100and
these methodologies have not accounted for speciﬁcations t hat are more unique to ML-based systems.
Other efforts[69][196][197]aim to verify more traditional speciﬁcations regarding ML a lgorithms.
Some of these techniques require functional speciﬁcations , written as constraints, to be fed into special-
ized solvers which then attempt to verify that they hold on a c onstraint model of the ML system. The
generalization of these techniques to deep learning is chal lenging because they require well-deﬁned,
mathematically speciﬁable properties as input which are no t unique to ML algorithms (given that such
properties do not easily lend themselves to such speciﬁcati ons). These techniques are only applicable
to well-speciﬁed deterministic or tractable systems that c an be implemented using traditional methods
(e.g., the C programming language) or via ML models. As a cons equence, these techniques cannot be
straightforwardly applied to arbitrary contexts, and doma in-speciﬁc effort is currently required even to
specify properties of interest, let alone verify them.
Indeed, there is much progress to be made with regard to the ve riﬁcation of deep neural networks, but
formal veriﬁcation can still be effectively utilised to rei nforce non-ML software employed to construct
the ML model itself. For example, researchers have demonstr ated a methodology in which developers
can use an interactive proof assistant to implement their ML system and prove formal theorems that their
implementation is free of errors [198]. Others have shown that overﬂow and underﬂow errors within
supporting software can propagate and affect the functiona lity of an ML model [199]. Additionally ,
researchers have identiﬁed a number of different run-time e rrors using a traditional formal methods-
based static-analysis tool to analyze YOLO, a commonly used open source ML vision software [140].
Issues identiﬁed include:
100For example: functionality, performance, reliability, av ailability, security, etc.
71•A number of memory leaks, such as ﬁles opened and not closed, a nd temporarily allocated data
not freed, leading to unpredictable behavior, crashes, and corrupted data;
•A large number of calls to free where the validity of the retur ned data is not checked. This could
lead to incorrect (but potentially plausible) weights bein g loaded to the network;
•Potential "divide by zeros" in the training code. This could lead to crashes during online training,
if the system were to be used in such a way; and
•Potential ﬂoating-point "divide by zeros," some of which we re located in the network cost calcula-
tion function. As noted above, this could be an issue during o nline training.
We note that many of the above errors are only applicable to la nguages such as C and C ++(i.e., statically
typed languages), and not Python, a language widely used in t he implementation of numerous ML
libraries and frameworks. As a dynamically typed language, Python brings about a different set of
program errors not typically exhibited by statically typed languages (e.g., type errors). Unfortunately ,
formal veriﬁcation techniques for the analysis of Python co de are inherently limited, with linters and
type checkers being the main available source of static anal ysis tools.
Though the Python situation differs from that encountered w ith C and C++, there are many ways that
potential faults arising from Python could affect the funct ionality of an ML model. This is a large gap
within the formal veriﬁcation ﬁeld that needs to be addresse d immediately , given the deployment of
safety-critical AI systems, such as autonomous vehicles, u tilizing Python. Previous research efforts101
[200]have attempted to formalise a subset of Python that would be a menable to veriﬁcation; however,
it has been notoriously difﬁcult to formalise and verify [201]dynamically typed languages. Although
optional static type hinting is now available for Python,102"the Python runtime does not enforce function
and variable type annotations. [Hints]can be used by third party tools such as type checkers, IDEs,
linters, etc." Furthermore, it is unlikely that the ML commu nity will constrain themselves to subsets of
Python which are statically-typed.103
Formal veriﬁcation techniques have been widely deployed fo r traditional safety-critical systems (as re-
quired by IEC 61508) for several decades, and have more recen tly been adopted by some tech companies
for speciﬁc applications.104However, the rapid introduction of machine learning in thes e environments
has posed a great challenge from both a regulatory and system assurance point of view. The lack of
applicable formal veriﬁcation techniques for AI systems st iﬂes the assurance avenues required to build
trust (i.e., regulations, veriﬁcation, and validation), c urbing the potential innovation and beneﬁts to be
gained from their deployment. The following open research p roblems must thus be addressed to allow
formal veriﬁcation to contribute to trust in AI development :
•Creation of speciﬁcations unique to AI, with corresponding mathematical frameworks, to con-
tribute to assurance of AI systems;
•Creation of novel formal veriﬁcation techniques which can a ddress the newly deﬁned speciﬁcations
mentioned above; and
101See Python semantics: ( https://github.com/kframework/python-semantics ).
102See ofﬁcial Python documentation ( https://docs.python.org/3/library/typing.html ) and MyPy ( h
ttps://github.com/python/mypy ).
103See the discussion following the feature request in the Tens orFlow codebase ( https://github.com/tensorflo
w/tensorflow/issues/12345 ).
104See Infer, an open source static analyzer ( https://fbinfer.com/ ).
72•Collaboration between ML and veriﬁcation researchers resu lting in deep learning systems that are
more amenable to veriﬁcation [202].
73B Veriﬁable Data Policies in Distributed Computing Systems
Current IT systems do not provide a mechanism to enforce a dat a policy (e.g., sharing restrictions,
anonymity restrictions) on data that is shared with another party - individuals and organizations are
required to trust that the data will be used according to thei r preferences. Google’s Project Oak105
aims to address this gap, by providing a reference implement ation of open source infrastructure for the
veriﬁably secure storage, processing, and exchange of any t ype of data.
With Oak, data is collected and used as it is today , but it is al so accompanied by enforceable policies that
deﬁne appropriate access, collection, and use. Data is stor ed in encrypted enclaves and remote attes-
tation between enclaves ensures that only appropriate code ever gets direct access to the secured data
(i.e. within the limits of what can be veriﬁed and as deﬁned by a conﬁgurable policy), and processing
of the data creates a veriﬁable record. In the long term, Goog le’s objective is to provide formal proofs
such that core properties of the system can be veriﬁed down to the hardware. Platforms that implement
this infrastructure could then form the bedrock of all sorts of other services from messaging, machine
learning, and identity management to operating system host ing, making meaningful control technically
feasible in a way that it is not today .
Oak uses enclaves and formal veriﬁcation. Taken together, i t is possible to verify that data is only pro-
cessed in a way that complies with a conﬁgurable policy that g oes with it. In short, data lives in enclaves
and moves from one enclave to another only when the sending en clave is able to convince itself that the
receiving enclave will obey the policy that goes with the dat a and will itself perform the same veriﬁcation
step before sending the data (or data derived from it) on to ot her enclaves. Movement outside enclaves
is only permitted when encrypted with keys available only to the enclave, or as allowed by policy (for
example, to show the data to speciﬁc people or when adequatel y anonymized, again speciﬁed by policy).
Oak combines formal veriﬁcation and remote attestation wit h binary transparency . Oak is being devel-
oped entirely as an open source project - this is deliberate a nd necessary . Because Oak is open source,
even in the absence of formal proofs, any independent third p arty (whether an individual researcher,
regulatory body , or consumer advocacy group) can examine Oa k’s source code and conﬁrm that the im-
plementation matches the expected behavior. With the corre ctness of Oak conﬁrmed insofar as possible,
a given Oak virtual machine needs to be able to attest that it i s running the "correct" binary of Oak.
This attestation makes it possible for the client (or sendin g enclave) to assure itself that the requesting
enclave will follow any policies on the data, because it know s that the Oak policy enforcement system
is running on that enclave and is "truly" Oak - that is: matche s the binary of Oak known from the open
source repository .
Usability and independent auditability are crucial to an Oa k system’s utility . Four types of actors are
expected to interact with Oak:
•End users: people who will use Oak apps;
•Application developers: people who will build Oak apps;
•Policy authors: people who will deﬁne and manage the policie s that accompany the data in an
Oak app;
105More detail on Oak’s technical aspects, including instruct ions for how to write programs targeting the current iterati on of
Oak, can be found on the Oak GitHub repo: https://github.com/project-oak
74•Veriﬁers: people who will add credibility to an Oak app by ver ifying that the policy is upheld.
A user-centric design perspective points to many questions such as: What do people need to know about
an Oak app when making a decision about whether to use it? How w ill people understand the effective
policy an Oak app is bound by? How will people’s preferences b e captured? How will we help people ﬁnd
veriﬁers to delegate trust decisions to? If Oak apps and data policies change, or a veriﬁer’s assessment
of a policy changes (which we expect can and will happen), how is this change communicated to the
people who need to know?
As important as it is to ensure end-users avoid critical mist akes and can understand the impact of Oak,
it is even more important to ensure developers are able to avo id critical mistakes when using Oak. This
requires deliberate design of how app makers will build, dep loy , and debug.
An Oak node without a correct and useful policy is useless. Oa k does not provide privacy by default,
it does so only if the policies speciﬁed are privacy-preserv ing. Thus, the user experience of specifying
and maintaining the policy that accompanies data is crucial to the successful use of Oak. Policy authors
will begin with a set of policy goals that will be reﬁned into a natural language representation of a set
of rules and will likely be translated into a set of rules that can be enforced by an Oak node. The policy
language for those rules will need to be determined based on t he types of protection that is relevant to
use cases and the rules that can be veriﬁably enforced. In som e cases, there will be desirable attestations
that cannot be formally veriﬁed (e.g., non-commercial use) . Depending on the context, policies may be
generated and managed by a group of people, sometimes the dev eloper and sometimes a cross-functional
team from within a company .
75C Interpretability
What has interpretability research focused on?
Interpretability research includes work in areas such as ex plaining a speciﬁc prediction [203][204][205]
[206], explaining global model behavior [207][195][208][209][210], building more interpretable
models[211][212][208][213][214][215][216], interactive visualization tools for human explo-
ration[217][218][122][219][220], and analyzing functional sub-components of neural networ ks to
understand what they are doing [221]. These areas of work are characterized separately below, al though
several overlap and interact with one another.
Explaining a speciﬁc prediction . Many techniques seek to explain a model’s prediction on som e given
input. For example, one might ask which part of the input–for image models, this might take the form of a
heatmap over input pixels [222][223][224][225][226][227][228]–or which training examples [203]
[204][205][206]were responsible for the model’s prediction. By examining t he model’s reasoning on
different input instances through these attribution metho ds, we can better decide whether or not to
trust the model: if a model, say , predicts that an image conta ins a wolf because of the snowy image
background and not because of any actual wolf features, then we can extrapolate that it is likely to
misclassify future images with snowy backgrounds [92].
Explaining global model behavior . Instead of explaining individual predictions, other tech niques aim
to construct human-understandable representations of a mo del’s global behavior, so that users can more
directly interrogate what a model might do on different inpu ts rather than having to extrapolate from
explanations of previous predictions. Examples include ap proximating a complex model with a simpler,
more interpretable model (like a shallow decision tree) [207][195][229][209][210]; or characterizing
the role and contribution of internal components of a model ( e.g., feature visualization or development
of geometric invariance) [223][228][86][230][231][232][233][234][235][236].
Current directions in interpretability research
Building more interpretable models . A separate but complementary line of work seeks to build mod els
that are constrained to be interpretable by design (as oppos ed to training a complex, hard-to-interpret
model and then attempting to analyze it post-hoc with one of t he above techniques) [211][212][208]
[213][214][215][216].
Interactive visualization tools for human exploration . A related research theme is the development
of tools that allow humans to interact with, modify , and expl ore an ML system (e.g., dashboards to
visualize model predictions and errors [217][218]; explanations of model representations [219][220]
[237][238]; or directly interacting with an AI agent [122].106).
Software and tools for practitioners . Most interpretability tools that have been developed are b est
used by ML researchers for interpretability research. A few software packages that are less research-
oriented allow novice users to better understand a dataset’ s distribution and inspect a model interac-
tively;107these packages primarily fall under the category of "intera ctive visualization tools." Moreover,
106Also see Google Quickdraw ( https://quickdraw.withgoogle.com/ ) and Bach doodle ( https://www.g
oogle.com/doodles/celebrating-johann-sebastian-bach ).
107See What-if tool for ML model exploration ( https://pair-code.github.io/what-if-tool/ ) and Facets
76most open-sourced code from interpretability research pri marily focuses on the method being intro-
duced and rarely include standardized benchmarks and compa risons with related work, with some ex-
ceptions.108We hope to see more software packages that empower novice use rs to use interpretability
techniques effectively as well as aid researchers by provid ing standardized benchmarks for comparing
methods. Lastly , much work is focused on interpretability a t a particular scale (i.e., individual examples
vs. dataset distribution); we desire more work at connectin g interpretability work along different axes
and scales[234].
for data exploration ( https://pair-code.github.io/facets/ ).
108See saliency repository ( /urlhttps://github.com/PAIR-code/saliency), InterpretML ( https://github.com/inter
pretml/interpret ), and TreeInterpreter ( https://github.com/andosa/treeinterpreter ).
77List of Recommendations for Reference
Institutional Mechanisms and Recommendations
1. A coalition of stakeholders should create a task force to r esearch options for conducting and fund-
ingthird party auditing of AI systems.
2. Organizations developing AI should run red teaming exercises to explore risks associated with
systems they develop, and should share best practices and to ols for doing so.
3. AI developers should pilot bias and safety bounties for AI systems to strengthen incentives and
processes for broad-based scrutiny of AI systems.
4. AI developers should share more information about AI incidents , including through collaborative
channels.
Software Mechanisms and Recommendations
5. Standards setting bodies should work with academia and in dustry to develop audit trail require-
ments for safety-critical applications of AI systems.
6. Organizations developing AI and funding bodies should su pport research into the interpretability
of AI systems, with a focus on supporting risk assessment and auditing.
7. AI developers should develop, share, and use suites of too ls for privacy-preserving machine
learning that include measures of performance against common standa rds.
Hardware Mechanisms and Recommendations
8. Industry and academia should work together to develop hardware security features for AI ac-
celerators or otherwise establish best practices for the us e of secure hardware (including secure
enclaves on commodity hardware) in machine learning contex ts.
9. One or more AI labs should estimate the computing power inv olved in a single project in great
detail ( high-precision compute measurement ), and report on the potential for wider adoption
of such methods.
10. Government funding bodies should substantially increa sefunding of computing power resources
for researchers in academia, in order to improve the ability of those researchers to verify claims
made by industry .
78