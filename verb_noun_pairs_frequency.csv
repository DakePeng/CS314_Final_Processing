Sentence,Verb,Noun Phrase,Root Noun,Frequency
By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.,improve,the weights,weight,1
By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.,improve,this way,way,1
By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.,improve,we,we,2
By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.,improve,the conditioning,conditioning,1
By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.,improve,the optimization problem,problem,1
By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.,improve,convergence,convergence,1
By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.,improve,stochastic gradient descent,descent,1
Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.,inspire,Our reparameterization,reparameterization,1
Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.,inspire,batch normalization,normalization,1
Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.,inspire,any dependencies,dependency,1
Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.,inspire,the examples,example,1
Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.,inspire,a minibatch,minibatch,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,This,this,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,our method,method,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,recurrent models,model,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,LSTMs,lstm,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,noise-sensitive applications,application,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,deep reinforcement learning,learning,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,generative models,model,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,which,which,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",mean,batch normalization,normalization,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,our method,method,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,recurrent models,model,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,LSTMs,lstm,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,noise-sensitive applications,application,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,deep reinforcement learning,learning,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,generative models,model,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,which,which,1
"This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.",apply,batch normalization,normalization,1
"Although our method is much simpler, it still provides much of the speed-up of full batch normalization.",provide,our method,method,1
"Although our method is much simpler, it still provides much of the speed-up of full batch normalization.",provide,it,it,1
"Although our method is much simpler, it still provides much of the speed-up of full batch normalization.",provide,the speed-up,up,1
"Although our method is much simpler, it still provides much of the speed-up of full batch normalization.",provide,full batch normalization,normalization,1
"In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time.",permit,more optimization steps,step,1
"In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time.",permit,the same amount,amount,1
"In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time.",permit,time,time,1
"We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",demonstrate,We,we,1
"We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",demonstrate,the usefulness,usefulness,1
"We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",demonstrate,our method,method,1
"We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",demonstrate,applications,application,1
"We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",demonstrate,supervised image recognition,recognition,1
"We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",demonstrate,generative modelling,modelling,1
"We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",demonstrate,deep reinforcement learning,learning,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,1 Introduction Recent successes,success,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,deep learning,learning,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,neural networks,network,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,ﬁrst-order gradient based optimization,optimization,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,amazing results,result,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,diverse domains,domain,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,computer vision,vision,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,speech recognition,recognition,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",show,language modelling,modelling,1
"1 Introduction Recent successes in deep learning have shown that neural networks trained by ﬁrst-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling .",train,ﬁrst-order gradient based optimization,optimization,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,the condition number,number,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,the Hessian matrix,matrix,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,the objective,objective,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,the optimum,optimum,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,the problem,problem,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,pathological curvature,curvature,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,ﬁrst-order gradient descent,descent,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,trouble,trouble,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",say,progress,progress,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",exhibit,pathological curvature,curvature,1
"If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature , and ﬁrst-order gradient descent will have trouble making progress [ 18,28].",make,progress,progress,1
"The amount of curvature, and thus the success of our optimization, is not invariant to reparameterization : there may be multiple equivalent ways of parameterizing the same model, some of which are much easier to optimize than others.",optimize,others,other,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",differ,the architectures,architecture,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",differ,neural networks,network,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",differ,applications,application,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compose,the architectures,architecture,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compose,neural networks,network,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compose,applications,application,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compose,they,they,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compose,conceptually simple computational building blocks,block,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compose,neurons,neuron,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",call,neurons,neuron,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,the architectures,architecture,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,neural networks,network,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,applications,application,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,they,they,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,conceptually simple computational building blocks,block,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,neurons,neuron,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,each such neuron,neuron,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,a weighted sum,sum,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,its inputs,input,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,a bias term,term,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,the application,application,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",compute,an elementwise nonlinear transformation,transformation,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",follow,the application,application,1
"While the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation.",follow,an elementwise nonlinear transformation,transformation,1
"Improving the general optimizability of deep networks is a challenging task , but since many neural architectures share these basic building blocks, improving these building blocks improves the performance of a very wide range of model architectures and could thus be very useful.",share,many neural architectures,architecture,1
"Improving the general optimizability of deep networks is a challenging task , but since many neural architectures share these basic building blocks, improving these building blocks improves the performance of a very wide range of model architectures and could thus be very useful.",share,these basic building blocks,block,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,develop,Several authors,author,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,develop,methods,method,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,develop,the conditioning,conditioning,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,develop,the cost gradient,gradient,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,develop,general neural network architectures,architecture,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,improve,the conditioning,conditioning,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,improve,the cost gradient,gradient,1
Several authors have recently developed methods to improve the conditioning of the cost gradient for general neural network architectures.,improve,general neural network architectures,architecture,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",leave,the cost gradient,gradient,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",leave,an approximate inverse,inverse,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",leave,the Fisher information matrix,matrix,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",leave,an approximately whitened natural gradient,gradient,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",multiply,the cost gradient,gradient,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",multiply,an approximate inverse,inverse,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",multiply,the Fisher information matrix,matrix,1
"One approach is to explicitly left multiply the cost gradient with an approximate inverse of the Fisher information matrix, thereby obtaining an approximately whitened natural gradient .",obtain,an approximately whitened natural gradient,gradient,1
"Such an approximate inverse can for example be obtained by using a Kronecker factored approximation to the Fisher matrix and inverting it (KFAC, ), by using anarXiv:1602.07868v3  ",obtain,Such an approximate inverse,inverse,1
"Such an approximate inverse can for example be obtained by using a Kronecker factored approximation to the Fisher matrix and inverting it (KFAC, ), by using anarXiv:1602.07868v3  ",obtain,example,example,1
"Such an approximate inverse can for example be obtained by using a Kronecker factored approximation to the Fisher matrix and inverting it (KFAC, ), by using anarXiv:1602.07868v3  ",obtain,a Kronecker factored approximation,approximation,1
"Such an approximate inverse can for example be obtained by using a Kronecker factored approximation to the Fisher matrix and inverting it (KFAC, ), by using anarXiv:1602.07868v3  ",obtain,the Fisher matrix,matrix,1
"Such an approximate inverse can for example be obtained by using a Kronecker factored approximation to the Fisher matrix and inverting it (KFAC, ), by using anarXiv:1602.07868v3  ",obtain,it,it,1
"Such an approximate inverse can for example be obtained by using a Kronecker factored approximation to the Fisher matrix and inverting it (KFAC, ), by using anarXiv:1602.07868v3  ",obtain,(KFAC,KFAC,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,we,we,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,standard ﬁrst order gradient descent,descent,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,the parameterization,parameterization,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,our model,model,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,gradients,gradient,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,that,that,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,the whitened natural gradients,gradient,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",use,these methods,method,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",give,gradients,gradient,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",give,that,that,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",give,the whitened natural gradients,gradient,1
"Alternatively, we can use standard ﬁrst order gradient descent without preconditioning, but change the parameterization of our model to give gradients that are more like the whitened natural gradients of these methods.",give,these methods,method,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",propose,example,example,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",propose,Raiko,Raiko,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",propose,et al,al,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",propose,the outputs,output,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",propose,each neuron,neuron,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",propose,zero output,output,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",propose,zero slope,slope,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",transform,the outputs,output,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",transform,each neuron,neuron,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",transform,zero output,output,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",transform,zero slope,slope,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",have,zero output,output,1
"For example, Raiko et al.  propose to transform the outputs of each neuron to have zero output and zero slope on average.",have,zero slope,slope,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",show,They,they,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",show,this transformation,transformation,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",show,the Fisher information matrix,matrix,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",show,the gradient,gradient,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",show,this,this,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",show,improved optimization performance,performance,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",diagonalize,this transformation,transformation,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",diagonalize,the Fisher information matrix,matrix,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",diagonalize,the gradient,gradient,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",diagonalize,this,this,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",diagonalize,improved optimization performance,performance,1
"They show that this transformation approximately diagonalizes the Fisher information matrix, thereby whitening the gradient, and that this leads to improved optimization performance.",whiten,the gradient,gradient,1
"Another approach in this direction is batch normalization , a method where the output of each neuron (before application of the nonlinearity) is normalized by the mean and standard deviation of the outputs calculated over the examples in the minibatch.",calculate,the examples,example,1
"Another approach in this direction is batch normalization , a method where the output of each neuron (before application of the nonlinearity) is normalized by the mean and standard deviation of the outputs calculated over the examples in the minibatch.",calculate,the minibatch,minibatch,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,reduce,This,this,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,reduce,covariate shift,shift,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,reduce,the neuron outputs,output,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,reduce,the authors,author,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,reduce,it,it,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,reduce,the Fisher matrix,matrix,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,reduce,the identity matrix,matrix,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,bring,it,it,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,bring,the Fisher matrix,matrix,1
This reduces covariate shift of the neuron outputs and the authors suggest it also brings the Fisher matrix closer to the identity matrix.,bring,the identity matrix,matrix,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,this second approach,approach,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,approximate natural gradient optimization,optimization,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,we,we,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,a simple but general method,method,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,weight normalization,normalization,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,the optimizability,optimizability,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,the weights,weight,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",propose,neural network models,model,1
"Following this second approach to approximate natural gradient optimization, we propose a simple but general method, called weight normalization , for improving the optimizability of the weights of neural network models.",call,weight normalization,normalization,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,The method,method,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,batch normalization,normalization,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,it,it,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,a deterministic method,method,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,that,that,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,batch normalization’s property,property,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,noise,noise,1
"The method is inspired by batch normalization, but it is a deterministic method that does not share batch normalization’s property of adding noise to the gradients.",inspire,the gradients,gradient,1
"In addition, the overhead imposed by our method is lower: no additional memory is required and the additional computation is negligible.",impose,our method,method,1
"In addition, the overhead imposed by our method is lower: no additional memory is required and the additional computation is negligible.",require,addition,addition,1
"In addition, the overhead imposed by our method is lower: no additional memory is required and the additional computation is negligible.",require,the overhead,overhead,1
"In addition, the overhead imposed by our method is lower: no additional memory is required and the additional computation is negligible.",require,our method,method,1
"In addition, the overhead imposed by our method is lower: no additional memory is required and the additional computation is negligible.",require,no additional memory,memory,1
"In addition, the overhead imposed by our method is lower: no additional memory is required and the additional computation is negligible.",require,the additional computation,computation,1
The method show encouraging results on a wide range of deep learning applications.,show,The method,method,1
The method show encouraging results on a wide range of deep learning applications.,show,results,result,1
The method show encouraging results on a wide range of deep learning applications.,show,a wide range,range,1
The method show encouraging results on a wide range of deep learning applications.,show,deep learning applications,application,1
The method show encouraging results on a wide range of deep learning applications.,encourage,results,result,1
The method show encouraging results on a wide range of deep learning applications.,encourage,a wide range,range,1
The method show encouraging results on a wide range of deep learning applications.,encourage,deep learning applications,application,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,We,we,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,standard artiﬁcial neural networks,network,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,the computation,computation,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,each neuron,neuron,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,a weighted sum,sum,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,input features,feature,2
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,an elementwise nonlinearity,nonlinearity,2
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,wx+b,wx+b,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,(1) where wis ak-dimensional weight vector,vector,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,a scalar bias term,term,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,xis ak-dimensional vector,vector,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,the rectiﬁer,rectiﬁer,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,max(:;0,max(:;0,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,the scalar output,output,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",consider,the neuron,neuron,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",follow,an elementwise nonlinearity,nonlinearity,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",follow,wx+b,wx+b,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",follow,(1) where wis ak-dimensional weight vector,vector,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",denote,xis ak-dimensional vector,vector,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",denote,input features,feature,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",denote,an elementwise nonlinearity,nonlinearity,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",denote,the rectiﬁer,rectiﬁer,1
"2 Weight Normalization We consider standard artiﬁcial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity: y=(wx+b); (1) where wis ak-dimensional weight vector, bis a scalar bias term, xis ak-dimensional vector of input features,(:)denotes an elementwise nonlinearity such as the rectiﬁer max(:;0), andydenotes the scalar output of the neuron.",denote,max(:;0,max(:;0,1
"After associating a loss function to one or more neuron outputs, such a neural network is commonly trained by stochastic gradient descent in the parameters w;bof each neuron.",train,a loss function,function,1
"After associating a loss function to one or more neuron outputs, such a neural network is commonly trained by stochastic gradient descent in the parameters w;bof each neuron.",train,one or more neuron outputs,output,1
"After associating a loss function to one or more neuron outputs, such a neural network is commonly trained by stochastic gradient descent in the parameters w;bof each neuron.",train,such a neural network,network,1
"After associating a loss function to one or more neuron outputs, such a neural network is commonly trained by stochastic gradient descent in the parameters w;bof each neuron.",train,stochastic gradient descent,descent,1
"After associating a loss function to one or more neuron outputs, such a neural network is commonly trained by stochastic gradient descent in the parameters w;bof each neuron.",train,the parameters,parameter,1
"After associating a loss function to one or more neuron outputs, such a neural network is commonly trained by stochastic gradient descent in the parameters w;bof each neuron.",train,each neuron,neuron,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",speed,the convergence,convergence,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",speed,this optimization procedure,procedure,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,an effort,effort,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,the convergence,convergence,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,this optimization procedure,procedure,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,we,we,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,each weight vector w,w,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,terms,term,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,a scalar parameter gand,gand,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,stochastic gradient descent,descent,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,respect,respect,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",propose,those parameters,parameter,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",reparameterize,each weight vector w,w,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",reparameterize,terms,term,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",reparameterize,a scalar parameter gand,gand,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",reparameterize,stochastic gradient descent,descent,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",reparameterize,respect,respect,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",reparameterize,those parameters,parameter,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",perform,stochastic gradient descent,descent,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",perform,respect,respect,1
"In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector vand a scalar parameter gand to perform stochastic gradient descent with respect to those parameters instead.",perform,those parameters,parameter,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,We,we,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,the weight vectors,vector,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,terms,term,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,the new parameters,parameter,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,w,w,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,=,=,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,ak-dimensional vector,vector,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",do,a scalar,scalar,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",use,w,w,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",use,=,=,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",gi,a scalar,scalar,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,We,we,2
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,the weight vectors,vector,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,terms,term,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,the new parameters,parameter,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,w,w,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,=,=,2
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,ak-dimensional vector,vector,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,a scalar,scalar,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,the Euclidean norm,norm,2
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,This reparameterization,reparameterization,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,the effect,effect,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,the weight vector w,w,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,we,we,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,havejjwjj,havejjwjj,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,g,g,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,the parameters,parameter,1
"We do so by expressing the weight vectors in terms of the new parameters using w=g jjvjjv (2) where vis ak-dimensional vector, gis a scalar, andjjvjjdenotes the Euclidean norm of v. This reparameterization has the effect of ﬁxing the Euclidean norm of the weight vector w: we now havejjwjj=g, independent of the parameters v. We therefore call this reparameterizaton weight normalization .",andjjvjjdenote,this reparameterizaton weight normalization,normalization,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,The idea,idea,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,the weight vector,vector,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,earlier work,work,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,optimization,optimization,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,the w-parameterization,parameterization,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,the normalization,normalization,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,each step,step,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",propose,stochastic gradient descent,descent,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",apply,the normalization,normalization,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",apply,each step,step,1
"The idea of normalizing the weight vector has been proposed before (e.g. ) but earlier work typically still performed optimization in the w-parameterization, only applying the normalization after each step of stochastic gradient descent.",apply,stochastic gradient descent,descent,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,propose,This,this,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,propose,our approach,approach,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,propose,we,we,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,propose,the model,model,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,propose,stochastic gradient descent,descent,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,propose,the new parameters,parameter,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,reparameterize,the model,model,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,reparameterize,stochastic gradient descent,descent,1
This is fundamentally different from our approach: we propose to explicitly reparameterize the model and to perform stochastic gradient descent in the new parameters v;gdirectly.,reparameterize,the new parameters,parameter,1
Doing so improves the conditioning of the gradient and leads to improved convergence of the optimization procedure:,improve,the conditioning,conditioning,1
Doing so improves the conditioning of the gradient and leads to improved convergence of the optimization procedure:,improve,the gradient,gradient,1
Doing so improves the conditioning of the gradient and leads to improved convergence of the optimization procedure:,improve,improved convergence,convergence,1
Doing so improves the conditioning of the gradient and leads to improved convergence of the optimization procedure:,improve,the optimization procedure,procedure,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,the norm,norm,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,the weight vector,vector,2
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,g,g,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,the direction,direction,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,we,we,2
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,convergence,convergence,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,our stochastic gradient descent optimization,optimization,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",speed,section,section,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",show,we,we,1
"By decoupling the norm of the weight vector ( g) from the direction of the weight vector ( v=jjvjj), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5.",show,section,section,1
"Instead of working with gdirectly, we may also use an exponential parameterization for the scale, i.e. g=es, wheresis a log-scale parameter to learn by stochastic gradient descent.",use,we,we,1
"Instead of working with gdirectly, we may also use an exponential parameterization for the scale, i.e. g=es, wheresis a log-scale parameter to learn by stochastic gradient descent.",use,an exponential parameterization,parameterization,1
"Instead of working with gdirectly, we may also use an exponential parameterization for the scale, i.e. g=es, wheresis a log-scale parameter to learn by stochastic gradient descent.",use,the scale,scale,1
"Instead of working with gdirectly, we may also use an exponential parameterization for the scale, i.e. g=es, wheresis a log-scale parameter to learn by stochastic gradient descent.",use,a log-scale parameter,parameter,1
"Instead of working with gdirectly, we may also use an exponential parameterization for the scale, i.e. g=es, wheresis a log-scale parameter to learn by stochastic gradient descent.",use,stochastic gradient descent,descent,1
"Instead of working with gdirectly, we may also use an exponential parameterization for the scale, i.e. g=es, wheresis a log-scale parameter to learn by stochastic gradient descent.",wheresis,a log-scale parameter,parameter,1
"Instead of working with gdirectly, we may also use an exponential parameterization for the scale, i.e. g=es, wheresis a log-scale parameter to learn by stochastic gradient descent.",wheresis,stochastic gradient descent,descent,1
"Empirically, however, we did not ﬁnd this to be an advantage.",ﬁnd,we,we,1
"Empirically, however, we did not ﬁnd this to be an advantage.",ﬁnd,this,this,1
"Empirically, however, we did not ﬁnd this to be an advantage.",ﬁnd,an advantage,advantage,1
22.1 Gradients Training a neural network in the new parameterization is done using standard stochastic gradient descent methods.,train,a neural network,network,1
22.1 Gradients Training a neural network in the new parameterization is done using standard stochastic gradient descent methods.,train,the new parameterization,parameterization,1
22.1 Gradients Training a neural network in the new parameterization is done using standard stochastic gradient descent methods.,do,22.1 Gradients,gradient,1
22.1 Gradients Training a neural network in the new parameterization is done using standard stochastic gradient descent methods.,do,a neural network,network,1
22.1 Gradients Training a neural network in the new parameterization is done using standard stochastic gradient descent methods.,do,the new parameterization,parameterization,1
22.1 Gradients Training a neural network in the new parameterization is done using standard stochastic gradient descent methods.,do,standard stochastic gradient descent methods,method,1
22.1 Gradients Training a neural network in the new parameterization is done using standard stochastic gradient descent methods.,use,standard stochastic gradient descent methods,method,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,differentiate,we,we,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,differentiate,2)to,2)to,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,differentiate,the gradient,gradient,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,differentiate,a loss function,function,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,differentiate,Lwith respect,respect,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,differentiate,the new parameters,parameter,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,obtain,2)to,2)to,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,obtain,the gradient,gradient,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,obtain,a loss function,function,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,obtain,Lwith respect,respect,1
Here we differentiate through (2)to obtain the gradient of a loss function Lwith respect to the new parameters v;g.,obtain,the new parameters,parameter,1
Doing so gives rgL=rwLv jjvjj;rvL=g jjvjjrwL grgL jjvjj2v; (3) whererwLis,give,rgL,rgL,1
Doing so gives rgL=rwLv jjvjj;rvL=g jjvjjrwL grgL jjvjj2v; (3) whererwLis,give,=,=,1
Doing so gives rgL=rwLv jjvjj;rvL=g jjvjjrwL grgL jjvjj2v; (3) whererwLis,give,rwLv,rwLv,1
Doing so gives rgL=rwLv jjvjj;rvL=g jjvjjrwL grgL jjvjj2v; (3) whererwLis,give,jjvjj;rvL,jjvjj;rvL,1
Doing so gives rgL=rwLv jjvjj;rvL=g jjvjjrwL grgL jjvjj2v; (3) whererwLis,give,g jjvjjrwL grgL jjvjj2v,jjvjj2v,1
the gradient with respect to the weights was used normally.,use,the gradient,gradient,1
the gradient with respect to the weights was used normally.,use,respect,respect,1
the gradient with respect to the weights was used normally.,use,the weights,weight,1
"Backpropagation using weight normalization thus only requires a minor modiﬁcation to the usual backpropagation equations, and is easily implemented using standard neural network software.",use,weight normalization,normalization,1
"Backpropagation using weight normalization thus only requires a minor modiﬁcation to the usual backpropagation equations, and is easily implemented using standard neural network software.",require,Backpropagation,backpropagation,1
"Backpropagation using weight normalization thus only requires a minor modiﬁcation to the usual backpropagation equations, and is easily implemented using standard neural network software.",require,weight normalization,normalization,1
"Backpropagation using weight normalization thus only requires a minor modiﬁcation to the usual backpropagation equations, and is easily implemented using standard neural network software.",require,a minor modiﬁcation,modiﬁcation,1
"Backpropagation using weight normalization thus only requires a minor modiﬁcation to the usual backpropagation equations, and is easily implemented using standard neural network software.",require,the usual backpropagation equations,equation,1
"Backpropagation using weight normalization thus only requires a minor modiﬁcation to the usual backpropagation equations, and is easily implemented using standard neural network software.",require,standard neural network software,software,1
"Backpropagation using weight normalization thus only requires a minor modiﬁcation to the usual backpropagation equations, and is easily implemented using standard neural network software.",use,standard neural network software,software,1
We provide reference implementations for Theano at https://github.com/TimSalimans/weight_ norm .,provide,We,we,1
We provide reference implementations for Theano at https://github.com/TimSalimans/weight_ norm .,provide,reference implementations,implementation,1
We provide reference implementations for Theano at https://github.com/TimSalimans/weight_ norm .,provide,Theano,Theano,1
We provide reference implementations for Theano at https://github.com/TimSalimans/weight_ norm .,provide,https://github.com/TimSalimans/weight_ norm,norm,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",show,This,this,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",show,weight normalization,normalization,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",show,two things,thing,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",accomplish,weight normalization,normalization,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",accomplish,two things,thing,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,This,this,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,weight normalization,normalization,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,two things,thing,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,it,it,2
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,the weight gradient,gradient,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,g,g,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,=,=,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,jjvjj,jjvjj,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,the gradient,gradient,1
"This shows that weight normalization accomplishes two things: it scales the weight gradient by g=jjvjj, and it projects the gradient away from the current weight vector.",scale,the current weight vector,vector,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",help,Both effects,effect,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",help,the covariance matrix,matrix,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",help,the gradient,gradient,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",help,identity and beneﬁt optimization,optimization,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",help,we,we,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",bring,the covariance matrix,matrix,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",bring,the gradient,gradient,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",bring,identity and beneﬁt optimization,optimization,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",bring,we,we,1
"Both effects help to bring the covariance matrix of the gradient closer to identity and beneﬁt optimization, as we explain below.",explain,we,we,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",learn,a neural network,network,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",learn,weight normalization,normalization,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",learn,standard gradient descent,descent,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",learn,momentum,momentum,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",use,standard gradient descent,descent,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",use,momentum,momentum,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,w,w,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,the norm,norm,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,vgrows,vgrows,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,the number,number,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,weight updates,update,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,a neural network,network,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,weight normalization,normalization,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,standard gradient descent,descent,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,momentum,momentum,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,v+,v+,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,our parameter update,update,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,v/rvL(steepest ascent/descent,descent,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,the current weight vector wsince,wsince,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,we,we,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,it,it,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",let,rvL(equation,rvl(equation,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",vdenote,v+,v+,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",vdenote,our parameter update,update,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",vdenote,v/rvL(steepest ascent/descent,descent,1
"Due to projecting away from w, the norm of vgrows monotonically with the number of weight updates when learning a neural network with weight normalization using standard gradient descent without momentum: Let v0=v+ vdenote our parameter update, with v/rvL(steepest ascent/descent), then vis necessarily orthogonal to the current weight vector wsince we project away from it when calculating rvL(equation 4).",calculate,rvL(equation,rvl(equation,1
"Since vis proportional to w, the update is thus also orthogonal to vand increases its norm by the Pythagorean theorem.",increase,its norm,norm,1
"Speciﬁcally, if jjvjj=jjvjj=c the new weight vector will have norm jjv0jj=p jjvjj2+c2jjvjj2=p 1 +c2jjvjjjjvjj.",norm,jjvjj=jjvjj,jjvjj,1
"Speciﬁcally, if jjvjj=jjvjj=c the new weight vector will have norm jjv0jj=p jjvjj2+c2jjvjj2=p 1 +c2jjvjjjjvjj.",norm,the new weight vector,vector,1
"Speciﬁcally, if jjvjj=jjvjj=c the new weight vector will have norm jjv0jj=p jjvjj2+c2jjvjj2=p 1 +c2jjvjjjjvjj.",norm,jjv0jj,jjv0jj,1
"Speciﬁcally, if jjvjj=jjvjj=c the new weight vector will have norm jjv0jj=p jjvjj2+c2jjvjj2=p 1 +c2jjvjjjjvjj.",norm,=,=,1
"Speciﬁcally, if jjvjj=jjvjj=c the new weight vector will have norm jjv0jj=p jjvjj2+c2jjvjj2=p 1 +c2jjvjjjjvjj.",norm,p,p,1
"Speciﬁcally, if jjvjj=jjvjj=c the new weight vector will have norm jjv0jj=p jjvjj2+c2jjvjj2=p 1 +c2jjvjjjjvjj.",norm,c2jjvjjjjvjj,c2jjvjjjjvjj,1
The rate of increase will depend on the the variance of the weight gradient.,depend,The rate,rate,1
The rate of increase will depend on the the variance of the weight gradient.,depend,increase,increase,1
The rate of increase will depend on the the variance of the weight gradient.,depend,the the variance,variance,1
The rate of increase will depend on the the variance of the weight gradient.,depend,the weight gradient,gradient,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,our gradients,gradient,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,cwill,cwill,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,the norm,norm,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,vwill,vwill,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,which,which,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,turn,turn,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,the scaling factor,factor,1
"If our gradients are noisy, cwill be high and the norm of vwill quickly increase, which in turn will decrease the scaling factor g=jjvjj.",be,g=jjvjj,jjvjj,1
"If the norm of the gradients is small, we getp 1 +c21, and the norm of vwill stop increasing.",getp,the norm,norm,2
"If the norm of the gradients is small, we getp 1 +c21, and the norm of vwill stop increasing.",getp,the gradients,gradient,1
"If the norm of the gradients is small, we getp 1 +c21, and the norm of vwill stop increasing.",getp,we,we,1
"If the norm of the gradients is small, we getp 1 +c21, and the norm of vwill stop increasing.",getp,1 +c21,c21,1
"If the norm of the gradients is small, we getp 1 +c21, and the norm of vwill stop increasing.",getp,vwill,vwill,1
"Using this mechanism, the scaled gradient self-stabilizes its norm.",use,this mechanism,mechanism,1
"Using this mechanism, the scaled gradient self-stabilizes its norm.",stabilize,this mechanism,mechanism,1
"Using this mechanism, the scaled gradient self-stabilizes its norm.",stabilize,the scaled gradient,gradient,1
"Using this mechanism, the scaled gradient self-stabilizes its norm.",stabilize,its norm,norm,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,This property,property,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,optimizers,optimizer,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,that,that,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,separate learning rates,rate,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,individual parameters,parameter,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,Adam,Adam,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,which,which,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,we,we,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,experiments,experiment,1
"This property does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam  which we use in experiments, or when using momentum.",hold,momentum,momentum,1
"However, qualitatively we still ﬁnd the same effect to hold.",ﬁnd,we,we,1
"However, qualitatively we still ﬁnd the same effect to hold.",ﬁnd,the same effect,effect,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,we,we,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,the ability,ability,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,the norm jjvjjmakes optimization,optimization,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,neural networks,network,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,weight normalization,normalization,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,the value,value,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,the learning rate,rate,2
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,the norm,norm,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,the unnormalized weights,weight,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",ﬁnd,an appropriate effective learning rate,rate,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,the norm jjvjjmakes optimization,optimization,2
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,neural networks,network,2
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,weight normalization,normalization,2
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,the value,value,2
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,the learning rate,rate,3
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,the ability,ability,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,the norm,norm,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,the unnormalized weights,weight,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",grow,an appropriate effective learning rate,rate,1
"Empirically, we ﬁnd that the ability to grow the norm jjvjjmakes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached.",reach,an appropriate effective learning rate,rate,1
"Once the norm of the weights has grown large with respect to the norm of the updates, the effective learning rate stabilizes.",grow,the norm,norm,2
"Once the norm of the weights has grown large with respect to the norm of the updates, the effective learning rate stabilizes.",grow,the weights,weight,1
"Once the norm of the weights has grown large with respect to the norm of the updates, the effective learning rate stabilizes.",grow,respect,respect,1
"Once the norm of the weights has grown large with respect to the norm of the updates, the effective learning rate stabilizes.",grow,the updates,update,1
Neural networks with weight normalization therefore work well with a much wider range of learning rates than when using the normal parameterization.,work,Neural networks,network,1
Neural networks with weight normalization therefore work well with a much wider range of learning rates than when using the normal parameterization.,work,weight normalization,normalization,1
Neural networks with weight normalization therefore work well with a much wider range of learning rates than when using the normal parameterization.,work,a much wider range,range,1
Neural networks with weight normalization therefore work well with a much wider range of learning rates than when using the normal parameterization.,work,rates,rate,1
Neural networks with weight normalization therefore work well with a much wider range of learning rates than when using the normal parameterization.,work,the normal parameterization,parameterization,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",observe,It,it,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",observe,neural networks,network,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",observe,batch normalization,normalization,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",observe,this property,property,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",observe,which,which,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",observe,this analysis,analysis,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",have,neural networks,network,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",have,batch normalization,normalization,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",have,this property,property,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",have,which,which,1
"It has been observed that neural networks with batch normalization also have this property , which can also be explained by this analysis.",have,this analysis,analysis,1
"By projecting the gradient away from the weight vector w, we also eliminate the noise in that direction.",eliminate,the gradient,gradient,1
"By projecting the gradient away from the weight vector w, we also eliminate the noise in that direction.",eliminate,the weight vector w,w,1
"By projecting the gradient away from the weight vector w, we also eliminate the noise in that direction.",eliminate,we,we,1
"By projecting the gradient away from the weight vector w, we also eliminate the noise in that direction.",eliminate,the noise,noise,1
"By projecting the gradient away from the weight vector w, we also eliminate the noise in that direction.",eliminate,that direction,direction,1
"If the covariance matrix of the gradient with respect to wis given by C, the covariance matrix of the gradient in vis given by D= (g2=jjvjj2)MwCMw.",give,C,C,1
"If the covariance matrix of the gradient with respect to wis given by C, the covariance matrix of the gradient in vis given by D= (g2=jjvjj2)MwCMw.",give,the covariance matrix,matrix,1
"If the covariance matrix of the gradient with respect to wis given by C, the covariance matrix of the gradient in vis given by D= (g2=jjvjj2)MwCMw.",give,the gradient,gradient,1
"If the covariance matrix of the gradient with respect to wis given by C, the covariance matrix of the gradient in vis given by D= (g2=jjvjj2)MwCMw.",give,D=,d=,1
"If the covariance matrix of the gradient with respect to wis given by C, the covariance matrix of the gradient in vis given by D= (g2=jjvjj2)MwCMw.",give,g2,g2,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,we,we,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,wis often (close to) a dominant eigenvector,eigenvector,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,the covariance matrix C,c,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,that eigenvector,eigenvector,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,a new covariance matrix,matrix,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,Dthat,Dthat,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,the identity matrix,matrix,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,which,which,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",ﬁnd,learning,learning,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",remove,that eigenvector,eigenvector,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,wis often (close to) a dominant eigenvector,eigenvector,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,the covariance matrix C,c,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,that eigenvector,eigenvector,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,a new covariance matrix,matrix,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,Dthat,Dthat,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,the identity matrix,matrix,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,which,which,1
"Empirically, we ﬁnd that wis often (close to) a dominant eigenvector of the covariance matrix C: removing that eigenvector then gives a new covariance matrix Dthat is closer to the identity matrix, which may further speed up learning.",give,learning,learning,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",feature,the special case,case,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",feature,our network,network,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",feature,a single layer,layer,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",feature,the input,input,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",feature,layer,layer,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",feature,zero mean and unit variance,variance,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",whiten,layer,layer,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,the special case,case,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,our network,network,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,a single layer,layer,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,the input,input,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,layer,layer,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,zero mean and unit variance,variance,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,these statistics,statistic,1
"For the special case where our network only has a single layer, and the input features xfor that layer are whitened (independently distributed with zero mean and unit variance), these statistics are given by [t] = 0 and[t] =jjvjj.",give,= 0 and[t] =jjvjj,jjvjj,1
"In that case, normalizing the pre-activations using batch normalization is equivalent to normalizing the weights using weight normalization.",use,batch normalization,normalization,1
"In that case, normalizing the pre-activations using batch normalization is equivalent to normalizing the weights using weight normalization.",use,weight normalization,normalization,1
"Convolutional neural networks usually have much fewer weights than pre-activations, so normalizing the weights is often much cheaper computationally.",have,Convolutional neural networks,network,1
"Convolutional neural networks usually have much fewer weights than pre-activations, so normalizing the weights is often much cheaper computationally.",have,much fewer weights,weight,1
"Convolutional neural networks usually have much fewer weights than pre-activations, so normalizing the weights is often much cheaper computationally.",have,pre,pre,1
"Convolutional neural networks usually have much fewer weights than pre-activations, so normalizing the weights is often much cheaper computationally.",have,-,-,1
"Convolutional neural networks usually have much fewer weights than pre-activations, so normalizing the weights is often much cheaper computationally.",have,activations,activation,1
"Convolutional neural networks usually have much fewer weights than pre-activations, so normalizing the weights is often much cheaper computationally.",have,the weights,weight,1
"In addition, the norm of vis non-stochastic, while the minibatch mean [t]and variance 2[t]can in general have high variance for small minibatch size.",have,the minibatch mean [t]and variance,variance,1
"In addition, the norm of vis non-stochastic, while the minibatch mean [t]and variance 2[t]can in general have high variance for small minibatch size.",have,2[t]can,2[t]can,1
"In addition, the norm of vis non-stochastic, while the minibatch mean [t]and variance 2[t]can in general have high variance for small minibatch size.",have,high variance,variance,1
"In addition, the norm of vis non-stochastic, while the minibatch mean [t]and variance 2[t]can in general have high variance for small minibatch size.",have,small minibatch size,size,1
Weight normalization can thus be viewed as a cheaper and less noisy approximation to batch normalization.,view,Weight normalization,normalization,1
Weight normalization can thus be viewed as a cheaper and less noisy approximation to batch normalization.,view,a cheaper and less noisy approximation,approximation,1
Weight normalization can thus be viewed as a cheaper and less noisy approximation to batch normalization.,view,normalization,normalization,1
"Although exact equivalence does not usually hold for deeper architectures, we still ﬁnd that our weight normalization method provides much of the speed-up of full batch normalization.",hold,exact equivalence,equivalence,1
"Although exact equivalence does not usually hold for deeper architectures, we still ﬁnd that our weight normalization method provides much of the speed-up of full batch normalization.",hold,deeper architectures,architecture,1
"Although exact equivalence does not usually hold for deeper architectures, we still ﬁnd that our weight normalization method provides much of the speed-up of full batch normalization.",provide,our weight normalization method,method,1
"Although exact equivalence does not usually hold for deeper architectures, we still ﬁnd that our weight normalization method provides much of the speed-up of full batch normalization.",provide,the speed-up,up,1
"Although exact equivalence does not usually hold for deeper architectures, we still ﬁnd that our weight normalization method provides much of the speed-up of full batch normalization.",provide,full batch normalization,normalization,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,addition,addition,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,its deterministic nature,nature,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,independence,independence,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,the minibatch input,input,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,our method,method,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,models,model,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,RNNs,rnn,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,LSTMs,lstm,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,noise-sensitive applications,application,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",mean,reinforcement learning,learning,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",apply,our method,method,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",apply,models,model,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",apply,RNNs,rnn,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",apply,LSTMs,lstm,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",apply,noise-sensitive applications,application,1
"In addition, its deterministic nature and independence on the minibatch input also means that our method can be applied more easily to models like RNNs and LSTMs, as well as noise-sensitive applications like reinforcement learning.",apply,reinforcement learning,learning,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,3 Data-Dependent Initialization,Initialization,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,Parameters,Parameters,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,a reparameterization effect,effect,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,batch normalization,normalization,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,the beneﬁt,beneﬁt,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,the scale,scale,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,the features,feature,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,each layer,layer,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",have,the neural network,network,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",generate,each layer,layer,1
"3 Data-Dependent Initialization of Parameters Besides a reparameterization effect, batch normalization also has the beneﬁt of ﬁxing the scale of the features generated by each layer of the neural network.",generate,the neural network,network,1
This makes the optimization robust against parameter initializations for which these scales vary across layers.,make,This,this,1
This makes the optimization robust against parameter initializations for which these scales vary across layers.,make,the optimization,optimization,1
This makes the optimization robust against parameter initializations for which these scales vary across layers.,make,parameter initializations,initialization,1
This makes the optimization robust against parameter initializations for which these scales vary across layers.,make,which,which,1
This makes the optimization robust against parameter initializations for which these scales vary across layers.,make,these scales,scale,1
This makes the optimization robust against parameter initializations for which these scales vary across layers.,make,layers,layer,1
"Since weight normalization lacks this property, we ﬁnd it is important to properly initialize our parameters.",lack,weight normalization,normalization,1
"Since weight normalization lacks this property, we ﬁnd it is important to properly initialize our parameters.",lack,this property,property,1
"Since weight normalization lacks this property, we ﬁnd it is important to properly initialize our parameters.",initialize,our parameters,parameter,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,We,we,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,the elements,element,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,vfrom,vfrom,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,a simple distribution,distribution,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,a ﬁxed scale,scale,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,which,which,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,our experiments,experiment,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,a normal distribution,distribution,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,mean,mean,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",propose,standard deviation,deviation,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,the elements,element,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,vfrom,vfrom,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,a simple distribution,distribution,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,a ﬁxed scale,scale,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,which,which,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,our experiments,experiment,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,a normal distribution,distribution,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,mean,mean,1
"We propose to sample the elements of vfrom a simple distribution with a ﬁxed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05.",sample,standard deviation,deviation,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,training,training,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,we,we,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,thebandgparameters,thebandgparameter,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,the minibatch statistics,statistic,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,all pre,pre,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,-,-,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,activations,activation,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,our network,network,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,batch normalization,normalization,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,a single minibatch,minibatch,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,data,datum,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",initialize,initialization,initialization,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",ﬁx,the minibatch statistics,statistic,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",ﬁx,all pre,pre,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",ﬁx,-,-,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",ﬁx,activations,activation,1
"Before starting training, we then initialize thebandgparameters to ﬁx the minibatch statistics of all pre-activations in our network, just like in batch normalization, but only for a single minibatch of data and only during initialization.",ﬁx,our network,network,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,This,this,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,an initial feedforward,feedforward,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,our network,network,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,a single minibatch,minibatch,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,data X,X,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,the following computation,computation,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,each neuron,neuron,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,vx jjvjj;andy=,jjvjj;andy=,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,(5) where[t]and[t]are,where[t]and[t]are,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,the mean and standard deviation,deviation,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,the pre,pre,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,-,-,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,the examples,example,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",do,the minibatch,minibatch,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",use,the following computation,computation,1
"This can be done efﬁciently by performing an initial feedforward pass through our network for a single minibatch of data X, using the following computation at each neuron: t=vx jjvjj;andy=t [t] [t] ; (5) where[t]and[t]are the mean and standard deviation of the pre-activation tover the examples in the minibatch.",use,each neuron,neuron,1
We can then initialize the neuron’s biase band scalegas g 1 [t]; b  [t] [t]; (6) so thaty=(wx+b).,initialize,We,we,1
We can then initialize the neuron’s biase band scalegas g 1 [t]; b  [t] [t]; (6) so thaty=(wx+b).,initialize,the neuron,neuron,1
We can then initialize the neuron’s biase band scalegas g 1 [t]; b  [t] [t]; (6) so thaty=(wx+b).,initialize,band scalegas,scalegas,1
We can then initialize the neuron’s biase band scalegas g 1 [t]; b  [t] [t]; (6) so thaty=(wx+b).,initialize,wx+b,wx+b,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",ensure,batch normalization,normalization,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",ensure,this method,method,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",ensure,all features,feature,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",ensure,zero mean and unit variance,variance,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",ensure,application,application,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",ensure,the nonlinearity,nonlinearity,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",have,all features,feature,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",have,zero mean and unit variance,variance,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",have,application,application,1
"Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity.",have,the nonlinearity,nonlinearity,1
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,our method,method,1
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,this,this,1
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,the minibatch,minibatch,1
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,we,we,2
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,initialization,initialization,1
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,subsequent minibatches,minibatche,1
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,slightly different statistics,statistic,1
"With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we ﬁnd this initialization method to work well.",hold,this initialization method,method,1
"The method can also be applied to networks without weight normalization, simply by doing stochastic gradient optimization on the parameters wdirectly, after initialization in terms of vandg: this is what we compare to in section 5.",apply,The method,method,1
"The method can also be applied to networks without weight normalization, simply by doing stochastic gradient optimization on the parameters wdirectly, after initialization in terms of vandg: this is what we compare to in section 5.",apply,networks,network,1
"The method can also be applied to networks without weight normalization, simply by doing stochastic gradient optimization on the parameters wdirectly, after initialization in terms of vandg: this is what we compare to in section 5.",apply,weight normalization,normalization,1
"The method can also be applied to networks without weight normalization, simply by doing stochastic gradient optimization on the parameters wdirectly, after initialization in terms of vandg: this is what we compare to in section 5.",compare,what,what,1
"The method can also be applied to networks without weight normalization, simply by doing stochastic gradient optimization on the parameters wdirectly, after initialization in terms of vandg: this is what we compare to in section 5.",compare,we,we,1
"The method can also be applied to networks without weight normalization, simply by doing stochastic gradient optimization on the parameters wdirectly, after initialization in terms of vandg: this is what we compare to in section 5.",compare,section,section,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,our work,work,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,this type,type,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,initialization,initialization,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,different authors,author,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,"20,14","20,14",1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,who,who,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,such data-based initialization,initialization,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,use,use,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,the standard parameterization,parameterization,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,terms,term,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",propose,w.,w.,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",work,use,use,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",work,the standard parameterization,parameterization,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",work,terms,term,1
"Independently from our work, this type of initialization was recently proposed by different authors [ 20,14] who found such data-based initialization to work well for use with the standard parameterization in terms of w.",work,w.,w.,1
The downside of this initialization method is that it can only be applied in similar cases as where batch normalization is applicable.,apply,it,it,1
The downside of this initialization method is that it can only be applied in similar cases as where batch normalization is applicable.,apply,similar cases,case,1
The downside of this initialization method is that it can only be applied in similar cases as where batch normalization is applicable.,apply,batch normalization,normalization,1
"For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods.",have,models,model,1
"For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods.",have,recursion,recursion,1
"For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods.",have,RNNs,rnn,1
"For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods.",have,LSTMs,lstm,1
"For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods.",have,we,we,1
"For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods.",have,standard initialization methods,method,1
"For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods.",resort,standard initialization methods,method,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",introduce,section,section,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,4 Mean-only Batch Normalization Weight normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,section,section,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,the scale,scale,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,neuron activations,activation,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,the parameters,parameter,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,batch normalization,normalization,2
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,the means,mean,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,the neuron activations,activation,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,We,we,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,the idea,idea,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,weight normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,a special version,version,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,which,which,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,we,we,3
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,mean-only batch normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,this normalization method,method,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,the minibatch,minibatch,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,full batch normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",make,the minibatch standard deviations,deviation,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,the means,mean,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,the neuron activations,activation,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,We,we,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,the idea,idea,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,weight normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,a special version,version,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,batch normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,which,which,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,we,we,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",depend,mean-only batch normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",subtract,this normalization method,method,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",subtract,we,we,2
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",subtract,the minibatch,minibatch,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",subtract,full batch normalization,normalization,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",subtract,the minibatch standard deviations,deviation,1
"4 Mean-only Batch Normalization Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization : With this normalization method, we subtract out the minibatch means like with full batch normalization, 4but we do not divide by the minibatch standard deviations.",mean,the minibatch,minibatch,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",compute,we,we,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",compute,neuron activations,activation,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",compute,t,t,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",compute,wx,wx,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",compute,+b,b,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",compute,y,y,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",use,t,t,1
"That is, we compute neuron activations using t=wx; ~t=t [t] +b; y",use,wx,wx,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,~t,~t,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,the weight vector,vector,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,weight normalization,normalization,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,the minibatch mean,mean,2
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,the pre,pre,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,-,-,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,activation,activation,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,training,training,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,we,we,2
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,a running average,average,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,which,which,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",parameterize,test time,time,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",use,weight normalization,normalization,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",substitute,which,which,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",substitute,we,we,1
"=(~t) (7) where wis the weight vector, parameterized using weight normalization, and [t]is the minibatch mean of the pre-activation t. During training, we keep a running average of the minibatch mean which we substitute in for [t]at test time.",substitute,test time,time,1
The gradient of the loss with respect to the pre-activation tis calculated as rtL=r~tL [r~tL]; (8) where[:]denotes once again the operation of taking the minibatch mean.,calculate,=,=,1
The gradient of the loss with respect to the pre-activation tis calculated as rtL=r~tL [r~tL]; (8) where[:]denotes once again the operation of taking the minibatch mean.,calculate,r,r,1
The gradient of the loss with respect to the pre-activation tis calculated as rtL=r~tL [r~tL]; (8) where[:]denotes once again the operation of taking the minibatch mean.,calculate,tL [r,tl [r,1
Mean-only batch normalization thus has the effect of centering the gradients that are backpropagated.,have,Mean-only batch normalization,normalization,1
Mean-only batch normalization thus has the effect of centering the gradients that are backpropagated.,have,the effect,effect,1
Mean-only batch normalization thus has the effect of centering the gradients that are backpropagated.,have,the gradients,gradient,1
Mean-only batch normalization thus has the effect of centering the gradients that are backpropagated.,have,that,that,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,addition,addition,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,this method,method,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,less noise,noise,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,training,training,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,the noise,noise,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,that,that,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,the law,law,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,large numbers,number,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",cause,that [t]and[r,[t]and[r,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",ensure,the law,law,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",ensure,large numbers,number,1
"In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that [t]and[r~t]are approximately normally distributed.",ensure,that [t]and[r,[t]and[r,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",have,the added noise,noise,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",have,much lighter tails,tail,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",have,the highly kurtotic noise,noise,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",have,the minibatch estimate,estimate,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",have,the variance,variance,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",have,full batch normalization,normalization,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",cause,the minibatch estimate,estimate,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",cause,the variance,variance,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",cause,full batch normalization,normalization,1
"Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization.",use,full batch normalization,normalization,1
"As we show in section 5.1, this leads to improved accuracy at test time.",show,we,we,1
"As we show in section 5.1, this leads to improved accuracy at test time.",show,section,section,1
"As we show in section 5.1, this leads to improved accuracy at test time.",lead,we,we,1
"As we show in section 5.1, this leads to improved accuracy at test time.",lead,section,section,1
"As we show in section 5.1, this leads to improved accuracy at test time.",lead,this,this,1
"As we show in section 5.1, this leads to improved accuracy at test time.",lead,improved accuracy,accuracy,1
"As we show in section 5.1, this leads to improved accuracy at test time.",lead,test time,time,1
"5 Experiments We experimentally validate the usefulness of our method using four different models for varied applications in supervised image recognition, generative modelling, and deep reinforcement learning.",use,four different models,model,1
"5 Experiments We experimentally validate the usefulness of our method using four different models for varied applications in supervised image recognition, generative modelling, and deep reinforcement learning.",use,varied applications,application,1
"5 Experiments We experimentally validate the usefulness of our method using four different models for varied applications in supervised image recognition, generative modelling, and deep reinforcement learning.",use,supervised image recognition,recognition,1
"5 Experiments We experimentally validate the usefulness of our method using four different models for varied applications in supervised image recognition, generative modelling, and deep reinforcement learning.",use,generative modelling,modelling,1
"5 Experiments We experimentally validate the usefulness of our method using four different models for varied applications in supervised image recognition, generative modelling, and deep reinforcement learning.",use,deep reinforcement learning,learning,1
"5.1 Supervised Classiﬁcation: CIFAR-10 To test our reparameterization method for the application of supervised classiﬁcation, we consider the CIFAR-10 data set of natural images .",test,our reparameterization method,method,1
"5.1 Supervised Classiﬁcation: CIFAR-10 To test our reparameterization method for the application of supervised classiﬁcation, we consider the CIFAR-10 data set of natural images .",test,the application,application,1
"5.1 Supervised Classiﬁcation: CIFAR-10 To test our reparameterization method for the application of supervised classiﬁcation, we consider the CIFAR-10 data set of natural images .",test,supervised classiﬁcation,classiﬁcation,1
"5.1 Supervised Classiﬁcation: CIFAR-10 To test our reparameterization method for the application of supervised classiﬁcation, we consider the CIFAR-10 data set of natural images .",set,the CIFAR-10 data,datum,1
"5.1 Supervised Classiﬁcation: CIFAR-10 To test our reparameterization method for the application of supervised classiﬁcation, we consider the CIFAR-10 data set of natural images .",set,natural images,image,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",base,The model,model,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",base,we,we,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",base,the ConvPool-CNN-C architecture,architecture,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",base,some small modiﬁcations,modiﬁcation,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,The model,model,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,we,we,2
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,the ConvPool-CNN-C architecture,architecture,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,some small modiﬁcations,modiﬁcation,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,the ﬁrst dropout layer,layer,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,a layer,layer,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,that,that,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",replace,Gaussian noise,noise,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,The model,model,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,we,we,4
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,the ConvPool-CNN-C architecture,architecture,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,some small modiﬁcations,modiﬁcation,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,the ﬁrst dropout layer,layer,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,a layer,layer,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,that,that,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,Gaussian noise,noise,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,the last hidden layer,layer,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,10 units,unit,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,192 units,unit,1
"The model we are using is based on the ConvPool-CNN-C architecture of , with some small modiﬁcations: we replace the ﬁrst dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 22max-pooling, rather than 33.",expand,22max-pooling,pooling,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,The only hyperparameter,hyperparameter,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,that,that,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,we,we,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,the standard deviation,deviation,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,the Gaussian noise,noise,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,the performance,performance,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,the network,network,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,a holdout set,set,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,10000 examples,example,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,the standard parameterization,parameterization,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,no weight normalization,normalization,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",choose,batch normalization,normalization,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",maximize,the performance,performance,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",maximize,the network,network,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",maximize,a holdout set,set,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",maximize,10000 examples,example,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",use,the standard parameterization,parameterization,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",use,no weight normalization,normalization,1
"The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization).",use,batch normalization,normalization,1
A full description of the resulting architecture is given in table A in the supplementary material.,give,A full description,description,1
A full description of the resulting architecture is given in table A in the supplementary material.,give,the resulting architecture,architecture,1
A full description of the resulting architecture is given in table A in the supplementary material.,give,table A,a,1
A full description of the resulting architecture is given in table A in the supplementary material.,give,the supplementary material,material,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,We,we,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,our network,network,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,CIFAR-10,CIFAR-10,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,Adam,Adam,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,200 epochs,epoch,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,a ﬁxed learning rate,rate,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,momentum,momentum,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",train,the ﬁrst 100 epochs,epoch,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",use,Adam,Adam,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",use,200 epochs,epoch,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",use,a ﬁxed learning rate,rate,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",use,momentum,momentum,1
"We train our network for CIFAR-10 using Adam  for 200 epochs, with a ﬁxed learning rate and momentum of 0.9 for the ﬁrst 100 epochs.",use,the ﬁrst 100 epochs,epoch,1
For the last 100 epochs we set the momentum to 0.5 and linearly decay the learning rate to zero.,set,the last 100 epochs,epoch,1
For the last 100 epochs we set the momentum to 0.5 and linearly decay the learning rate to zero.,set,we,we,1
For the last 100 epochs we set the momentum to 0.5 and linearly decay the learning rate to zero.,set,the momentum,momentum,1
For the last 100 epochs we set the momentum to 0.5 and linearly decay the learning rate to zero.,set,the learning rate,rate,1
We use a minibatch size of 100.,use,We,we,1
We use a minibatch size of 100.,use,a minibatch size,size,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",evaluate,We,we,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",evaluate,5 different parameterizations,parameterization,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",evaluate,the network,network,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",evaluate,batch normalization,normalization,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",evaluate,weight normalization,normalization,2
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",evaluate,mean-only batch normalization,normalization,2
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",evaluate,the normal parameterization,parameterization,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",use,batch normalization,normalization,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",use,weight normalization,normalization,2
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",use,mean-only batch normalization,normalization,2
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",combine,mean-only batch normalization,normalization,1
"We evaluate 5 different parameterizations of the network: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization, 5) using mean-only batch normalization with the normal parameterization.",use,the normal parameterization,parameterization,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,initialize,The network parameters,parameter,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,initialize,the scheme,scheme,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,initialize,section,section,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,initialize,all four cases,case,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,initialize,identical parameters,parameter,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,use,the scheme,scheme,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,use,section,section,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,have,all four cases,case,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,have,identical parameters,parameter,1
The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out.,start,identical parameters,parameter,1
For each case we pick the optimal learning rate in f0:0003;0:001;0:003;0:01g.,pick,each case,case,1
For each case we pick the optimal learning rate in f0:0003;0:001;0:003;0:01g.,pick,we,we,1
For each case we pick the optimal learning rate in f0:0003;0:001;0:003;0:01g.,pick,the optimal learning rate,rate,1
For each case we pick the optimal learning rate in f0:0003;0:001;0:003;0:01g.,pick,g.,g.,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,find,The resulting error curves,curve,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,find,training,training,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,find,ﬁgure,ﬁgure,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,provide,The resulting error curves,curve,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,provide,training,training,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,provide,ﬁgure,ﬁgure,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,provide,both weight normalization,normalization,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,provide,batch normalization,normalization,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,provide,a signiﬁcant speed-up,up,1
The resulting error curves during training can be found in ﬁgure 1: both weight normalization and batch normalization provide a signiﬁcant speed-up over the standard parameterization.,provide,the standard parameterization,parameterization,1
"Batch normalization makes slightly more progress per epoch than weight normalization early on, although this is partly offset by the higher computational cost: with our implementation, training with batch normalization was about 16% slower compared to the standard parameterization.",make,Batch normalization,normalization,1
"Batch normalization makes slightly more progress per epoch than weight normalization early on, although this is partly offset by the higher computational cost: with our implementation, training with batch normalization was about 16% slower compared to the standard parameterization.",make,slightly more progress,progress,1
"Batch normalization makes slightly more progress per epoch than weight normalization early on, although this is partly offset by the higher computational cost: with our implementation, training with batch normalization was about 16% slower compared to the standard parameterization.",make,epoch,epoch,1
"Batch normalization makes slightly more progress per epoch than weight normalization early on, although this is partly offset by the higher computational cost: with our implementation, training with batch normalization was about 16% slower compared to the standard parameterization.",make,weight normalization,normalization,1
"Batch normalization makes slightly more progress per epoch than weight normalization early on, although this is partly offset by the higher computational cost: with our implementation, training with batch normalization was about 16% slower compared to the standard parameterization.",offset,this,this,1
"Batch normalization makes slightly more progress per epoch than weight normalization early on, although this is partly offset by the higher computational cost: with our implementation, training with batch normalization was about 16% slower compared to the standard parameterization.",offset,the higher computational cost,cost,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",seem,the later stage,stage,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",seem,training,training,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",seem,weight normalization,normalization,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",seem,batch normalization,normalization,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",seem,about the same speed,speed,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",seem,the normal parameterization,parameterization,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",seem,mean-only batch normalization,normalization,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",optimize,about the same speed,speed,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",optimize,the normal parameterization,parameterization,1
"During the later stage of training, weight normalization and batch normalization seem to optimize at about the same speed, with the normal parameterization (with or without mean-only batch normalization) still lagging behind.",optimize,mean-only batch normalization,normalization,1
"After optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set.",use,the different parameterizations,parameterization,1
"After optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set.",evaluate,the network,network,1
"After optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set.",evaluate,200 epochs,epoch,1
"After optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set.",evaluate,the different parameterizations,parameterization,1
"After optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set.",evaluate,we,we,1
"After optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set.",evaluate,their performance,performance,1
"After optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set.",evaluate,the CIFAR-10 test set,set,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",summarize,The results,result,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",summarize,table,table,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",have,The results,result,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",have,table,table,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",have,weight normalization,normalization,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",have,the normal parameterization,parameterization,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",have,mean-only batch normalization,normalization,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",have,similar test accuracy,accuracy,1
"The results are summarized in table 2: weight normalization, the normal parameterization, and mean-only batch normalization have similar test accuracy ( 8:5% error).",have,( 8:5% error,error,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",combine,weight normalization,normalization,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",have,Mean-only batch normalization,normalization,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",have,weight normalization,normalization,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",have,the best performance,performance,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",have,7:31% test error,error,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",have,mean-only batch normalization,normalization,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",have,the normal parameterization,parameterization,1
"Mean-only batch normalization combined with weight normalization has the best performance at 7:31% test error, and interestingly does much better than mean-only batch normalization combined with the normal parameterization:",combine,the normal parameterization,parameterization,1
"This suggests that the noise added by batch normalization can be useful for regularizing the network, 50 50 100 150 20000.050.1 training epochstraining error    normal param.",suggest,This,this,1
"This suggests that the noise added by batch normalization can be useful for regularizing the network, 50 50 100 150 20000.050.1 training epochstraining error    normal param.",suggest,the noise,noise,1
"This suggests that the noise added by batch normalization can be useful for regularizing the network, 50 50 100 150 20000.050.1 training epochstraining error    normal param.",suggest,batch normalization,normalization,1
"This suggests that the noise added by batch normalization can be useful for regularizing the network, 50 50 100 150 20000.050.1 training epochstraining error    normal param.",suggest,the network,network,1
"This suggests that the noise added by batch normalization can be useful for regularizing the network, 50 50 100 150 20000.050.1 training epochstraining error    normal param.",suggest,error    normal param,param,1
"This suggests that the noise added by batch normalization can be useful for regularizing the network, 50 50 100 150 20000.050.1 training epochstraining error    normal param.",add,batch normalization,normalization,1
"This suggests that the noise added by batch normalization can be useful for regularizing the network, 50 50 100 150 20000.050.1 training epochstraining error    normal param.",epochstraine,error    normal param,param,1
WN + mean−only BN mean−only BNFigure 1: Training error for CIFAR-10 using different network parameterizations.,use,different network parameterizations,parameterization,1
"For weight normalization ,batch normalization , and mean-only batch normalization we show results using Adam with a learning rate of 0.003.",show,weight normalization,normalization,1
"For weight normalization ,batch normalization , and mean-only batch normalization we show results using Adam with a learning rate of 0.003.",show,we,we,1
"For weight normalization ,batch normalization , and mean-only batch normalization we show results using Adam with a learning rate of 0.003.",show,results,result,1
"For weight normalization ,batch normalization , and mean-only batch normalization we show results using Adam with a learning rate of 0.003.",show,Adam,Adam,1
"For weight normalization ,batch normalization , and mean-only batch normalization we show results using Adam with a learning rate of 0.003.",show,a learning rate,rate,1
"For weight normalization ,batch normalization , and mean-only batch normalization we show results using Adam with a learning rate of 0.003.",use,Adam,Adam,1
"For weight normalization ,batch normalization , and mean-only batch normalization we show results using Adam with a learning rate of 0.003.",use,a learning rate,rate,1
For the normal parameterization we instead use 0.0003 which works best in this case.,use,the normal parameterization,parameterization,1
For the normal parameterization we instead use 0.0003 which works best in this case.,use,we,we,1
For the normal parameterization we instead use 0.0003 which works best in this case.,use,which,which,1
For the normal parameterization we instead use 0.0003 which works best in this case.,use,this case,case,1
For the last 100 epochs the learning rate is linearly decayed to zero.,decay,the last 100 epochs,epoch,1
For the last 100 epochs the learning rate is linearly decayed to zero.,decay,the learning rate,rate,1
but that the reparameterization provided by weight normalization or full batch normalization is also needed for optimal results.,provide,weight normalization,normalization,1
but that the reparameterization provided by weight normalization or full batch normalization is also needed for optimal results.,provide,full batch normalization,normalization,1
but that the reparameterization provided by weight normalization or full batch normalization is also needed for optimal results.,need,the reparameterization,reparameterization,1
but that the reparameterization provided by weight normalization or full batch normalization is also needed for optimal results.,need,weight normalization,normalization,1
but that the reparameterization provided by weight normalization or full batch normalization is also needed for optimal results.,need,full batch normalization,normalization,1
but that the reparameterization provided by weight normalization or full batch normalization is also needed for optimal results.,need,optimal results,result,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,We,we,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,the substantial improvement,improvement,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,mean-only B.N.,B.N.,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,weight normalization,normalization,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,regular batch normalization,normalization,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,the distribution,distribution,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,the noise,noise,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,the normalization method,method,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",hypothesize,training,training,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",cause,the normalization method,method,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",cause,training,training,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,We,we,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,the substantial improvement,improvement,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,mean-only B.N.,B.N.,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,weight normalization,normalization,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,regular batch normalization,normalization,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,the distribution,distribution,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,the noise,noise,3
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,the normalization method,method,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,training,training,3
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,mean-only batch normalization,normalization,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,the minibatch mean,mean,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,a distribution,distribution,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,that,that,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,approximately Gaussian,Gaussian,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,full batch normalization,normalization,2
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",have,much higher kurtosis,kurtosis,2
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",add,full batch normalization,normalization,1
"We hypothesize that the substantial improvement by mean-only B.N. with weight normalization over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis.",add,training,training,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",combine,weight normalization,normalization,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,we,we,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,the result,result,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,mean-only batch normalization,normalization,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,weight normalization,normalization,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,the,the,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,the-art,art,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,CIFAR-10,CIFAR-10,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,methods,method,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,that,that,1
"As far as we are aware, the result with mean-only batch normalization combined with weight normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation.",represent,data augmentation,augmentation,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",test,Convolutional V AE Next,Next,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",test,we,we,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",test,the effect,effect,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",test,weight normalization,normalization,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",test,deep convolutional variational autoencoders,autoencoder,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",test,CV AEs,AEs,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",apply,deep convolutional variational autoencoders,autoencoder,1
"5.2 Generative Modelling: Convolutional V AE Next, we test the effect of weight normalization applied to deep convolutional variational autoencoders (CV AEs)",apply,CV AEs,AEs,1
"[ 13,24,25], trained on the MNIST data set of images of handwritten digits and the CIFAR-10 data set of small natural images.",train,the MNIST data,datum,1
"[ 13,24,25], trained on the MNIST data set of images of handwritten digits and the CIFAR-10 data set of small natural images.",train,images,image,1
"[ 13,24,25], trained on the MNIST data set of images of handwritten digits and the CIFAR-10 data set of small natural images.",train,handwritten digits,digit,1
"[ 13,24,25], trained on the MNIST data set of images of handwritten digits and the CIFAR-10 data set of small natural images.",train,small natural images,image,1
"[ 13,24,25], trained on the MNIST data set of images of handwritten digits and the CIFAR-10 data set of small natural images.",set,images,image,1
"[ 13,24,25], trained on the MNIST data set of images of handwritten digits and the CIFAR-10 data set of small natural images.",set,handwritten digits,digit,1
"Variational auto-encoders are generative models that explain the data vector xas arising from a set of latent variables z, through a joint distribution of the form p(z;x) =p(z)p(xjz), where the decoder p(xjz)is speciﬁed using a neural network.",arise,a set,set,1
"Variational auto-encoders are generative models that explain the data vector xas arising from a set of latent variables z, through a joint distribution of the form p(z;x) =p(z)p(xjz), where the decoder p(xjz)is speciﬁed using a neural network.",arise,latent variables,variable,1
"Variational auto-encoders are generative models that explain the data vector xas arising from a set of latent variables z, through a joint distribution of the form p(z;x) =p(z)p(xjz), where the decoder p(xjz)is speciﬁed using a neural network.",arise,z,z,1
"Variational auto-encoders are generative models that explain the data vector xas arising from a set of latent variables z, through a joint distribution of the form p(z;x) =p(z)p(xjz), where the decoder p(xjz)is speciﬁed using a neural network.",use,a neural network,network,1
A lower bound on the log marginal likelihood logp(x) can be obtained by approximately inferring the latent variables zfrom the observed data xusing anencoder distribution q(zjx)that is also speciﬁed as a neural network.,obtain,the log,log,1
A lower bound on the log marginal likelihood logp(x) can be obtained by approximately inferring the latent variables zfrom the observed data xusing anencoder distribution q(zjx)that is also speciﬁed as a neural network.,obtain,marginal likelihood logp(x,logp(x,1
A lower bound on the log marginal likelihood logp(x) can be obtained by approximately inferring the latent variables zfrom the observed data xusing anencoder distribution q(zjx)that is also speciﬁed as a neural network.,obtain,the latent variables,variable,1
A lower bound on the log marginal likelihood logp(x) can be obtained by approximately inferring the latent variables zfrom the observed data xusing anencoder distribution q(zjx)that is also speciﬁed as a neural network.,obtain,the observed data,datum,1
A lower bound on the log marginal likelihood logp(x) can be obtained by approximately inferring the latent variables zfrom the observed data xusing anencoder distribution q(zjx)that is also speciﬁed as a neural network.,obtain,xusing anencoder distribution,distribution,1
A lower bound on the log marginal likelihood logp(x) can be obtained by approximately inferring the latent variables zfrom the observed data xusing anencoder distribution q(zjx)that is also speciﬁed as a neural network.,obtain,q(zjx)that,q(zjx)that,1
A lower bound on the log marginal likelihood logp(x) can be obtained by approximately inferring the latent variables zfrom the observed data xusing anencoder distribution q(zjx)that is also speciﬁed as a neural network.,obtain,a neural network,network,1
This lower bound is then optimized to ﬁt the model to the data.,optimize,the model,model,1
This lower bound is then optimized to ﬁt the model to the data.,optimize,the data,datum,1
This lower bound is then optimized to ﬁt the model to the data.,ﬁt,the model,model,1
This lower bound is then optimized to ﬁt the model to the data.,ﬁt,the data,datum,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,We,we,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,a similar implementation,implementation,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,the CV AE,AE,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,some modiﬁcations,modiﬁcation,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,the encoder,encoder,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,decoder,decoder,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,ResNet  blocks,block,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,the diagonal posterior,posterior,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",follow,auto-regressive variational inference1,inference1,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",parameterize,the encoder,encoder,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",parameterize,decoder,decoder,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",parameterize,ResNet  blocks,block,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",parameterize,the diagonal posterior,posterior,1
"We follow a similar implementation of the CV AE as in  with some modiﬁcations, mainly that the encoder and decoder are parameterized with ResNet  blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.",parameterize,auto-regressive variational inference1,inference1,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,MNIST,MNIST,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,the encoder,encoder,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,3 sequences,sequence,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,two ResNet blocks,block,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,each,each,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,the ﬁrst sequence,sequence,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,16 feature maps,map,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,the others,other,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",consist,32 feature maps,map,1
"For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the ﬁrst sequence acting on 16 feature maps, the others on 32 feature maps.",act,16 feature maps,map,1
"The ﬁrst two sequences are followed by a 2-times subsampling operation implemented using 22stride, while the third sequence is followed by a fully connected layer with 450 units.",follow,The ﬁrst two sequences,sequence,1
"The ﬁrst two sequences are followed by a 2-times subsampling operation implemented using 22stride, while the third sequence is followed by a fully connected layer with 450 units.",follow,a 2-times subsampling operation,operation,1
"The ﬁrst two sequences are followed by a 2-times subsampling operation implemented using 22stride, while the third sequence is followed by a fully connected layer with 450 units.",follow,the third sequence,sequence,2
"The ﬁrst two sequences are followed by a 2-times subsampling operation implemented using 22stride, while the third sequence is followed by a fully connected layer with 450 units.",follow,a fully connected layer,layer,2
"The ﬁrst two sequences are followed by a 2-times subsampling operation implemented using 22stride, while the third sequence is followed by a fully connected layer with 450 units.",follow,450 units,unit,2
"The decoder has a similar architecture, but with reversed direction.",have,The decoder,decoder,1
"The decoder has a similar architecture, but with reversed direction.",have,a similar architecture,architecture,1
"The decoder has a similar architecture, but with reversed direction.",have,reversed direction,direction,1
"For CIFAR-10, we used a neural architecture with ResNet units and multiple intermediate stochastic layers1.",use,CIFAR-10,CIFAR-10,1
"For CIFAR-10, we used a neural architecture with ResNet units and multiple intermediate stochastic layers1.",use,we,we,1
"For CIFAR-10, we used a neural architecture with ResNet units and multiple intermediate stochastic layers1.",use,a neural architecture,architecture,1
"For CIFAR-10, we used a neural architecture with ResNet units and multiple intermediate stochastic layers1.",use,ResNet units,unit,1
"For CIFAR-10, we used a neural architecture with ResNet units and multiple intermediate stochastic layers1.",use,multiple intermediate stochastic layers1,layers1,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,We,we,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,Adamax,Adamax,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,=,=,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,optimization,optimization,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,combination,combination,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,the form,form,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,an exponential moving average,average,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,that,that,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,parameters,parameter,1
"We used Adamax  with= 0:002for optimization, in combination with Polyak averaging  in the form of an exponential moving average that averages parameters over approximately 10 epochs.",use,approximately 10 epochs,epoch,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,ﬁgure,ﬁgure,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,we,we,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,a function,function,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,number,number,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,training epochs,epoch,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,error bars,bar,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,multiple different random seeds,seed,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",plot,parameters,parameter,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",bind,a function,function,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",bind,number,number,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",bind,training epochs,epoch,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",bind,error bars,bar,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",bind,multiple different random seeds,seed,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",bind,parameters,parameter,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",base,multiple different random seeds,seed,1
"In ﬁgure 3, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters.",base,parameters,parameter,1
"As can be seen, the parameterization with weight normalization has lower variance and converges to a better optimum.",have,the parameterization,parameterization,1
"As can be seen, the parameterization with weight normalization has lower variance and converges to a better optimum.",have,weight normalization,normalization,1
"As can be seen, the parameterization with weight normalization has lower variance and converges to a better optimum.",have,lower variance,variance,1
"As can be seen, the parameterization with weight normalization has lower variance and converges to a better optimum.",have,converges,converge,1
We observe similar results across different hyper-parameter settings.,observe,We,we,1
We observe similar results across different hyper-parameter settings.,observe,similar results,result,1
We observe similar results across different hyper-parameter settings.,observe,different hyper-parameter settings,setting,1
1Manuscript in preparation 650 100 150 200 250 300 training epochs88.0 87.5 87.0 86.5 86.0 85.5 85.0 84.5 84.0 bound on marginal likelihoodConvolutional VAE on MNIST normal parameterization weight normalization 0,bind,marginal likelihoodConvolutional VAE,VAE,1
1Manuscript in preparation 650 100 150 200 250 300 training epochs88.0 87.5 87.0 86.5 86.0 85.5 85.0 84.5 84.0 bound on marginal likelihoodConvolutional VAE on MNIST normal parameterization weight normalization 0,bind,MNIST normal parameterization weight normalization,normalization,1
"50 100 150 200 250 300 350 400 450 training epochs800085009000950010000bound on marginal likelihoodConvolutional VAE on CIFAR-10 normal parameterization weight normalizationFigure 3: Marginal log likelihood lower bound on the MNIST (top) and CIFAR-10 (bottom) test sets for a convolutional V AE during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,the MNIST,MNIST,1
"50 100 150 200 250 300 350 400 450 training epochs800085009000950010000bound on marginal likelihoodConvolutional VAE on CIFAR-10 normal parameterization weight normalizationFigure 3: Marginal log likelihood lower bound on the MNIST (top) and CIFAR-10 (bottom) test sets for a convolutional V AE during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,(top,top,1
"50 100 150 200 250 300 350 400 450 training epochs800085009000950010000bound on marginal likelihoodConvolutional VAE on CIFAR-10 normal parameterization weight normalizationFigure 3: Marginal log likelihood lower bound on the MNIST (top) and CIFAR-10 (bottom) test sets for a convolutional V AE during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,CIFAR-10,CIFAR-10,1
"50 100 150 200 250 300 350 400 450 training epochs800085009000950010000bound on marginal likelihoodConvolutional VAE on CIFAR-10 normal parameterization weight normalizationFigure 3: Marginal log likelihood lower bound on the MNIST (top) and CIFAR-10 (bottom) test sets for a convolutional V AE during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,(bottom) test sets,set,1
"50 100 150 200 250 300 350 400 450 training epochs800085009000950010000bound on marginal likelihoodConvolutional VAE on CIFAR-10 normal parameterization weight normalizationFigure 3: Marginal log likelihood lower bound on the MNIST (top) and CIFAR-10 (bottom) test sets for a convolutional V AE during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,a convolutional V AE,AE,1
"For MNIST, we provide standard error bars to indicate variance based on different initial random seeds.",provide,MNIST,MNIST,1
"For MNIST, we provide standard error bars to indicate variance based on different initial random seeds.",provide,we,we,1
"For MNIST, we provide standard error bars to indicate variance based on different initial random seeds.",provide,standard error bars,bar,1
"For MNIST, we provide standard error bars to indicate variance based on different initial random seeds.",provide,variance,variance,1
"For MNIST, we provide standard error bars to indicate variance based on different initial random seeds.",provide,different initial random seeds,seed,1
"For MNIST, we provide standard error bars to indicate variance based on different initial random seeds.",indicate,variance,variance,1
"For MNIST, we provide standard error bars to indicate variance based on different initial random seeds.",indicate,different initial random seeds,seed,1
"5.3 Generative Modelling: DRA W Next, we consider DRAW, a recurrent generative model by .",consider,DRA W Next,Next,1
"5.3 Generative Modelling: DRA W Next, we consider DRAW, a recurrent generative model by .",consider,we,we,1
"5.3 Generative Modelling: DRA W Next, we consider DRAW, a recurrent generative model by .",consider,DRAW,DRAW,1
"5.3 Generative Modelling: DRA W Next, we consider DRAW, a recurrent generative model by .",consider,a recurrent generative model,model,1
"DRAW is a variational auto-encoder with generative model p(z)p(xjz)and encoder q(zjx), similar to the model in section 5.2, but with both the encoder and decoder consisting of a recurrent neural network comprised of Long Short-Term Memory (LSTM)  units.",comprise,Long Short-Term Memory (LSTM)  units,unit,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",consist,LSTM units,unit,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",consist,a memory cell,cell,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",consist,additive dynamics,dynamic,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",consist,input,input,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",consist,that,that,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",consist,which information,information,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",consist,the memory,memory,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",ﬂow,which information,information,1
"LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information ﬂows in and out of the memory.",ﬂow,the memory,memory,1
The additive dynamics enables learning of long-range dependencies in the data.,enable,The additive dynamics,dynamic,1
The additive dynamics enables learning of long-range dependencies in the data.,enable,long-range dependencies,dependency,1
The additive dynamics enables learning of long-range dependencies in the data.,enable,the data,datum,1
The additive dynamics enables learning of long-range dependencies in the data.,learn,long-range dependencies,dependency,1
The additive dynamics enables learning of long-range dependencies in the data.,learn,the data,datum,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,each time,time,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,step,step,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,the model,model,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,DRAW,DRAW,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,the same set,set,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,weight vectors,vector,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,the cell states,state,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,the LSTM units,unit,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,its encoder,encoder,1
"At each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder.",use,decoder,decoder,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,apply,batch normalization,normalization,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,apply,this model,model,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,the recurrent nature,nature,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,this process,process,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,it,it,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,batch normalization,normalization,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,this model,model,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,the cell states,state,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,their ability,ability,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,diminish,information,information,1
Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information.,pass,information,information,1
"Fortunately, weight normalization can be applied trivially to the weight vectors of each LSTM unit, and we ﬁnd this to work well empirically.",apply,weight normalization,normalization,1
"Fortunately, weight normalization can be applied trivially to the weight vectors of each LSTM unit, and we ﬁnd this to work well empirically.",apply,the weight vectors,vector,1
"Fortunately, weight normalization can be applied trivially to the weight vectors of each LSTM unit, and we ﬁnd this to work well empirically.",apply,each LSTM unit,unit,1
"Fortunately, weight normalization can be applied trivially to the weight vectors of each LSTM unit, and we ﬁnd this to work well empirically.",apply,we,we,1
"Fortunately, weight normalization can be applied trivially to the weight vectors of each LSTM unit, and we ﬁnd this to work well empirically.",apply,this,this,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,take,We,we,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,take,the Theano implementation,implementation,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,take,DRAW,draw,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,take,it,it,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,take,the MNIST data,datum,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,take,handwritten digits,digit,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,provide,it,it,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,provide,the MNIST data,datum,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,provide,handwritten digits,digit,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,model,the MNIST data,datum,1
We take the Theano implementation of DRAW provided at https://github.com/jbornschein/ draw and use it to model the MNIST data set of handwritten digits.,model,handwritten digits,digit,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,make,We,we,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,make,a single modiﬁcation,modiﬁcation,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,make,the model,model,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,apply,We,we,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,apply,a single modiﬁcation,modiﬁcation,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,apply,the model,model,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,apply,we,we,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,apply,weight normalization,normalization,1
We then make a single modiﬁcation to the model: we apply weight normalization to all weight vectors.,apply,all weight vectors,vector,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",see,ﬁgure,ﬁgure,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,ﬁgure,ﬁgure,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,this,this,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,convergence,convergence,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,the optimization procedure,procedure,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,the initialization method,method,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,learning rate,rate,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,that,that,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,use,use,1
"As can be seen in ﬁgure 4, this signiﬁcantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization.",speed,the normal parameterization,parameterization,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound,epochsbound,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,marginal log,log,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,normal parameterization weight normalization Figure,figure,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,the MNIST test,test,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,DRAW,draw,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,training,training,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,both the standard implementation,implementation,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,our modiﬁcation,modiﬁcation,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",likelihood,weight normalization,normalization,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,the MNIST test,test,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,DRAW,draw,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",bind,training,training,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",set,DRAW,draw,1
"0102030405060708090100−120−115−110−105−100−95−90−85−80 training epochsbound on marginal log likelihood    normal parameterization weight normalization Figure 4: Marginal log likelihood lower bound on the MNIST test set for DRAW during training, for both the standard implementation as well as our modiﬁcation with weight normalization .",set,training,training,1
"100 epochs is not sufﬁcient for convergence for this model, but the implementation using weight normalization clearly makes progress much more quickly than with the standard parameterization.",use,weight normalization,normalization,1
DQN Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment .,apply,we,we,1
DQN Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment .,apply,weight normalization,normalization,1
DQN Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment .,apply,the problem,problem,1
DQN Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment .,apply,Reinforcement Learning,Learning,1
DQN Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment .,apply,games,game,1
DQN Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment .,apply,the Atari Learning Environment,Environment,1
This is an application for which batch normalization is not well suited: the noise introduced by estimating the minibatch statistics destabilizes the learning process.,introduce,the minibatch statistics,statistic,1
This is an application for which batch normalization is not well suited: the noise introduced by estimating the minibatch statistics destabilizes the learning process.,introduce,the learning process,process,1
This is an application for which batch normalization is not well suited: the noise introduced by estimating the minibatch statistics destabilizes the learning process.,destabilize,the minibatch statistics,statistic,1
This is an application for which batch normalization is not well suited: the noise introduced by estimating the minibatch statistics destabilizes the learning process.,destabilize,the learning process,process,1
We were not able to get batch normalization to work for DQN without using an impractically large minibatch size.,get,batch normalization,normalization,1
We were not able to get batch normalization to work for DQN without using an impractically large minibatch size.,get,DQN,DQN,1
We were not able to get batch normalization to work for DQN without using an impractically large minibatch size.,get,an impractically large minibatch size,size,1
We were not able to get batch normalization to work for DQN without using an impractically large minibatch size.,work,batch normalization,normalization,1
We were not able to get batch normalization to work for DQN without using an impractically large minibatch size.,work,DQN,DQN,1
We were not able to get batch normalization to work for DQN without using an impractically large minibatch size.,work,an impractically large minibatch size,size,1
"In contrast, weight normalization is easy to apply in this context, as is the initialization method of section 3.",apply,this context,context,1
Stochastic gradient learning is performed using Adamax  with momentum of 0.5.,perform,Stochastic gradient learning,learning,1
Stochastic gradient learning is performed using Adamax  with momentum of 0.5.,perform,Adamax,Adamax,1
Stochastic gradient learning is performed using Adamax  with momentum of 0.5.,perform,momentum,momentum,1
Stochastic gradient learning is performed using Adamax  with momentum of 0.5.,use,Adamax,Adamax,1
Stochastic gradient learning is performed using Adamax  with momentum of 0.5.,use,momentum,momentum,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",search,We,we,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",search,optimal learning rates,rate,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",search,f0:0001;0:0003;0:001;0:003g,g,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",search,0:0003,0:0003,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",search,weight normalization,normalization,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",search,0:0001,0:0001,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",search,the normal parameterization,parameterization,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",ﬁnde,0:0003,0:0003,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",ﬁnde,weight normalization,normalization,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",ﬁnde,0:0001,0:0001,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",ﬁnde,the normal parameterization,parameterization,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",work,weight normalization,normalization,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",work,0:0001,0:0001,1
"We search for optimal learning rates in f0:0001;0:0003;0:001;0:003g, generally ﬁnding 0:0003 to work well with weight normalization and 0:0001 to work well for the normal parameterization.",work,the normal parameterization,parameterization,2
We also use a larger minibatch size (64) which we found to be more efﬁcient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance).,use,We,we,1
We also use a larger minibatch size (64) which we found to be more efﬁcient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance).,use,a larger minibatch size,size,1
We also use a larger minibatch size (64) which we found to be more efﬁcient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance).,use,which,which,1
We also use a larger minibatch size (64) which we found to be more efﬁcient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance).,use,we,we,1
We also use a larger minibatch size (64) which we found to be more efﬁcient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance).,use,our hardware,hardware,1
We also use a larger minibatch size (64) which we found to be more efﬁcient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance).,use,(Amazon Elastic Compute Cloud g2.2xlarge,g2.2xlarge,1
We also use a larger minibatch size (64) which we found to be more efﬁcient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance).,use,GPU instance,instance,1
"However, we use a Python/Theano/Lasagne reimplementation of their work, adapted from the implementation available athttps://github.com/spragunr/deep_q_rl , so there may be small additional differences in implementation.",use,we,we,1
"However, we use a Python/Theano/Lasagne reimplementation of their work, adapted from the implementation available athttps://github.com/spragunr/deep_q_rl , so there may be small additional differences in implementation.",use,a Python/Theano/Lasagne reimplementation,reimplementation,1
"However, we use a Python/Theano/Lasagne reimplementation of their work, adapted from the implementation available athttps://github.com/spragunr/deep_q_rl , so there may be small additional differences in implementation.",use,their work,work,1
"However, we use a Python/Theano/Lasagne reimplementation of their work, adapted from the implementation available athttps://github.com/spragunr/deep_q_rl , so there may be small additional differences in implementation.",use,the implementation,implementation,1
"However, we use a Python/Theano/Lasagne reimplementation of their work, adapted from the implementation available athttps://github.com/spragunr/deep_q_rl , so there may be small additional differences in implementation.",use,small additional differences,difference,1
"However, we use a Python/Theano/Lasagne reimplementation of their work, adapted from the implementation available athttps://github.com/spragunr/deep_q_rl , so there may be small additional differences in implementation.",use,implementation,implementation,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,show,Figure,figure,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,show,the training curves,curve,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,show,DQN,DQN,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,show,the standard parameterization,parameterization,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,show,weight normalization,normalization,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,show,Space Invaders,Invaders,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,obtain,DQN,DQN,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,obtain,the standard parameterization,parameterization,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,obtain,weight normalization,normalization,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,obtain,Space Invaders,Invaders,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,use,DQN,DQN,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,use,the standard parameterization,parameterization,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,use,weight normalization,normalization,1
Figure 5 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders.,use,Space Invaders,Invaders,1
Using weight normalization the algorithm progresses more quickly and reaches a better ﬁnal result.,progress,weight normalization,normalization,1
Using weight normalization the algorithm progresses more quickly and reaches a better ﬁnal result.,progress,the algorithm,algorithm,1
Using weight normalization the algorithm progresses more quickly and reaches a better ﬁnal result.,progress,a better ﬁnal result,result,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,show,Table,table,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,show,the ﬁnal evaluation scores,score,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,show,DQN,DQN,2
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,show,weight normalization,normalization,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,show,four games,game,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,show,average weight normalization,normalization,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,show,the performance,performance,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,obtain,DQN,DQN,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,obtain,weight normalization,normalization,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,obtain,four games,game,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,improve,average weight normalization,normalization,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,improve,the performance,performance,1
Table 6 shows the ﬁnal evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN.,improve,DQN,DQN,1
"50 100 150 20005001000150020002500 training epochstest reward per episode    normal parameterization weight normalization Figure 5: Evaluation scores for Space Invaders obtained by DQN after each epoch of training, for both the standard parameterization and using weight normalization.",obtain,DQN,DQN,1
"50 100 150 20005001000150020002500 training epochstest reward per episode    normal parameterization weight normalization Figure 5: Evaluation scores for Space Invaders obtained by DQN after each epoch of training, for both the standard parameterization and using weight normalization.",obtain,each epoch,epoch,1
"50 100 150 20005001000150020002500 training epochstest reward per episode    normal parameterization weight normalization Figure 5: Evaluation scores for Space Invaders obtained by DQN after each epoch of training, for both the standard parameterization and using weight normalization.",obtain,training,training,1
Learning rates for both cases were selected to maximize the highest achieved test score.,select,Learning rates,rate,1
Learning rates for both cases were selected to maximize the highest achieved test score.,select,both cases,case,1
Learning rates for both cases were selected to maximize the highest achieved test score.,select,the highest achieved test score,score,1
Learning rates for both cases were selected to maximize the highest achieved test score.,maximize,the highest achieved test score,score,1
"Game normal weightnorm Mnih Breakout 410 403 401 Enduro 1,250 1,448 302 Seaquest 7,188 7,375 5,286 Space Invaders 1,779 2,179 1,975 Figure 6: Maximum evaluation scores obtained by DQN, using either the normal parameterization or using weight normalization.",obtain,DQN,DQN,1
"Game normal weightnorm Mnih Breakout 410 403 401 Enduro 1,250 1,448 302 Seaquest 7,188 7,375 5,286 Space Invaders 1,779 2,179 1,975 Figure 6: Maximum evaluation scores obtained by DQN, using either the normal parameterization or using weight normalization.",use,either the normal parameterization,parameterization,1
"Game normal weightnorm Mnih Breakout 410 403 401 Enduro 1,250 1,448 302 Seaquest 7,188 7,375 5,286 Space Invaders 1,779 2,179 1,975 Figure 6: Maximum evaluation scores obtained by DQN, using either the normal parameterization or using weight normalization.",use,weight normalization,normalization,1
The scores indicated by Mnih et al. are those reported by : Our normal parameterization is approximately equivalent to their method.,indicate,Mnih et al,al,1
The scores indicated by Mnih et al. are those reported by : Our normal parameterization is approximately equivalent to their method.,indicate,.,.,1
Differences in scores may be caused by small differences in our implementation.,cause,Differences,difference,1
Differences in scores may be caused by small differences in our implementation.,cause,scores,score,1
Differences in scores may be caused by small differences in our implementation.,cause,small differences,difference,1
Differences in scores may be caused by small differences in our implementation.,cause,our implementation,implementation,1
"Speciﬁcally, the difference in our score on Enduro and that reported by  might be due to us not using a play-time limit during evaluation.",use,a play-time limit,limit,1
"Speciﬁcally, the difference in our score on Enduro and that reported by  might be due to us not using a play-time limit during evaluation.",use,evaluation,evaluation,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,We,we,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,weight normalization,normalization,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,a simple reparameterization,reparameterization,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,the weight vectors,vector,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,a neural network,network,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,that,that,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,the convergence,convergence,1
"6 Conclusion We have presented weight normalization , a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization.",present,stochastic gradient descent optimization,optimization,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",apply,Weight normalization,normalization,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",apply,four different models,model,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",apply,supervised image recognition,recognition,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",apply,generative modelling,modelling,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",apply,deep reinforcement learning,learning,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",apply,a consistent advantage,advantage,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",apply,applications,application,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",show,a consistent advantage,advantage,1
"Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications.",show,applications,application,1
"The reparameterization method is easy to apply, has low computational overhead, and does not introduce dependencies between the examples in a minibatch, making it our default choice in the development of new deep learning architectures.",make,it,it,1
"The reparameterization method is easy to apply, has low computational overhead, and does not introduce dependencies between the examples in a minibatch, making it our default choice in the development of new deep learning architectures.",make,the development,development,1
"The reparameterization method is easy to apply, has low computational overhead, and does not introduce dependencies between the examples in a minibatch, making it our default choice in the development of new deep learning architectures.",make,new deep learning architectures,architecture,1
Acknowledgments We thank John Schulman for helpful comments on an earlier draft of this paper.,thank,We,we,1
Acknowledgments We thank John Schulman for helpful comments on an earlier draft of this paper.,thank,John Schulman,Schulman,1
Acknowledgments We thank John Schulman for helpful comments on an earlier draft of this paper.,thank,helpful comments,comment,1
Acknowledgments We thank John Schulman for helpful comments on an earlier draft of this paper.,thank,an earlier draft,draft,1
Acknowledgments We thank John Schulman for helpful comments on an earlier draft of this paper.,thank,this paper,paper,1
